\documentclass{elsarticle}
\usepackage{graphicx}
%\usepackage{multicol}
%\usepackage{footmisc}
\usepackage{amstext}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage[english]{babel}
%\usepackage[official,right]{eurosym}
\selectlanguage{english}
\include{preamramics}
\hyphenation{ExecEngine}
\newtheorem{lemma}{Lemma}
\def\id#1{\text{\it #1\/}}
\def\Events{{\mathit E}}
\begin{document}

\title{A Theory for Distributed Database Transactions}
\author[ou,ordina]{Stef Joosten\fnref{fn1}}
\ead{stef.joosten@ou.nl}
\author[utwente]{Sebastiaan Joosten\fnref{fn2}}
\address[ou]{Open Universiteit Nederland, Postbus 2960, 6401 DL Heerlen, the Netherlands}
\address[ordina]{Ordina NV, Nieuwegein, the Netherlands}
\address[utwente]{University of Twente, Enschede, the Netherlands}
\fntext[fn1]{ORCID 0000-0001-8308-0189}
\fntext[fn2]{ORCID 0000-0002-6590-6220}

\begin{abstract}
	A shared data space can be problematic in globally distributed information systems,
	because it can become a performance bottleneck or a single point of failure.
	Yet, transaction theory as we know it assumes a shared data space.
	Several solutions exist, such as nested transactions and distributed transactions.
	Practitioners tend to avoid them when possible because they are complicated and error-prone.

	In this contribution we let go of the shared data space requirement.
	We propose a transaction theory based on events rather than states.
	By expressing the required properties A (atomic), C (consistent), I (isolated), and D (durable)
	in terms of events we can avoid the assumption of a shared state space.

	This makes transactions suitable for more databases than just relational ones.
	The soundness of the theory has been established by means of formal proof.
	To show that the theory works, the theory is demonstrated by a prototype developed in Ampersand\footnote{\tt https://github.com/AmpersandTarski/Ampersand}.
	To demonstrate that the theory is about real transactions, this contribution defines semantics of conventional database transactions.
	To fulfill our promise, we also gives semantics of transactions on the semantic web.
\end{abstract}

\begin{keyword}
relation algebra\sep software development\sep legal reasoning\sep information systems design\sep Ampersand\sep MirrorMe\sep reactive programming
\end{keyword}
\maketitle

\section{Introduction}
\label{sct:Introduction}
	A distributed information system (DIS) serves its users with things such as
	registering data, guarding consistency, protecting that data,
	and providing it accurately and meaningfully for its intended purpose to various users.
	These users want predictable behaviour, even when they change a shared state,
	e.g. when reserving a seat in a global flight reservation system like Galileo or Amadeus.
	Although we cannot always avoid sharing a state,
	we usually can avoid sharing an entire database.
	Globally distributed applications such as Netflix and LinkedIn cannot afford to rely on a shared database for obvious reasons of performance and reliability.
	Many cloud applications use distributed storage models instead.
	Examples are event stores (e.g.\ Kafka), triple stores (e.g.\ semantic web), graph databases (e.g. Neo4J) and others.
	Many so-called NoSQL databases lack functionality for transactions,
	leaving the problems for the user to solve.
	As a result, architectural patterns exist for the purpose of doing transactions in a distributed setting.
	Some of these patterns simply circumvent the problem.
	Others will isolate transactionality into a smaller sub-problem.
	Still others introduce locking mechanisms to do the job.

	Our work is inspired by the observation that the merge of Git corresponds to the commit that we know from transactional databases.
	We show this correspondence by expressing the characteristic properties of transactions in a theory of time.
	For this purpose we build on a well-known theory of time~\cite{DBLP:journals/ipl/FosterCWZ18},
	which is generally applicable,
	well-established, and founded in a solid mathematical structure~\cite{Foster14}.
	This means we have to redefine Atomicity, Consistency, Isolation, and Durability in terms of events.
	Informally, {\em Atomicity} means that a sequence of primitive actions can be executed as though it were a single action.
	{\em Consistency} means that integrity rules are warranted.
	{\em Isolation} means that actions can be done without any knowledge of what other users are doing.
	{\em Durability} means that history does not change.

	The result of this work contributes to the body of foundational theory of information systems
	by defining transactions in a theory of time.
	By showing that transactions come quite naturally from an event perspective,
	we give formal substance under the observation that event systems like Git do not need new features to behave transactionally.

	\section{Related work}
\label{sct:Related work}
	Distributed information systems are dynamic systems,
	so time is the first aspect to focus on.
	We will define transactions in terms of traces,
	which draws on the seminal work of Hoare on communicating sequential processes~\cite{HoareHe98},
	work on process algebras by Bergstra and Klop~\cite{DBLP:journals/iandc/BergstraK84}, and
	Robin Milner's work on CCS~\cite{Milner1989}.
	Trace theory is today the usual way of defining distributed event-based systems.
	Trace algebras also have sound interpretations for analog real-time systems and hybrid systems~\cite{Foster17b}.
	This paper, however, sticks to the interpretation of event sequences, because distributed information systems are event-based.
	Yet, we define the theory in trace algebra so as to impose as few restrictions as possible.
	We have used Isabelle to verify the required proofs,
	to increase our confidence in this theory.
	
\cite{Foster17c}
	In the nineties, scientists saw that category theory can be used to formalize database schemas~\cite{Frederiks1997},
	which was advocated as a means to further foundational theories of information systems.
	An interesting read is~\cite{Foster17c}\footnote{\tt\tiny https://towardsdatascience.com/distributed-transactions-and-why-you-should-care-116b6da8d72}.
	It explains how modern distributed database cope with ACID properties in a way that makes sense.

	ACID 2.0 - Associative, Commutative, Idempotent, Distributed

\section{History}
\label{sct:History}
	A treatment of time lays the foundation of our theory of distributed information systems (DIS).
	Because of the distributed nature of a DIS,
	we may not know in which order two events\footnote{Cambridge Dictionary defines ``event'' as: anything that happens} have happened.
	For this reason a partial order is used to describe the history of a DIS.

	We use variables $e$, $e'$, $e_1$, and $e_2$ to represent events.
	Let $\mathbb E$ represent the set of all conceivable events.

	We represent the ``history of a DIS up to some point in time'' by a relation $\id{prec}$ (pronounce: {\em precedes}),
	being a set of pairs from $\mathbb E$.
\[\id{prec}\subseteq\mathbb E\times\mathbb E\]
	Let $e_1\ \id{prec}\ e_2$ represent the fact that event $e_1$ has happened before event $e_2$,
	making $\id{prec}$ a relation on events in $\mathbb E$.
	We require:
\begin{eqnarray}
	&&\id{prec}\ \subseteq\ \overline{I}\label{req:prec irreflexive}\\
	&&\id{prec}^+\cap{\flip{\id{prec}}}^+\ \subseteq\ I\label{req:precPlus asy}
\end{eqnarray}
	The events in the DIS, of which $\id{prec}$ represents the history, are 
\begin{eqnarray}
	\id{events}(\id{prec})&=&(V;\id{prec}\cap I)\ \cup\ (I\cap\id{prec};V)
\end{eqnarray}
	Requirement~\ref{req:prec irreflexive} excludes the situation that $e$ has happened before itself.
	To represent time faithfully, equation~\ref{req:precPlus asy} requires that the transitive closure $\id{prec}^+$ is antisymmetric.
	This makes $\id{prec}^*$ a reflexive, transitive, and antisymmetric relation, which lets us use it as a temporal order between events.
	We call $\id{prec}$ ``a history'' because it represents all events that have happened up to a certain point in time and the (partial) order between them.
	Let $\mathbb H$ be the set of all conceivable histories, meaning that every $\id{prec}\in\mathbb H$.

	In a history $\id{prec}$, an event $e$ that precedes another event is called a {\em past} event,
	which we define as \ $\exists e_1:\ e\ \id{prec}\ e_1$.
	All other events are called {\em current} events in $\id{prec}$.
	Function $\id{current}$ computes the current events in history $\id{prec}$.
	Function $\id{initial}$ computes the initial events, which are the events without preceding events.
\begin{eqnarray}
	\id{current}(\id{prec})&=&\overline{\id{prec}}^+;\id{prec}^+\cap I\\
	\id{initial}(\id{prec})&=&\id{prec}^+;\overline{\id{prec}}^+\cap I
\end{eqnarray}

	A history $\id{prec}$ where $\id{prec}^*$ is a total order is a special case, which we call a {\em sequential history}.
	Note that any nonempty finite sequential history has one current event and one initial event.
	Other finite histories may have more.
	In a sequential history structure, every event except the initial one has precisely one preceding event.
	In general, however, events may have multiple preceding events and multiple succeeding events.

	We define $\leq$ (called ``prefix'') to represent the fact that history $\id{prec}$ lies in the past of history $\id{prec}'$.
\begin{definition}[$\leq$]
\label{def:lies in the past}
\begin{displaymath}
	\id{prec}\leq\id{prec}'\ \Leftrightarrow\ \id{prec}^*\subseteq\id{prec}'^*\ \wedge\ \id{initial}(\id{prec})\subseteq\id{initial}(\id{prec}')
\end{displaymath}
\end{definition}

\section{Traces}
\label{sct:Traces}
	Transactions leave {\em traces} in the history of a DIS, so let us define these traces.
	For this purpose we have chosen definitions that satisfy trace algebra
	and represent traces in discrete-event systems.
	Other interpretations of the same trace algebra comprise real-time systems and hybrid systems,
	which has been  proven by Foster~\cite{Foster17b}.
	We prove (TBD) that our definitions comply with Foster's axioms of trace algebra,
	but stick to one interpretation (discrete event systems).

	Let $\mathbb T$ be the set of all conceivable traces.
	With \id{prec} being a history, we define a trace $(e_1,e_2)_\id{prec}$ as follows:
\begin{definition}[Traces]
\label{def:Traces}
\item $(e_1,e_2)_\id{prec}\ =\ e_1;\id{prec}^*\cap\id{prec}^*;e_2$
\end{definition}
	If it is obvious from the context which history ($\id{prec}$) is meant, we may write $(e_1,e_2)$ rather than $(e_1,e_2)_\id{prec}$.
	A trace algebra is a triple $\la\mathbb T,+,0\ra$,
	where the operator $+$ (concatenation) and empty trace $0$ satisfy the axioms of trace algebras:
\begin{eqnarray}
	x+(y+z)&=&(x+y)+z\label{axi:associative}\\
	0+x&=&x+0\label{axi:lr-identity}\\
	x+y\ =\ x+z&\Rightarrow&y=z\label{axi:left cancellation}\\
	x+z\ =\ y+z&\Rightarrow&x=y\label{axi:right cancellation}\\
	x+z\ =\ 0&\Rightarrow&x=0\label{axi:no inverse}
\end{eqnarray}
	In terms of universal algebra,
	equations~\ref{axi:associative} and~\ref{axi:lr-identity} state that a trace algebra is a {\em monoid}.
	Note that $+$ is {\em not} commutative.
	Equations~\ref{axi:left cancellation} and~\ref{axi:right cancellation} state that $+$ is {\em cancellative}.
	Equation~\ref{axi:no inverse} implies that there is no inverse.
	It makes the cancellative monoid into a trace algebra.

	For showing that definition~\ref{def:Traces} satisfies the axioms of trace algebra,
	we define the empty trace ($0$) and the trace concatenation ($+$) as $I_\mathbb E$ and $;$ (relation composition).
\begin{definition}[trace operators]
\item $0\ =\ I$
\item $x+y\ =\ x;y$
\end{definition}

	Let us define prefix for traces ($\leq$), to correspond closely to the prefix for histories
	(as given in definition~\ref{def:lies in the past}):
\begin{definition}[Prefix]
\label{def:Prefix of traces}
\item $\begin{array}[t]{rcl}
	(e,e_1)_\id{prec}\leq(e,e_2)_{\id{prec}'}&\Leftrightarrow&\id{prec}\leq\id{prec}'\ \wedge\ e_1\ \id{prec}'\ e_2
\end{array}$
\end{definition}
	We have defined this operator such that it satisfies Foster's algebraic definition of prefix~\cite{Foster17b}.
	(proof needed)
\begin{eqnarray}
	x\leq y&\Rightarrow&\exists z:\ x+z=y\label{def:Prefix}
\end{eqnarray}
	The prefix allows us to define subtraction of traces, to have a way to ``cut off'' an ``older'' part of a trace.
\begin{definition}[Subtract]
\label{def:Subtract}
\item $\begin{array}[t]{rcll}
	(e,e_1)_\id{prec}-(e,e_2)_{\id{prec}'}&=&(e_1,e_2)_{\id{prec}'}&\text{if }(e,e_1)_\id{prec}\leq(e,e_2)_{\id{prec}'}\\
	   &=&0&\text{otherwise}
\end{array}$
\end{definition}
	Definition~\ref{def:Subtract} satisfies Foster's definition of subtract:
\begin{equation}
\label{axi:Foster subtract}
\begin{array}[t]{rcll}
	y-x&=&z&\text{if }x\leq y\ \wedge\ \exists z: y=x+z\\
	   &=&0&\text{otherwise}
\end{array}
\end{equation}

\section{State}
\label{sct:State}
	Typically, distributed information systems are stateful.
	Every event in the history of a DIS may cause a different (local) state.
	(The distributed nature of a DIS means that states are local.)
	Let $\mathbb S$ be the set of all conceivable states.
	So we need a function $\id{state}:\mathbb E\rightarrow\mathbb S$ that assigns one state to every event $e$ in history $\id{prec}$.
	State $\id{state}(e)$ represents the state immediately after $e$ has happened.
	If the history $\id{prec}$ is obvious from the context, we may write $\id{state}(e)$ rather than $\id{state}(e)$.
	We assume an empty state, $\emptyset$, which is the state immediately before an initial event has happened.
	The global state of the system, if relevant, can be computed by $\{\id{state}(e) | e\in\id{current}(\id{prec})\}$.
	Locally we work with one specific current event, $e$, with $\id{state}(e)$ representing the local current state.

	In a DIS, new states typically emerge as a change on an existing state.
	So we want to describe a DIS in terms of changes.
	Let $\mathbb D$ be the set of all conceivable changes and let $\delta_\id{prec}:\mathbb E\times\mathbb E\rightarrow\mathbb D$
	be a function that maps a pair of events to the corresponding change.
	The change between the states of events $e_1$ and $e_2$ is given by $\delta_\id{prec}(e_1,e_2)$.
	Let us define a DIS as a tuple $(\id{prec},\delta_\id{prec})$,
	in which $\id{prec}$ is a history and
	$\delta_\id{prec}$ represents the changes of state that have taken place throughout the history.

	We need operators to compute a new state ($s_2$) from a given state ($s_1$).
	In fact, we need two operators, $\oplus:\mathbb S\times\mathbb D\rightarrow\mathbb S$ and $\ominus:\mathbb S\times\mathbb S\rightarrow\mathbb D$, such that
\begin{equation}
	s_2\ =\ s_1\oplus(s_2\ominus s_1)
\label{req:oplus and ominus}
\end{equation}
	Let us call $s_2\ominus s_1$ the difference between states $s_2$ and $s_1$.
	The following equations represent the laws about state.
	They must be kept true all the time in a DIS.
\begin{eqnarray}
	\delta_\id{prec}(e_1,e_2)&=&\id{state}(e_2)\ominus\id{state}(e_1)\\
	e_1\ \id{prec}\ e_2&\Rightarrow&\id{state}(e_2)\ =\ \id{state}(e_1)\oplus\delta_\id{prec}(e_1,e_2)
\end{eqnarray}

\section{Transactions}
\label{sct:Transactions}
	With the theory in place, let us proceed to define transactions.
	Let us first define the notion of transaction
	and then define the classical properties of transactions: atomic, consistent, isolated, and durable.

	A transaction is denoted as $\id{act}_{e_1,e_2,\Delta}$, which is a mapping from one DIS to another
\begin{equation}
\label{def:transaction}
\begin{array}{rl}
	\multicolumn{2}{l}{e_2\not\in\id{events}(\id{prec})\ \wedge\ (e_1,e_2)\in\id{prec}'^*\ \Rightarrow}\\
	&\id{act}_{e_1,e_2,\Delta}(\id{prec},\delta_\id{prec})\ =\ (\id{prec}',\delta'_{\id{prec}'})\\
	&\text{where}\\
	&\ \ \begin{array}[t]{rcll}
		\delta'_{\id{prec}'}(e,e')&=&\Delta&\text{if }(e,e')=(e_1,e_2)\\
		\delta'_{\id{prec}'}(e,e')&=&\delta_\id{prec}(e,e')&\text{otherwise}\\
		\end{array}
\end{array}
\end{equation}
	The state function can be computed recursively as follows:
\begin{equation}
\begin{array}{rcll}
	\id{state}(e)&=&\id{state}(e_1)\oplus\delta(e_1,e)&\text{if }(e_1,e)\in\id{prec}\\
	                       &=&\emptyset&\text{otherwise}
\end{array}
\end{equation}

	Now let us define the classical characteristic properties of transactions.
	Let us start with atomic.

	In his paper "Splitting Atoms Safely"~\cite{JONES2007109} Cliff Jones\footnote{%
	Jones is author of the Atomicity Manifesto and author of the formal method VDM.}
	describes the intended meaning of atomicity in natural language:
	``If an action is said to be executed “atomically”, it is assumed that it will not be affected by interference
	and that the environment will not be able to observe intermediate steps of the action in question.''
	Jones argues that interference must be tolerated, however.
	With shared state programs, he says, an action must achieve some required result even though its state can be changed by other interfering processes.

	The purpose of this paper is to propose a useful formal definition of atomicity.
	Atomicity expresses that a trace of multiple events can be treated as a single pair $\in\id{prec}$.
	To define this, we need an auxiliary function
	$\id{atomize}:\mathbb E\times\mathbb E\times\mathbb H\rightarrow\mathbb H$,
	which replaces a trace $(e_1,e_2)_\id{prec}$ by the pair $(e_1,e_2)$
	in a history $\id{prec}$.
\begin{eqnarray}
	\begin{array}[t]{r@{}l@{}l}
	\multicolumn{3}{l}{\id{atomize}(e_1,e_2,\id{prec})}\\
	   \ =\ \id{prec}-&(&\{ (e',e)\in\id{prec}| e_1\ \id{prec}^*\ e\ \wedge\ e\ \id{prec}^*\ e_2\}\ \cup\\
			  & &\{ (e,e')\in\id{prec}| e_1\ \id{prec}^*\ e\ \wedge\ e\ \id{prec}^*\ e_2\}\\
			  &)\\
		\cup&&\{ (e_1,e_2) \}\\
	\end{array}
\end{eqnarray}

\begin{definition}[Atomic]
\item A transaction $\id{act}_{e_1,e_2,\Delta}$ is atomic in history $\id{prec}$ iff:
\[\begin{array}{rl}
	&\id{act}_{e_1,e_2,\Delta}(\id{prec},\delta_\id{prec})\ =\ (\id{prec}',\delta'_{\id{prec}'})\\
	\Rightarrow\\
	&\id{atomize}(e_1,e_2,\id{prec})\ \leq\ \id{prec}'
\end{array}\]
\label{def:Atomicity}
\end{definition}

	Now let us turn to consistency.
	This property means that a transaction preserves truth of the state.
	We need a predicate $\mathcal P\{\mathbb S\}$ to express that a state is consistent.
\begin{definition}[Consistency]
\item A transaction $\id{act}_{e_1,e_2,\Delta}$ is consistent w.r.t. $\mathcal P$ iff:
\[\mathcal P\{\id{state}(e_1)\}\ \Rightarrow\ \mathcal P\{\id{state}(e_2)\}\]
\label{def:Consistency}
\end{definition}

	The third property is isolation.
	A transaction $\id{act}(e_1,e_2)$ is isolated, i.e. performed in isolation, if it does not affect any other transaction in the system.
	This means that the only event in trace $(e_1,e_2)_\id{prec}$ that precedes events outside that trace is $e_1$.
	Likewise, the only event in trace $(e_1,e_2)_\id{prec}$ that is preceded by events outside that trace is $e_2$.
	We can now define isolation as the property that the traces of the ``atomized'' history
	are the same as the atomization of all traces in that history.
\begin{definition}[Isolation]
\item A transaction $\id{act}_{e_1,e_2,\Delta}$ works isolated in history $\id{prec}$ iff:
\label{def:Isolation}
\begin{eqnarray}
	(\id{atomize}(e_1,e_2,\id{prec}))^*&=&\id{atomize}(e_1,e_2,\id{prec}^*)
\end{eqnarray}
\end{definition}

	Then we have durability.
	This property expresses that a transaction must preserve history.
\begin{definition}[Durability]
\item A transaction $\id{act}_{e_1,e_2,\Delta}$ is durable iff:
\label{def:Durability}
\[\begin{array}{rl}
	&\id{act}_{e_1,e_2,\Delta}(\id{prec},\delta_\id{prec})\ =\ (\id{prec}',\delta'_{\id{prec}'})\\
	\Rightarrow\\
	&\id{prec}^*\subseteq\id{prec}'^*\ \wedge\ \delta_\id{prec}\subseteq\delta'_{\id{prec}'}
\end{array}\]
\end{definition}
	Note that durability is already satisfied in the definition of transaction (equation~\ref{def:transaction}). (proof needed)

\section{Bibliography}
\bibliographystyle{elsarticle-harv}
\bibliography{doc}


\end{document}

\section{Progress}
\label{sct:Progress}

	A DIS grows by adding events to it.
	We describe each time a new event $e$ happens as a mapping, $\id{happens}_{P,e}$, from one history structure to another:
\begin{equation}
\begin{array}[b]{l}
	P\subseteq\Events\ \wedge\ e\notin E\\
	\ \ \Rightarrow\ \ \id{happens}_{P,e}(\la \Events, \id{prec}\ra)\ =\ \la \Events\cup\{e\}, \id{prec}\cup\{(e_1,e) | e_1\in P\}\ra
\end{array}
\label{req:def happens}
\end{equation}
	This happening adds one event to $\Events$ and enlarges relation $\id{prec}$ with one pair for every immediate predecessor of $e$.
	To understand why $P\subseteq\Events$ is necessary,
	consider $\la \Events', \id{prec}'\ra=\id{happens}_{P,e}(h)$.
	Now suppose event $e_1\in P$ but $e_1\notin\Events$.
	Equation~\ref{req:def happens} tells us that $(e_1,e)\in\id{prec}'$.
	However, this violates requirement~\ref{req:precInE}.
	So it is necessary that every $e_1\in P$ is an element of $\Events$.
	The immediate predecessors of $e$ are specified in set $P$, which we call the predecessor set.
	We need the predecessor set to cater for the distributed nature of a DIS.
\begin{lemma}
\label{lemma:happens preserves history structure}
	If $\la \Events, \id{prec}\ra$ is a history structure and $P\subseteq\Events$ and $e\notin\Events$
	then $\id{happens}_{P,e}(\la \Events, \id{prec}\ra)$ is a history structure.
\end{lemma}
	Lemma~\ref{lemma:happens preserves history structure} is a preservation property.
	It says that $\id{happens}_{P,e}$ preserves the history structure,
	provided that $P\subseteq\Events$ and $e\notin\Events$.
	This lemma has been proven in proof assistant Isabelle/HOL.
	Condition $P\subseteq\Events$ is necessary to satisfy equation~\ref{req:precInE}.
	Condition $e\notin\Events$ is necessary too,
	because any $e\in\Events$ violates the antisymmetry of $\id{prec}^+$ (requirement~\ref{req:precPlus asy}).

	With lemma~\ref{lemma:happens preserves history structure} we can prove that $\id{happens}_{P,e}(h)=h'$ implies that $h\subseteq h'$.

	Let us define predicate $\id{HS}\la \Events, \id{prec}\ra$
	to denote that $\la \Events, \id{prec}\ra$ is a history structure.
\begin{eqnarray}
	&&\id{HS}\la \emptyset, \emptyset\ra\label{def:zeroHS}\\
&&\begin{array}{@{}l}
	P\subseteq\Events\ \wedge\ e\notin E\ \wedge\ \id{HS}\la \Events, \id{prec}\ra\ \Rightarrow\ \id{HS}(\id{happens}_{P,e}\la \Events, \id{prec}\ra)
\end{array}
\label{def:HS}
\end{eqnarray}
	Equation~\ref{def:zeroHS} says that $\la \emptyset, \emptyset\ra$ is a history structure.
	Equation~\ref{def:HS} specifies recursively that $\id{happens}_{P,e}(h)$ is a history structure iff
	$h$ is a history structure and if $P$ and $e$ satisfy the specified conditions.
	This definition allows us to prove properties of finite history structures by induction.
	In practical situations, every history structure is finite.

	Note that $\id{happens}_{P,e}$ can make current events become past events.
	If $h'=\id{happens}_{P,e}(h)$, every event in $P$ is a past event in $h'$ and event $e$ becomes current in $h'$.

\section{States}
	This section defines the requirements for predictable behavior of a distributed information system (DIS).
	We consider behavior to be predictable if every state can be computed unambiguously from its history.
	For that purpose we introduce the notion of state.
	Let $\mathbb S$ represent the set of all conceivable states.

	This section expands history structures to stateful history substructures
	to represent the (global) state of an information system at any point in time.
	Since this paper is about distributed systems,
	we may not assume that the global state of a DIS at some point in time is observable.
	The state of a DIS is observable only in its local parts.
	So we shall define multiple states, each of which may be interpreted locally.

	We define a {\em stateful history structure} as a triple $\la\Events, \id{prec}, \id{state}\ra$ in which
	$\la \Events, \id{prec}\ra$ is a history structure and $\id{state}:\Events\rightarrow{\mathbb S}$ is a function
	that maps every event in $\Events$ to a state.
	We will use predicate $\id{SHS}\la \Events, \id{prec}, \id{state}\ra$
	to express that $\la \Events, \id{prec}, \id{state}\ra$ is a stateful history structure.
	Let $s=\id{state}(e)$ represent the fact that $s$ is the state immediately after event $e$ has happened.
	Or shorter: $s$ is the state after event $e$.
%	Instead of $s=\id{state}(e)$, we may also write $(s,e)\in\id{state}$ because a function is a univalent and total relation
%	(and therefore a set of pairs).
	A {\em current state} is the state after a current event
	and a {\em past state} is the state after a past event.

	In a stateful history structure, the occurrence of an event $e$ defines a new history structure.
	This is the way systems evolve over time.
	So we define the occurrence of an event as a mapping, $\id{occurs}_{P,e,s}$, from one SHS to another SHS:
\begin{equation}
\begin{array}{l}
	P\subseteq\Events\ \wedge\ e\notin E\\
	\ \ \Rightarrow\ \ 
\begin{array}[t]{l}
	\id{occurs}_{P,e,s}(\la\Events, \id{prec}, \id{state}\ra)\\
	\ \ =\ \begin{array}[t]{@{}l@{\hspace{.2em}}c@{\hspace{.2em}}ll}
		\text{Let }\id{newState}(x)&=&s\text{,}&\text{if }x=e\\
				&=&\id{state}(x)\text{,}&\text{otherwise}\\
		\multicolumn{4}{@{}l}{\text{in }\la\Events\cup\{e\}, \id{prec}\cup\{(e_2,e)|e_2\in P\}, \id{newState}\}\ra}
		\end{array}
\end{array}
\end{array}
\label{req:def occurs}
\end{equation}
	This definition extends the definition of $\id{happens}$ (eqn.~\ref{req:def happens}).
	By using lemma~\ref{lemma:happens preserves history structure} we can prove that $\id{occurs}_{P,e,s}$ preserves
	the stateful history structure.
	Also, we will use the subset symbol $\subseteq$ to represent the fact that $h$ is a {\em stateful historic substructure} of $h'$.
	And it will come as no surprise that $\id{occurs}_{P,e,s}(h)=h'$ implies that $h\subseteq h'$.
	We will use predicate $\id{SHS}\la \Events, \id{prec}, \id{state}\ra$
	to express that $\la \Events, \id{prec}, \id{state}\ra$ is a stateful history structure.
	To describe the growth of a stateful history structure by the event,
	we define $\id{SHS}$ recursively using $\id{occurs}_{P,e,s}$ as recurring step:
\begin{eqnarray}
	&&\id{SHS}\la \emptyset, \emptyset, \emptyset\ra\\
&&\begin{array}{@{}l}
	P\subseteq\Events\ \wedge\ e\notin E\ \wedge\ \id{SHS}\la \Events, \id{prec}, \id{state}\ra\\
	\ \ \Rightarrow\ \id{SHS}(\id{occurs}_{P,e,s}\la \Events, \id{prec}, \id{state}\ra)
\end{array}
\label{req:recursive def occurs}
\end{eqnarray}

\section{Transactions}
\label{sct:Transactions}
	In practice, a new state is computed from the current state by specifying a change, $\Delta$.
	Such changes are called {\em transactions}.
	We use $s_1\oplus\Delta$ to compute a new state from an existing state $s_1$ and a change $\Delta$.

	We can define a stateful history structure,
	in which the state after every event is computed from a change, $\Delta$.
	For this purpose we must designate one specific state, $\mathcal I\in\mathbb S$, as the initial state.
	We must additionally specify an existing event $e_1$ in order to compute $\id{state}(e_1)\oplus\Delta$.
	The following mapping $\id{occurByDelta}_{P,e_1,\Delta,e}$ defines how an SHS grows by one event ($e$)
	using only the change between the state of event $e_1$ to the new event $e$:
\begin{eqnarray}
&&\begin{array}{l}
	\id{occurByDelta}_{P,e_1,\Delta,e}(\la\Events, \id{prec}, \id{state}\ra)\\
	\ \ =\ \begin{array}[t]{@{}l@{\hspace{.2em}}c@{\hspace{.2em}}ll}
		\text{Let }s_1&=&\id{state}(e_1)\text{,}&\text{if }e_1\in P\\
				&=&\mathcal I\text{,}&\text{otherwise}\\
		\text{Let }\id{newState}(x)&=&s_1\text{,}&\text{if }x=e\\
		&=&\id{state}(x)\text{,}&\text{otherwise}\\
		\multicolumn{4}{@{}l}{\text{in }\la\Events\cup\{e\}, \id{prec}\cup\{(e_2,e)|e_2\in P\}, \id{newState}\}\ra}
		\end{array}
\end{array}
\label{req:SHS by Delta}
\end{eqnarray}
	This mapping serves as recursion step in the following definition of a stateful history structure
\begin{eqnarray}
	&&\id{SHS}\la \emptyset, \emptyset, \emptyset\ra\\
&&\begin{array}{@{}l}
	P\subseteq\Events\ \wedge\ e\notin E\ \wedge\ \id{SHS}\la \Events, \id{prec}, \id{state}\ra\\
	\ \ \Rightarrow\ \id{SHS}(\id{occurByDelta}_{P,e_1,\Delta,e}\la \Events, \id{prec}, \id{state}\ra)
\end{array}
\label{req:recursive def SHS by Delta}
\end{eqnarray}
	In practice, information systems build states incrementally.
	Each event comes with an increment ($\Delta$) that changes the current state.
	For this purpose we define an incremental history structure (IHS).
	A {\em incremental history structure} is a quadruple $\la\Events, \id{prec}, \delta, \iota\ra$ in which:
\begin{itemize}
\item	$\la \Events, \id{prec}\ra$ is a history structure;
\item	$\delta(e_1,e_2)$ represents a change, $\id{state}(e_2)\ominus\id{state}(e_1)$, for every pair $(e_1,e_2)\in\id{prec}$;
\item	$\iota(e)$ represents the change $\id{state}(e_1)\ominus\mathcal I$ for every event $e\in\Events$ that has no predecessor in $\id{prec}$.
\end{itemize}
	Since histories are built event-by-event, we define IHSs incrementally.
	For that purpose we use the following mapping, $\id{increment}_{P,e_1,e,\Delta,\iota}$,
	which adds one event ($e$) to an IHS:
\begin{eqnarray}
\begin{array}{l@{}l}
	\multicolumn{2}{l}{\id{increment}_{P,e_1,e,\Delta}\la\Events, \id{prec}, \delta, \iota\ra}\\
	\ \ =\ &\begin{array}[t]{@{}l@{\hspace{0.2cm}}c@{\hspace{0.2cm}}ll}
		\text{Let }\delta'(x,y)&=&\Delta\text{, }&\text{if }x=e_1\wedge y=e\wedge x\in P\\
				       &=&(\id{state}(e_1)\oplus\Delta)\ominus\id{state}(x)\text{, }&\text{if }x\neq e_1\wedge y=e\wedge x\in P\\
				       &=&\delta(x,y)\text{, }&\text{if }(x,y)\in\id{prec}
		\end{array}\\
		&\begin{array}[t]{@{}l@{\hspace{0.2cm}}c@{\hspace{0.2cm}}ll}
			\text{Let }\iota'(x)&=&\Delta\text{, }&\text{if }x=e\wedge P=\emptyset\\
				    &=&\iota(x)\text{, }&\text{if }x\neq e\wedge P=\emptyset
		\end{array}\\
		&\text{in }\la\Events\cup\{e\}, \id{prec}\cup\{(e_2,e)|e_2\in P\}, \delta', \iota'\}\ra
\end{array}
\label{req:increment}
\end{eqnarray}
	This definition inserts a new event, $e$, into a history structure with $e_1$ as its predecessor.
	The new state, $\id{state}(e)$, is computed by $\id{state}(e_1)\oplus\Delta$; let us call it $s$.
	Beside $e_1$, there may be other events preceding $e$ in the predecessor set $P$.
	The change from every predecessor $x$ can be computed by $s\ominus\id{state}(x)$.
	All these changes are added to the function $\delta$.
	The function $\iota$ is there for initialization purposes.
	To get an initial history structure (with no events in it) going,
	we must add events while there are no other events available as predecessor.
	We use the empty predecessor set as condition to signal that there is no previous state.
	The mapping $\id{increment}_{P,e_1,e,\Delta}$ can be used as recursive step in defining predicate $\id{IHS}$,
	which characterizes an incremental history structure:
\begin{eqnarray}
	&&\id{IHS}\la \emptyset, \emptyset, \emptyset, \emptyset\ra\\
&&\begin{array}{@{}l}
	P\subseteq\Events\ \wedge\ e\notin E\ \wedge\ \id{IHS}\la \Events, \id{prec}, \delta, \iota\ra\\
	\ \ \Rightarrow\ \id{IHS}(\id{increment}_{P,e_1,e,\Delta}\la \Events, \id{prec}, \delta, \iota\ra)
\end{array}
\label{req:recursive def IHS}
\end{eqnarray}
The following mapping transforms a stateful history structure to an incremental history structure:
\begin{eqnarray}
&&\begin{array}{l}
	\id{shs2ihs}(\la\Events, \id{prec}, \id{state}\ra)\\
	\ \ =\ \begin{array}[t]{@{}l}
		\text{Let }\forall(e_1,e_2)\in\id{prec}:\ \delta(e_1,e_2)=\id{state}(e_2)\ominus\id{state}(e_1)\\
		\text{Let }\id{initial}=\{e\in\Events|\forall e_1\in\Events: (e_1,e)\notin\id{prec}\}\\
		\text{Let }\forall e\in\id{initial}:\ \iota(e)=\id{state}(e)\ominus\mathcal I\\
		\text{in }\la\Events, \id{prec}, \delta, \iota\ra
		\end{array}
\end{array}
\label{req:shs2ihs}
\end{eqnarray}
	Vice versa, we can also map an incremental history structure to a stateful history structure:
\begin{eqnarray}
&&\begin{array}{l}
	\id{ihs2shs}(\la\Events, \id{prec}, \delta, \iota\ra)\\
	\ \ =\ \begin{array}[t]{@{}l}
		\text{Let }\forall(e_1,e_2)\in\id{prec}:\ \id{state}(e_2)=\id{state}(e_1)\oplus\delta(e_1,e_2)\\
		\text{Let }\id{initial}=\{e\in\Events|\forall e_1\in\Events: (e_1,e)\notin\id{prec}\}\\
		\text{Let }\forall e\in\id{initial}:\ \id{state}(e)=\mathcal I\oplus\iota(e)\\
		\text{in }\la\Events, \id{prec}, \id{state}\ra
		\end{array}
\end{array}
\label{req:ihs2shs}
\end{eqnarray}
	The\marginpar{claim} fact that $\id{shs2ihs}$ and $\id{ihs2shs}$ are each other's inverse proves that the choice to represent
	history by an IHS or SHS can be made on practical grounds.

	We must prove\marginpar{claim} that
\begin{equation}
	\id{ihs2shs}(\id{increment}_{P,e_1,e,\Delta}(\id{shs2ihs}(h)))\ =\ \id{occurs}_{P,e,\id{state}(e_1)\oplus\Delta}(h)
\end{equation}

\section{Isolation}
\label{sct:Isolation}
	How does a user perceive a distributed information system (DIS)?
	There are different options.
	An example is an information screen with flight arrival times in an airport,
	where a user sees the current state of flight arrivals and sees it change each time a new event occurs.
	This is an example of non-transactional use: the user consumes information passively.
	However, the topic of this paper is transactional use,
	an example of which is to make a flight reservation.
	In that example, the user interacts with the system for the purpose of reserving a seat on a flight.
	She sees a current state and provides events to the system by filling out fields in a reservation screen.
	These events cause changes in the state (e.g.\ from {\em The flight is not paid} to {\em The flight has been paid}).
	With this type of use, i.e transactional use, the user expects a predictable result.
	That is: if the current state is $s$ and the user specifies a change $\Delta$,
	the resulting state must be $s\oplus\Delta$.
	This requirement is the incremental variant of requirement~\ref{req:transaction}:
\begin{equation}
	h'=\id{occurByDelta}_{P,e_1,\Delta,e}(h)\ \Rightarrow\ \id{state}(e)\ =\ \id{state}(e_1)\oplus\Delta
\label{req:occurByDelta}
\end{equation}
	Since $\id{occurByDelta}$ preserves a stateful history structure (equation~\ref{req:recursive def SHS by Delta}),
	the user can interact repeatedly with the system and update the state predictably in a sequence of changes.
	This yields a user experience of being the only user of the system.
	That property is called {\em isolation}.

\section{Consistency}
\label{sct:Consistency}
	From a user perspective, {\em consistency} means that the system is free of contradictions.%
\footnote{A user might have wanted the system to provide the truth itself,
	but that is beyond what technical systems can guarantee. For people can feed the system with lies.
	However, freedom of contradictions is quite desirable as second-best, because truth is free of contradictions.}
	We require a predicate on states to determine whether a state is consistent.
	So we extend the notion of stateful historic structure with that predicate.

	A {\em nonconflicting history structure} is a quadruple $\la\Events, \id{prec}, \id{state}, \id{consistent}\ra$ in which
	$\la \Events, \id{prec}, \id{state}\ra$ is a SHS and $\id{consistent}$ is a predicate on $\mathbb S$,
	which holds for every state in the SHS.
	We will use predicate $\id{NHS}$ as a definition:
\begin{equation}
\begin{array}{l@{}ll}
	\multicolumn{3}{l}{\id{NHS}\la \Events, \id{prec}, \id{state}, \id{consistent}\ra}\\
	\ \Leftrightarrow\ &\id{SHS}\la \Events, \id{prec}, \id{state}\ra&\wedge\\
	&\id{consistent}\subseteq\mathbb S&\wedge\\
	&\forall e\in\Events:\ \id{consistent}(\id{state}(e))&
\end{array}
\label{def:NHS}
\end{equation}
	Equation~\ref{def:NHS} defines a nonconflicting history structure:
	For practical use, we require $\id{consistent}$ to be decidable.

\section{Atomicity}
\label{sct:Atomicity}
	{\em Atomicity} guarantees all-or-nothing outcomes.
	Let an example clarify this.
	If I purchase a song on the internet, I must pay.
	As a result, it is acceptable that the payment is done and the song is delivered.
	It is also acceptable that the payment is not done and the song is not delivered.
	However, it is unacceptable that the payment is done and the song is not delivered or vice-versa.

	Consider this:
	\[h'\ =\ \id{increment}_{P,e_2,e_3,\Delta_2}(\id{increment}_{P,e_1,e_2,\Delta_1}(h))\]
	Imagine that $\Delta_1$ represents my payment and $\Delta_2$ represents the delivery of my song.
	In the corresponding history structure, we will see that
	\[\{(e_1,e_2), (e_2,e_3)\}\subseteq\id{prec} \]
	It means that the overall state change caused by a sequence of events can be committed as a whole or rejected completely.

	So let us first discuss event sequences.
	Let us define an event sequence as a totally ordered history structure.
	Note that a history structure that is not totally ordered may contain multiple historic substructures
	that are totally ordered.
	Typically, event sequences occur multiply in a history structure.

\section{Durability}
\label{sct:Durability}
	In the eyes of the user, durability means that history is not changed.
	In the previous definitions durability has been secured by defining every change as an expansion of an existing system.
	Events are added only. Once added they are never removed or replaced.
	This corresponds to working practices in financial ledgers.
	Transactions are never removed, only compensated.

\section{Bibliography}
\bibliographystyle{elsarticle-harv}
\bibliography{doc}


\end{document}

\section{Threads}
	So how does a stateful history structure represent a distributed information system?
\marginpar{claim}
	Part of the answer is that a substructure of a stateful history structure is a stateful history structure by itself.

	The answer is that we have made no assumptions about location,
	like for instance that there is a shared memory.
	Even if we assume that each state exists in one location,
	a stateful history structure can still have multiple current states.
	Each of these states can exist in a separate location.

	The purpose of threads is to support deterministic computations.
	We consider any nonempty history structure that is totally ordered to be a thread.
	A {\em thread} is a history structure $\la \Events, \id{prec}\ra$ that satisfies
\begin{eqnarray}
	\forall e_1,e_2&:&e_1\in\Events\ \wedge\ e_2\in\Events\ \Rightarrow\ e_1\ \id{prec}+\ e_2\vee e_2\ \id{prec}^+\ e_1\label{eqn:order is total}\\
	\exists e&:&e\in\Events\label{eqn:nonempty thread}
\end{eqnarray}
	The total order (\ref{eqn:order is total}) is required to get deterministic results from a computation in a thread.
	In order to talk about the first and the last event in a thread, we require at least one event (\ref{eqn:nonempty thread}) in a thread.
	We shall use variable $t$ to represent threads.
	The first event in a thread $t$ is the infimum of the events in $t$;
	the last event is the supremum.

	We need the notion of state%
	\footnote{Cambridge Dictionary meaning of ``state'':
	a condition or way of being that exists at a particular time}
	to describe things that change over time.
	We shall use variables $s$, $s_1$, and $s_2$ to denote states.
	Let $\id{state}(e)$ represent the state after event $e$ has happened.

\section{Distributed Information System}
	
	Now equation~\ref{eqn:eval step} is the recursion step and the initial state is
	the starting point of the recursive computation of the final state of $t$.
	We consider a distributed information system as a set of processes that share a history.
	The system is distributed because processes need not share all histories.
	We need the notion of process%
	\footnote{Cambridge Dictionary meaning of ``process'':
		a series of actions that you take in order to achieve a result}
	to describe how components of a DIS interact.
	Let $\mathbb P$ be the set of all processes.
	We have formulated the following requirements:
	\begin{itemize}
	\item Belong/Contain\\
		To represent the fact that an event {\em belongs to} a process or
		(equivalently) that a process {\em contains} certain events,
		let $e\in p$ represent the statement that event $e$ belongs to process $p$.
	\item Shared history\\
		To abstract away from location, we make no assumptions with respect to the place where a process lives.
		However, to partake in a distributed system, processes must be able to share history.
		For that purpose we define what it means to be part of a history:
		\begin{equation}
			p\ \id{partof}\ h\ \Leftrightarrow\ (\forall e: e\in p\Rightarrow e\in h)
		\end{equation}
		We define an order between processes that share a history.
		\begin{equation}
			p_1\ \preceq\ p_2\ \Leftrightarrow(\forall e: e\in p_1\Rightarrow e\in p_2)
		\end{equation}
		We can prove that
		\begin{equation}
			p_1\ \preceq\ p_2\ \Rightarrow\ (\exists h:p_1\ \id{partof}\ h\wedge p_2\ \id{partof}\ h)
		\end{equation}
	\item Total order\\
		We require a process to have predictable behaviour, for which we need the process to contain a series (sequence) of events.
		So we require that $(\{e|e\in p\}, \preceq)$ is a total order.
		Apart from being a partial order, $\preceq$ must satisfy (for every $e_1$, $e_2$, and $p$):
		\begin{equation}
			e_1\in p\ \wedge\ e_2\in p\ \Rightarrow\ e_1\preceq e_2\vee e_2\preceq e_1
		\end{equation}
	\end{itemize}
	
\section{States}
	\begin{itemize}
	\item Revert\\
		A change can be used to compute a previous state.
		For this purpose we require an operator $\ominus$ such that
	\begin{equation}
		s_1\oplus\Delta=s_2\ \Leftrightarrow\ s_1=s_2\ominus\Delta
	\end{equation}
		As a consequence, we have for every event $e$:
	\begin{equation}
		\id{state}(\id{past}(e))=\id{state}(\id{pres}(e))\ominus\id{delta}(e)
	\end{equation}
		With $\preceq$ being reflexive, antisymmetric, and transitive,
		it follows immediately that $\oplus$ is idempotent, commutative, and associative.
	\end{itemize}



\section{Isolation}
\label{sct:Isolation}
{\em Isolation} means that actions can be done without any knowledge of what other users are doing simultaneously.

\section{Durability}
\label{sct:Durability}
{\em Durability} means that the contents of a database cannot change unintendedly.

\section{Bibliography}
\bibliographystyle{elsarticle-harv}
\bibliography{doc}


\end{document}

%% Obsolete

\section{Consistency}
\label{sct:Consistency}
Each user of a database issues a stream of events.
	The order (in time) of events is given by a relation\footnote{%
A relation $\declare{r}{A}{B}$ is a set of pairs. Each pair $\pair{a}{b}$ in that set satisfies $a\in A$ and $b\in B$}
	$\declare{pred}{Event}{Event}$,
	where $\pair{e1}{e2}\in\id{pred}$ means that event $e1$ precedes event $e2$.
	From a user perspective this stream is a sequence, so every event except the first has exactly one predecessor.
	
	For simplicity's sake, we assume the user sees a finite set of items at any given moment.
	The items seen by the user at any moment are given by a relation $\declare{state}{Event}{Item}$,
	where $\pair{e}{i}\in\id{state}$ means that item $i$ exists immediately after $e$ has happened.
	So the set of items visible to the user immediately after $e$ has happened can be computed by $\{ i | \pair{e}{i}\in\id{state} \}$.
	Note that the relation $\id{state}$ is interpreted in an open world.
	If $\pair{e}{i}\not\in\id{state}$ this does not mean $i$ does not exist.
	It just means that the pair $\pair{e}{i}$ is not in $\id{state}$, so the user will not see it.

	To cope with change, a user may insert and/or delete items at will.
	Let us specify these deletions and insertions by two relations, $\declare{ins}{Event}{Item}$ and $\declare{del}{Event}{Item}$,
	where $\pair{e}{i}\in\id{ins}$ means that pair $\pair{e}{i}$ is to be added to $\id{state}$,
	and $\pair{e}{i}\in\id{del}$ means that pair $\pair{e}{i}$ is to be removed from $\id{state}$.
	More precisely, let us consider an event $e$.
	The items $\{ i | \pair{e}{i}\in\id{state} \}$ are the items that exist immediately after the preceding event has happened,
	minus the items to be removed, plus the items to be inserted.
	The following equation defines this \define{behaviour}:
\begin{equation}
	\id{state}\ =\ (\id{pred};\id{state}-\id{del})\cup\id{ins}
\label{behaviour}
\end{equation}
	Equation~\ref{behaviour} says that the items visible to the user immediately after an event $e$
	are all items that are inserted together with items that were visible immediately after the preceding event, except those that were deleted.

	We define $A_{(\id{ins},\id{del})}$ as the function\footnote{The name $A$ is chosen as a reminder to the word ``action''.}
	that embodies this change:
\begin{equation}
	A_{(\id{ins},\id{del})}(x)\ =\ (x-\id{del})\cup\id{ins}
\end{equation}

	This raises the question whether we can compose two actions into a single action.
	For this, we derive:
\[\begin{array}{r@{~}l}
	&(((x-\id{del})\cup\id{ins})-\id{del}')\cup\id{ins}'\\
	=&\hspace{1in}\{x-y=x\cap\overline{y}\}\\
	&(((x\cap\overline{\id{del}})\cup\id{ins})\cap\overline{\id{del}'})\cup\id{ins}'\\
	=&\hspace{1in}\{\text{distribute $\cap$ over $\cup$}\}\\
	&(x\cap\overline{\id{del}}\cap\overline{\id{del}'})\cup(\id{ins}\cap\overline{\id{del}'})\cup\id{ins}'\\
	=&\hspace{1in}\{\text{De Morgan}\}\\
	&(x-(\id{del}\cup\id{del}'))\cup(\id{ins}-\id{del}')\cup\id{ins}'\\
	=&\hspace{1in}\{\text{Assume}\ \id{ins}\cap\id{del}=\emptyset\ \text{and}\ \id{ins}'\cap\id{del}'=\emptyset\}\\
	&(x-(\id{del}\cup\id{del}'))\cup(\id{ins}\cup\id{ins}')
\end{array}\]
	This yields composition ($;$) of $A$:
\begin{equation}
	A_{(\id{ins},\id{del})}\ ;\ A_{(\id{ins}',\id{del}')}\ =\ A_{((\id{ins}-\id{del}')\cup\id{ins}', \id{del}\cup\id{del}')}
\label{composition of A}
\end{equation}
	
	The composition operator turns out to be commutative, but under a condition.
	This will appear to be useful later on in this paper.
	Equation~\ref{composition of A} shows that we only need to investigate commutativity in the first argument of $A$: 
\[\begin{array}{r@{~}rcl}
	&A_{(\id{ins},\id{del})}\ ;\ A_{(\id{ins}',\id{del}')}&=&A_{(\id{ins}',\id{del}')}\ ;\ A_{(\id{ins},\id{del})}\\
	\Leftrightarrow\\
	&A_{((\id{ins}-\id{del}')\cup\id{ins}', \id{del}\cup\id{del}')}&=&A_{((\id{ins}'-\id{del})\cup\id{ins}, \id{del}'\cup\id{del})}\\
	\Leftrightarrow\\
	&(\id{ins}-\id{del}')\cup\id{ins}'&=&(\id{ins}'-\id{del})\cup\id{ins}\\
	\Leftrightarrow\\
	&(\id{ins}\cap\overline{\id{del}'})\cup\id{ins}'&=&(\id{ins}'\cap\overline{\id{del}})\cup\id{ins}\\
	\Leftrightarrow\\
	&(\id{ins}\cup\id{ins}')\cap(\overline{\id{del}'}\cup\id{ins}')&=&(\id{ins}\cup\id{ins}')\cap(\overline{\id{del}}\cup\id{ins})\\
	\Leftrightarrow\\
	&(\id{ins}\cup\id{ins}')-\overline{\overline{\id{del}'}\cup\id{ins}'}&=&(\id{ins}\cup\id{ins}')-\overline{\overline{\id{del}}\cup\id{ins}}\\
	\Leftrightarrow\\
	&\id{del}'\cap\overline{\id{ins}'}&=&\id{del}\cap\overline{\id{ins}}\\
	\Leftrightarrow\\
	&\id{del}'-\id{ins}'&=&\id{del}-\id{ins}
\end{array}\]
	For the purpose of making commutativity independent of its arguments,
	we must check the condition $\id{del}'-\id{ins}'=\id{del}-\id{ins}$.

\section{Isolation}
\label{sct:Isolation}
	If a user were alone,
	The moment a transaction starts, its user is isolated from other users.
	That moment is represented by a relation $\declare{beginTransaction}{Event}{Transaction}$,
	where $\pair{e}{t}\in\id{beginTransaction}$ means that event $e$ marks the start of transaction $t$.


\section{Ampersand}
\label{sct:Ampersand}
	In this section we explain the basics of Ampersand.
	The reader is expected to have sufficient background in relation algebra to understand the remainder of this paper.

	The core of an Ampersand script is a triple $\la\rules,\rels,\concepts\ra$,
	which consists of a set of rules $\rules$, relations $\rels$, and concepts $\concepts$.
	An Ampersand script is interpreted by the compiler as an information system.
	The rules constitute a theory in heterogeneous relation algebra.
	They constrain a body of data that resides in a database.

	The Ampersand compiler generates a database from relations in the script.
	A database application%
\footnote{Ampersand generates an application that consists of a relational database and interface components.
	Currently this application runs server-side on a PHP/MySQL platform and on a web-browser on the client-side.}
	assists users to keep rules satisfied throughout the lifetime of the database. It is also generated by Ampersand.

	A rule is a constraint in the form of an equality between two terms.
	Terms are built from relations.
	Ampersand interprets every relation as a finite set of pairs, which are stored in the database.
	The phase in which Ampersand takes a script and turns it into a database, is what we will refer to as \define{compile-time}.
	The phase in which a user interacts with the database, is what we will refer to as \define{run-time}.
	At run-time, Ampersand can decide which rules are satisfied by querying the database.
	The compiler generates all software needed to keep rules satisfied at run-time.
	If one of the rules is not satisfied as a result of data that has changed,
	that change is reverted (rolled back) to the original state.
	Either way, Ampersand ensures that all rules remain satisfied.
	
	Atoms are values that have no internal structure, meant to represent data elements in a database.
	From a business perspective, atoms are used to represent concrete items of the world,
	such as \atom{Peter}, \atom{1}, or \atom{the king of France}.
	By convention throughout the remainder of this paper, variables $a$, $b$, and $c$ are used to represent \emph{atoms}.
	The set of all atoms is called $\atoms$.
        Each atom is an instance of a \emph{concept}.

	Concepts (from set $\concepts$) are names we use to classify atoms in a meaningful way.
	For example, you might choose to classify \atom{Peter} as a person, and \atom{074238991} as a telephone number.
        We will use variables $A$, $B$, $C$, $D$ to represent concepts.
	The term $\ident{A}$ represents the \emph{identity relation} of concept $A$.
	The expression $a \in A$ means that atom $a$ is an \emph{instance} of concept $A$.
	The declaration of $A\isa B$ (pronounce: $A$ is a $B$)
	in an Ampersand script states that any instance of $A$ is an instance of $B$ as well.
	We call this {\em specialization}, but it is also known as {\em generalization} or {\em subtyping}.
	Specialization is needed to allow statements such as: ``An orange is a fruit that ....''.
	Specialization is not used in the remainder of this paper.

	Relations (from set $\rels$) are used in information systems to store data.
%	A \define{fact} is a statement that is true in a business context.
%	Facts are stored and kept as data in a computer.
	As data changes over time, so do the contents of these relations.
	In this paper relations are represented by variables $r$, $s$, and $d$.
	We represent the declaration of a relation $r$ by $\declare{nm}{A}{B}$,
	in which \id{nm} is a name and $A$ and $B$ are concepts.
	We call $A$ the source concept and $B$ the target concept of the relation.
	The pair $\pair{A}{B}$ is called the \emph{type} of the relation.
	The term $\fullt{A}{B}$ represents the \emph{universal relation} over concepts $A$ and $B$.

	The meaning of relations in Ampersand is defined by an interpretation function $\mathfrak{I}$.
	It maps each relation to a set of pairs.
	The type system of Ampersand guarantees that each pair in $r$ respects the type of $r$:
\begin{equation}
	\pair{a}{b}\in\ti{\declare{nm}{A}{B}} \Rightarrow\ a \in A \wedge b \in B \label{typing of relations}
\end{equation}
	The type system is strong, which means that every atom has a type.
	The type system is static, which means that type checking is entirely done at compile-time.

	Terms are used to combine relations using operators.
	The set of terms is called $\terms$.
\begin{definition}[terms]
\label{def:terms}
\item   The set of terms, $\terms$, is the smallest set that satisfies, for all $r,s \in \terms$, $d\in\rels$ and $A,B \in \concepts$: 
\begin{eqnarray}
	d&\in&\terms         \quad\quad\text{(every relation is a term)}\\
	(r \cup s)&\in&\terms\quad\quad\text{(union)}\\
	(r \cap s)&\in&\terms\quad\quad\text{(intersection)}\\
	(r-s)&\in&\terms     \quad\quad\text{(difference)}\label{def:difference}\\
	(r;s)&\in&\terms     \quad\quad\text{(composition)}\\
	(r\backslash s)&\in&\terms     \quad\quad\text{(right residual)}\\
	(r\slash s)&\in&\terms     \quad\quad\text{(left residual)}\\
	\flip r&\in&\terms   \quad\quad\text{(converse)}\\
	\cmpl{r}&\in&\terms   \quad\quad\text{(complement)}\\
%	\kleeneplus r&\in&\terms   \quad\quad\text{(Kleene closure)}\\
%	\kleenestar r&\in&\terms   \quad\quad\text{(Kleene closure)}\\
	\ident{A}&\in&\terms \quad\quad\text{(identity)}\\
	\fullt{A}{B}&\in&\terms \quad\quad\text{(full set)}
\end{eqnarray}
\end{definition}
	Throughout the remainder of this paper,	terms are represented by variables $r$, $s$, $d$, and $t$.
	The \define{type} of a term $r$ is a pair of concepts, given by $\tf{r}$.
	$\mathfrak{T}$ is a partial function that maps terms to types.
	If a term has one type, it is called \define{type correct}.
	The Ampersand compiler requires all terms to be type correct.
	If no type can be assigned, the compiler gives an error message.
	If multiple types are assignable to a term, the ambiguity is signalled by the compiler.
	In that case it will also emit an error message and prompt the programmer to disambiguate the term by adding type information.
	In practice, the derivation of types can disambiguate most terms.
	So, the programmer experiences some freedom to denote distinct relations by the same name without the obligation to specify their types at all occurrences.
	The compiler does not generate any code if there is a term that is not type correct.
	The type function and its restrictions are discussed in~\cite{Joosten2015}
	and have no consequences for the remainder of this paper.

 	The meaning of terms in Ampersand is given by an interpretation function $\mathfrak{I}$.
	Let $A$ and $B$ be finite sets of atoms, then $\mathfrak{I}$ maps each term to the set of pairs for which that term stands.
\begin{definition}[interpretation of terms]
\label{interpretation of terms}
\item   For every $A,B\in\concepts$ and $r,s\in\terms$
\begin{eqnarray}
	\ti{r}		 &=&\{\pair{a}{b}|\ a\ r\ b\}	\\
	\ti{r \cup s}	 &=&\{\pair{a}{b}|\ \pair{a}{b}\in\ti{r}\ \text{or }\ \pair{a}{b}\in\ti{s}\}	\\
	\ti{r \cap s}	 &=&\{\pair{a}{b}|\ \pair{a}{b}\in\ti{r}\ \text{and}\ \pair{a}{b}\in\ti{s}\}	\\
	\ti{r-s}	 &=&\{\pair{a}{b}|\ \pair{a}{b}\in\ti{r}\ \text{and}\ \pair{a}{b}\notin\ti{s}\}	\\
	\ti{r;s}	 &=&\{\pair{a}{c}|\ \text{for some}\ b,\ \pair{a}{b}\in\ti{r}\ \text{and}\ \pair{b}{c}\in\ti{s}\}	\\
	\ti{r\backslash s}	 &=&\{\pair{b}{c}|\ \text{for all}\ a,\ \pair{a}{b}\in\ti{r}\ \text{implies}\ \pair{a}{c}\in\ti{s}\}	\\
	\ti{r\slash s}	 &=&\{\pair{a}{b}|\ \text{for all}\ c,\ \pair{b}{c}\in\ti{s}\ \text{implies}\ \pair{a}{c}\in\ti{r}\}	\\
	\ti{\flip{r}}	 &=&\{\pair{b}{a}|\ \pair{a}{b}\in\ti{r}    \}	\\
	\ti{\cmpl{\declare{r}{A}{B}}}	 &=&\fullt{A}{B}-r	\\
%	\ti{\kleeneplus{r}}	 &=&\text{the smallest set that satisfies }\kleeneplus{r}=r\ \cup\ \kleeneplus{r};\kleeneplus{r}	\\
%	\ti{\kleenestar{\declare{r}{A}{A}}}	 &=&\ident{A}\cup\kleeneplus{r}\\
	\ti{\ident{A}} 	 &=&\{\pair{a}{a}|\ a\in A\}	\\
	\ti{\fullt{A}{B}}&=&\{\pair{a}{b}|\ a\in A, b\in B\}
\end{eqnarray}
\end{definition}
%	The Kleene closure operators (postfix $\kleeneplus{\ }$ and $\kleenestar{\ }$) have been implemented partially in the current Ampersand implementation.

	A \define{rule} is a pair of terms $r,s\in\terms$ with $\tf{r}=\tf{s}$, which is syntactically recognizable as a rule.
\[\text{RULE}\ r = s\]
	This means \(\ti{r} = \ti{s}\). In practice, many rules are written as:
\[\text{RULE}\ r\subs s\]
	This is a shorthand for 
\[\text{RULE}\ r\cap s = r\]
	We have enhanced the type function $\mathfrak{T}$ and the interpretation function $\mathfrak{I}$ to cover rules as well.
	If $\tf{r}=\tf{s}$ and $\tf{s}=\pair{A}{B}$:
\begin{eqnarray}
	\tf{\text{RULE}\ r = s}   &=&\pair{A}{B}\\
	\tf{\text{RULE}\ r\subs s}&=&\pair{A}{B}\\
 resolved this \end{eqnarray}

	We call a rule $\text{RULE}\ r = s$ \define{satisfied} when its interpretation equals $\ti{\fullt{A}{B}}$.
	As the population of relations used in $r$ changes with time, the satisfaction of the rule changes accordingly.
	A software developer, who conceives these rules, must consider how to keep each one of them satisfied.
	We call a rule \define{violated} if it is not satisfied.
	The set $\ti{(s-r)\cup(r-s)}$ is called the \emph{violation set} of \(\text{RULE}\ r = s\).
	To \define{resolve} violations means to change the contents of relations such that the rule is satisfied%
\footnote{To \define{restore invariance} is sometimes used as a synonym to resolving violations.
	To \define{preserve invariance} is sometimes used as a synonym to keeping a rule satisfied.}.
	Consequently, such a rule may also be referred to as \define{invariant}.
	Some ways of resolving violations are presented in section~\ref{sct:Code Fragments}.
	Each pair in the violation set of a rule is called a violation of that rule.

	The software developer must define how to resolve violations when they occur.
	She does so by inserting and/or deleting pairs in appropriately chosen relations.
	Whatever choice she makes, she must ensure that her code yields data that satisfies the rules.
	When we say: ``rule $r$ specifies this action'' we mean that satisfaction of rule $r$ is a postcondition of any action specified by rule $r$. 

\section{Software for Legal Reasoning}
\label{sct:Conceptual analysis}

% \begin{figure}[htb]
% \begin{center}
%   \includegraphics[angle=90,scale=.357]{LogicalDataModel.pdf}
% \end{center}
% \caption{Conceptual data model (from section~\ref{sct:Conceptual analysis})}
% \label{fig:conceptual model}
% \end{figure}

\section{Bibliography}
\bibliographystyle{elsarticle-harv}
\bibliography{doc}


\end{document}
