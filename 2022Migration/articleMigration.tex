\documentclass{elsarticle}
% \usepackage{graphicx}
\usepackage{hyperref}
%\usepackage{multicol}
%\usepackage{footmisc}
\usepackage{amstext}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{amsthm}
\usepackage[english]{babel}
%\usepackage[official,right]{eurosym}
\selectlanguage{english}
\hyphenation{ExecEngine}
\newtheorem{lemma}{Lemma}
\begin{document}
\include{preambleMigrations}

% TODO Algemeen:
% - type van triple-set en atom-set definieren zodat viol_u ?->P(?x?)
%   (Bas: ik heb foute types weggehaald, maar dit nog niet gedefinieerd)
% - nieuwe populatie na de disjoint union zou de populatie met ENFORCE regels toegepast moeten zijn
%  (en in het bijzonder de ISA regels)

% TODO Stef:
% - stukje literatuuronderzoek
% https://ieeexplore.ieee.org/abstract/document/7445334?casa_token=ECzi6XeV2ncAAAAA:KhWzB8XBFOUJ0C6AD-XjX_ryuA9ARvTd3gm6RR-ZNiR8sZ1858FJpQ7zKQhkAZDlv8IjPdgD
% https://ieeexplore.ieee.org/abstract/document/8549944?casa_token=9qiGqNzh2Q0AAAAA:C-cYogExB35nGxQdxLcdBh4JoLNvM0OHedAMhCbB5V4kb4_6nzHUvc23xSJbeoBu67LSiz-Y
% https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.651.9298&rep=rep1&type=pdf
% https://www.scirp.org/html/4-7800724_106592.htm
% https://www.researchgate.net/profile/Ranjana-Badre/publication/318665687_GUI_for_Data_Migration_and_Query_Conversion/links/5b45bbea0f7e9b1c722386e5/GUI-for-Data-Migration-and-Query-Conversion.pdf
% https://journal3.uin-alauddin.ac.id/index.php/literatify/article/view/12567

% Bewijsverplichtingen:
% -> het migratie-systeem is typefout-vrij (opmerking: als we de type-checker buiten beschouwing willen laten, kunnen we dit niet beschrijven)
% -> het migratie-systeem is vrij van overtredingen op regels die het migratie-systeem moet bewaken
% -> het migratie-systeem bevat alle oude data, ihb nog steeds na het toepassen van de enforce regels
% -> er is een pad naar ingebruikname van het nieuwe systeem (vanaf de initiÃ«le toestand van het migratie-systeem)
% -> corollary: op het moment van ingebruikname van het nieuwe systeem, is het migratie-systeem vrij van overtredingen op regels die het nieuwe systeem moet bewaken
% -> optioneel: na een begrensd aantal 'voortgangs-stappen' kan het nieuwe systeem in gebruik genomen worden
% -> optioneel: er bestaat een migratie-systeem dat de oude functionaliteit behoudt (mogelijk uitbreid) totdat het nieuwe systeem in gebruik genomen is?

\title{Data Migration under a Changing Schema}
\author[ou,ordina]{Stef Joosten\fnref{fn1}}
\ead{stef.joosten@ou.nl}
\author[umn]{Sebastiaan Joosten\fnref{fn2}}
\address[ou]{Open Universiteit Nederland, Heerlen, the Netherlands}
\address[ordina]{Ordina NV, Nieuwegein, the Netherlands}
\address[umn]{University of Minnesota, Minneapolis, USA}
\fntext[fn1]{ORCID 0000-0001-8308-0189}
\fntext[fn2]{ORCID 0000-0002-6590-6220}

\begin{abstract}
   Software generators can help to increase the frequency of releases and their reliability.
   They save on time spent on development and time spent on fixing human-induced mistakes by compiling a specification into a working information system.
   However, many generators do not support data migrations.
   A data migration is necessary when an incremental deployment changes the schema of the system.
   As a consequence, developers tend to avoid migrations or migrate data ``by hand''.

   To address this problem, this paper proposes a theory for data migrations aimed at automating the migration process.
   The problem at large is how to preserve the semantics of that data under a changing schema.
   This paper proposes a theory for deploying an incremental change.
   The theory is applicable in general, but will be implemented in a software generator called Ampersand.

   This paper aims to preserve the semantics of data by satisfying concrete business requirements.
   The migration process is based on the assumptions that
   software is deployed incrementally,
   the existing data set may be polluted,
   human interaction may be required,
   the meaning of data must be preserved,
   the business continues during the migration without interruption (zero down-time),
   and there is a compiler to generate an information system from a given schema.
   The correctness of the migration is the focus of this paper, while efficiency is outside its scope.
\end{abstract}

\begin{keyword}
generative software\sep incremental software deployment\sep data migration\sep relation algebra\sep Ampersand\sep schema change
\end{keyword}
\maketitle

\section{Introduction}
\label{sct:Introduction}
   The purpose of this work is to automate the data migrations that come with incrementally developing and deploying information systems.
   This paper proposes a theory for such data migrations in the context of generated information systems%
\footnote{In the sequel, the word ``system'' refers to the phrase ``information system''. This simplifies the language a little. }.

   This paper is founded in the belief that software generators help developers to increase the frequency of deploying successive increments.
   They also prevent some human-induced errors, both in the process and in the resulting software.
   We believe that software generation can thus lead to a higher pace of deployments, a more predictable deployment process, and more reliable information systems.

   A problem is that software generators typically do not support the data migration process, especially when the schema changes.
   The ``manual'' effort required to migrate data slows down the development process and introduces mistakes%
\footnote{These effects can be measured in terms of DevOps metrics such as
   deployment frequency,
   reliability of deployments, and
   change failure rate~\cite{DevOps2021}.}.

   This paper proposes a theory for data migrations that is meant to be implemented in a software generator called Ampersand~\cite{JoostenRAMiCS2017, Joosten-JLAMP2018}.
   It aims at automating data migrations, so incremental changes can be done more reliably and faster.
   This contributes to a more agile software development process.
   We believe that users will experience a more organic evolution of the system because increments are smaller, occur more frequently and more reliably.
   
   This paper assumes that the existing system and the desired system have different schemas.
   The existing system is the system that is already deployed in production,
   which makes it necessary to preserve the existing data.
   The desired system contains a number of changes and is meant to replace the existing system.

   Data migration for other purposes has been described in the literature.
   For instance, if a data migration is done for switching to another platform or to different technology,
   e.g.~\cite{Gholami2016,Bisbal1999},
   migration engineers may deliberately avoid schema differences and functionality changes to avoid introducing new errors in an otherwise error-prone migration process.
   For example, Ataei, Khan, and Walkingshaw~\cite{Ataei2021,Walkingshaw2014} define an increment as a variation between two data structures.
   They show how to unify databases with slight variations by preserving all variations in one comprehensive database.
   Obviously, such data migrations are not suitable for incremental software deployment,
   if the schema changes and the old functionality has to be replaced.
   Examples like these have convinced the authors to define a new method for data migration that is specifically meant for systems in production that are
   deployed in increments.
   
   The contribution of this paper is to derive a migration schema from two artifacts: an existing system
   (which has its own schema and data set) and the specification of a desired system (which has a different schema and will ``import'' relevant parts of the existing data).   
   Our theory is based on the following assumptions and requirements:
\begin{itemize}
   \item {\em incremental deployment}\\The data migration is meant to deploy a software increment in production.
   \item {\em data pollution}\\The existing data set may be polluted, but it satisfies its schema.
   \item {\em human intervention}\\The data migration may require human interaction, which may take time.
   \item {\em semantic continuity}\\the meaning of data must be preserved.
   \item {\em zero down-time}\\The business continues during the migration without interruption.
   \item {\em generative software development}\\A compiler exists to generate an information system from a given schema.
\end{itemize}
   This paper focuses on the correctness of the migration.
   Efficiency of the migration is beyond the scope.

\section{Analysis}
\label{sct:Analysis}
\subsection{Information Systems}
   The purpose of an information system is to make data meaningful to its users.
   The analysis of the problem starts with a formal definition of information systems,
   in which the schema contains all information that makes data meaningful.
   We then define a migration procedure from one information system to another,
   which preserves the meaning of the data as much as possible.

   Every user has her\footnote{Throughout this paper, we ask the reader to read ``she'' or ``her'' to include the masculin form as well.} own tasks and responsibilities
   and may work from different locations and on different moments.
   This collective use by multiple users serves a purpose which we will loosely call ``the business''.
   It is this business that depends on the semantics of the data to draw the right conclusions and carry out their tasks.
   This paper uses semantic constraints to represent this meaning.
   
   Information systems are typically used by actors (both users and computers) who are distributed and work with data all the time.
   As a consequence, the data in a system changes continually.
   In practice, actors ``talk to'' the system through an ingress mechanism, which connects each user to the right service(s).
   The ingress function is provided by the deployment platform, so it is beyond the scope of this paper.
   Also, every service runs independent of other services,
   meaning that each service can be stopped, (re)started and substituted without disrupting the other services.
   For practical purposes, this presumes a platform of managed services, such as Kubernetes,
   even though the theory in this paper is not specific to any platform.

   Every system contains a data set, which represents the state of the system.
   Every service produces and consumes events that may change the state of the system.
   This state is represented in a persistent store, aka the database%
\footnote{Whether the database is local, remote, or globally distributed is immaterial for the theory in this paper.}.
   Events that the system detects may cause the state to change.
\begin{figure}[bht]
   \begin{center}
     \includegraphics[scale=.45]{figures/datamigration-Pre-migration.png}
   \end{center}
\caption{Anatomy of an information system}
\label{fig:pre-migration}
\end{figure}
   To keep our theory technology independent, data sets are assumed to contains triples.
   This makes our theory valid for any kind of database that triples can represent,
   such as SQL databases, object-oriented databases, graph databases, triple stores, and other no-SQL databases.
   The system's semantics is represented as constraints,
   which most database management systems refer to as integrity rules.
   The purpose of these rules is to keep all constraints satisfied.

   Like most other software generators, Ampersand is a compiler that generates information systems from a script.
   Ampersand uses a syntactically sugared form of heterogeneous relation algebra,
   which works with relations in a way similar to Alloy~\cite{Alloy2006} and allegories%
\footnote{Allegories are a specific type of categories, which the reader does not need to understand for the purpose of this paper}~\cite{Zielinski2013}.
   Each rule in Ampersand is a constraint (aka invariant), which the system keeps satisfied as long as that rule lives%
\footnote{The dotted lines in the diagrams show which set of rules is enforced.}.
   So, developers spend no effort to extract constraints from specifications or (even more laborious) from code
   because all constraints are explicitly available as rules in the code.

   This, and the absence of imperative code in an Ampersand script, makes Ampersand a suitable platform on which to implement the theory.
   It also allows us to be explicit about ``preserving the meaning as much as possible''.
   An Ampersand script contains just enough information to generate a complete system,
   which means that a classical database schema (i.e.\ data structure plus semantics) can be extracted from the Ampersand script.

\subsection{Data Migrations}
   Data migration occurs when a desired system replaces an existing one,
   while preserving the meaning of the present data as much as possible~\cite{Spivak2012}.
   Just copying the set of data from the existing system to the desired system is obviously wrong if the schemas of both systems differ.

   In practice, data migrations are done by deploying the desired system and the existing system side-by-side,
   while transferring data in a controlled fashion.
   Typically, a migration strategy (e.g. case by case, customer by customer, or in batches) is in place
   to ensure the preservation of valuable data and the quality of the migration.
   To automate this process completely is unrealistic in many cases%
\footnote{Note that smaller increments improve the chance that a fully automic migration is possible,
although there will always be cases where human intervention is required.}.
   Manual interventions can be necessary to resolve data pollution, new business rules, or to fix known issues in the old system.
   Consequently, a migration engineer must have the freedom to change the migration script to suit the particulars of the migration.
   Such complications may arise mostly as a result of data pollution.
   We distinguish data pollution in the following categories:
\begin{itemize}
   \item data pollution that violates a constraint in the schema.
   \item data pollution that violates a constraint that is not in the schema.
   \item data pollution that cannot be captured in constraints on the data set,
   such as a street address that has become obsolete because someone has moved without notification.
   \item data pollution that is a consequence of an erroneous constraint in the schema.
\end{itemize}
   The first category is about violations of constraints from the schema.
   Such violations do not occur in our theory because the existing system was generated by Ampersand,
   so the data set satisfies all constraints in the schema.
   This sets our approach apart from other approaches to formalize data migration, e.g.~\cite{Thalheim2013}.
   The second category, data pollution that is not captured by any of the constraints mentioned in the schema,
   can be dealt with by adding constraints in the desired system.
   In that case, a migration engineer must ensure that the desired system can start with data that satisfies all constraints in its schema.
   So some human intervention may be necessary in specific cases.
   The third category of pollution must be dealt with by working procedures, to prevent such pollution as much as possible.
   In some cases, constraints on data can be formulated,
   so the system can help the user and simplify (or eliminate) the corresponding working procedure.
   So, such constraints may help to move this type of pollution to one of the other categories.
   The last category of data pollution, erroneous constraints, must be fixed by replacing them by the correct constraints in the desired system.

   This illustrates that automating data migrations may require user intervention,
   either by a migration engineer or by end-users.
   In our approach, such user interventions are given some time by
   defining two distinct moments:
\begin{enumerate}
   \item the moment of transition (the MoT), i.e. the moment that changed functionality is made available to users;
   \item the moment of cleanup (the MoC), i.e. the moment after which the old software and data can be removed.%
\footnote{Backup mechanisms are outside the scope of this paper.}.
\end{enumerate}

   During the time between the MoT and MoC,
   the existing system and the desired system are kept alive, side by side, as shown in figure~\ref{fig:migration phase}.
\begin{figure}[bht]
   \begin{center}
     \includegraphics[scale=.35]{figures/datamigration-Migration phase.png}
   \end{center}
\caption{Migration phase}
\label{fig:migration phase}
\end{figure}

   The migration requires a third schema, which specifies the migration itself.
   The migration schema comprises the schemas of both the existing system and the desired system.
   The rules of the migration schema will cause the right data to be copied correctly into the desired system.
   This migration schema preserves data from the existing system, it may introduce new data automatically (generated from the existing data set),
   and it may require users to introduce new data (which will take time). 
   So the idea of a migration schema is to compute the disjoint union of the existing and the desired schemas
   and to present it to the migration engineer in the form of source code,
   so the migration engineer can change everything she wants to suit the particulars of the data migration.

   During the migration phase, transactions in the existing system are allowed because the migration engine will transport them to the desired system.
   This allows the business to finish transactions in the existing system while the desired system is already up and running.
   Similarly, the existing system must execute transactions from the desired system, just in case the business calls off the migration.
   Rules in the migration system that require user interaction are given time by requiring that the migration phase ends
   only after all migration rules are at rest.
   At the MoC, the migration engineer takes down everything but the desired system,
   leaving the business with a successful migration.
\begin{figure}[bht]
   \begin{center}
     \includegraphics[scale=.35]{figures/datamigration-Post-migration.png}
   \end{center}
\caption{The system after the data migration}
\label{fig:post-migration}
\end{figure}

   The following section introduces the definitions required to migrate data from one system to another.

\section{Terminology}
\label{sct:Terminology}
   An {\em information system} is a combination of data set, schema, and functionality.
   For the purpose of this paper, the functionality is ignored because it does not impact the migration.
   Section~\ref{sct:Data sets} desribes data sets. Schemas are treated in section~\ref{sct:Schemas}.
   Then section~\ref{sct:Information Systems} defines information systems.

\subsection{Data sets}
\label{sct:Data sets}
   A data set $\dataset$ describes a set of structured data, which is typically stored persistently in a database of some kind.
   The notation $\dataset_{\infsys}$ refers to the data set of a particular information system $\infsys$.
   The purpose of a data set is to describe the data of a system at one point in time. 
   Before defining data sets, let us first define the constituent notions of atom, concept, specialization, relation, and triple.
   
   Atoms serve as data elements.
   Atoms are values without internal structure of interest, meant to represent atomic data elements (e.g. dates, strings, numbers, etc.) in a database.
   From a business perspective, atoms represent concrete items of the world,
   such as \atom{Peter}, \atom{1}, or \atom{the king of France}.
   By convention throughout the remainder of this paper, variables $a$, $b$, and $c$ represent \emph{atoms}.
   All atoms are taken from an infinite set called $\Atoms$.
   
   Concepts are names that group atoms of the same type.
   All concepts are taken from an infinite set $\Concepts$.
   $\Concepts$ and $\Atoms$ are disjoint.
   For example, a developer might choose to classify \atom{Peter} and \atom{Melissa} as \concept{Person},
   and \atom{074238991} as a \concept{TelephoneNumber}.
   In this example, \concept{Person} and \concept{TelephoneNumber} are concepts.
   In the sequel, variables $A$, $B$, $C$, $D$ will represent concepts.

   The relation $\inst:\Pair{\Atoms}{\Concepts}$ relates atoms to concepts.
   The term $a\inst C$ means that atom $a$ is an \emph{instance} of concept $C$.
   This relation is used in the type system, in which $\inst$ assigns one or more concepts to every atom in the data set.
   Since $\inst$ is a relation and every relation is a set of pairs,
   set operators $\cup$, $\cap$, and $-$ can be used on $\inst$.

%    Specialization, also known as {\em generalization} or {\em subtyping}, is a relation between concepts.
%    The statement $A\isa B$ (pronounce: $A$ is a $B$) states that any instance of $A$ is an instance of $B$ as well.
% \begin{equation}
%    \label{eqn:specialization}
%    A\isa B\ \Leftrightarrow\ \forall a: a\inst A\rightarrow a\inst B
% \end{equation}
%    Specialization is needed to allow statements such as: ``An employee is a person'' or ``A human is a mammal''.
%    In her script, a user can declare a specialization as a pair of concepts in the relation $\isa$.
%    A compiler can construct $\isa$ as the transitive, reflexive closure of all user-defined specializations.
%    It must make sure that $\isa$ is antisymmetric, so that $\isa$ is a partial order of concepts.
%    Specialization causes atoms to have multiple concepts of which they can be an instance.
%    As a result, if an atom is an instance of concept $A$ and $A\isa B$,
%    this atom has all properties that atoms of type $B$ have.

   Relations serve to organize and store data, to allow a developer to represent facts.
   In this paper, variables $r$, $s$, and $d$ represent relations.
   All relations are taken from an infinite set $\Rels$.
   $\Rels$ is disjoint from $\Concepts$ and $\Atoms$.
   Every relation $r$ has a name, a source concept, and a target concept.
   The notation $r=\declare{n}{A}{B}$ denotes that relation $r$ has name $n$, source concept $A$, and target concept $B$.
   The part $\pair{A}{B}$ is called the {\em signature} of the relation.

%    The following is for wide tables only
%    Specialization distributes in a straightforward way over signatures:
% \begin{equation}
%    \pair{A}{B}\isa\pair{C}{D}\ \Leftrightarrow\ A\isa C\ \wedge\ B\isa D
% \label{eqn:isa-signature}
% \end{equation}

   Triples serve to represent data.
   A triple\footnote{Please note that this paper uses the word {\em triple} in a more restricted way than in natural language.}
   is an element of $\Triple{\Atoms}{\Rels}{\Atoms}$.
   For example, $\triple{\text{\atom{Peter}}}{\declare{\id{phone}}{\tt Person}{\tt TelephoneNumber}}{\text{\atom{074238991}}}$ is a triple.

   Let $\Dataset$ be the set of all pairs $\pair{\triples}{\inst}$, so
   $\Dataset$ is $\Pair{(\Triple{\Atoms}{\Rels}{\Atoms})}{(\Pair{\Atoms}{\Concepts})}$.
   A data set $\dataset$ is a tuple $\pair{\triples}{\inst}$ that satisfies:
\begin{eqnarray}
   \triple{a}{\declare{n}{A}{B}}{b}\in\triples&\Rightarrow&a\inst A\ \wedge\ b\inst B
   \label{eqn:wellTypedEdge}
\end{eqnarray}
   Looking at the example,
   equation~\ref{eqn:wellTypedEdge} says that \atom{Peter} is an instance of {\tt Person} and \atom{074238991} is an instance of {\tt TelephoneNumber}.
   In practice, users can say that Peter has telephone number 074238991.
   So, the ``thing'' that \atom{Peter} refers to (which is Peter) has \atom{074238991} as a telephone number.
   This ``meaning from practice'' has no consequences in the formal world.
   Users are free to attach any practical meaning to a triple.

   The notations $\triples_{\dataset}$ and $\inst_{\dataset}$ are used to disambiguate $\triples$ and $\inst$ when necessary.
   To save writing in the sequel, the notation $a\ r\ b$ means that $\triple{a}{r}{b}\in\triples$.

   A relation $r$ can serve as a container of pairs,
   as defined by the function $\id{pop}_r:\Dataset\rightarrow\powerset{\Pair{\Atoms}{\Atoms}}$.
   It defines a set of pairs, also known as the population of $r$:
\begin{equation}
   \pop{r}{\dataset}\ =\ \{ \pair{a}{b}\mid\ \triple{a}{r}{b}\in\triples_{\dataset}\}
\label{eqn:pop-rel}
\end{equation}
   Equation~\ref{eqn:wellTypedEdge} implies that for every data set $\dataset$:
\[\pair{a}{b}\in\pop{\declare{n}{A}{B}}{\dataset}\ \Rightarrow\ a\inst_{\dataset}A\ \wedge\ b\inst_{\dataset}B\]
   For a developer, this means that the type of an atom depends only on the relation in which it resides; not on the actual population of the database.

   Concepts too can be seen as containers of atoms,
   defined by the function $\id{pop}_C:\Dataset\rightarrow\powerset{\Atoms}$.
\begin{equation}
   \pop{C}{\dataset}\ =\ \{ x\mid\ x\ \inst_{\dataset}\ C\}
\label{eqn:pop-concept}
\end{equation}

\subsection{Schemas}
\label{sct:Schemas}
   Schemas serve to capture the semantics of an information system~\cite{Spivak2012}.
   A schema defines concepts, specializations, relations, and rules.
   A software engineer defines a schema on design time, so that semantic checks can be implemented at compile time.

   A schema $\schema$ is a tuple $\triple{\concepts}{\rels}{\rules}$,
   in which $\concepts$ is a finite set of concepts,
   $\rels$ is a finite set of relations,
   and $\rules$ is a finite set of rules.
   Each rule in a schema serves to constrain the data set at runtime, to ensure its semantic integrity.
   Every rule is an element of an infinite set called $\Rules$,
   which is disjoint from $\Atoms$, $\Concepts$, $\Rels$, and $\Dataset$.
   In this paper, variables $u$ and $v$ represent rules.
   The notation $\schema_{\infsys}$ refers to the schema of information system $\infsys$.
   Disambiguation sometimes requires to write $\concepts_{\schema}$, $\rels_{\schema}$, and $\rules_{\schema}$
   rather than $\concepts$, $\rels$, and $\rules$ respectively.
   For every rule $u$ in the schema, there is a predicate $\sat{u}{\dataset}$ that constrains data set $\dataset$.
%  Every schema is an element of an infinite set called $\Schema$.

   A schema must ensure that triples ``make sense'' in the semantics defined by the schema.
   This means that it must satisfy:
\begin{eqnarray}
   \declare{n}{A}{B}\in\rels&\Rightarrow&A\in\concepts\ \wedge\ B\in\concepts
   \label{eqn:relationsIntroduceConcepts}
%    A\ \isa\ B&\Rightarrow&A\in\concepts\ \wedge\ B\in\concepts
%    \label{eqn:isasIntroduceConcepts}\\
%    The following is for wide tables only
%    \pair{A}{B}\isa\pair{C}{D}\ \wedge\ \declare{n}{C}{D}\in\rels&\Rightarrow&\declare{n}{A}{B}\in\rels
%    \label{eqn:subrelations}\\
%    \isa\ \text{is a partial order}
%    \label{eqn:isasPartialOrder}\\
%    \exists\dataset\in\Dataset-\emptyset:\ \sat{u}{\dataset}
%    \label{eqn:consistentRules}
\end{eqnarray}
   Requirement~\ref{eqn:relationsIntroduceConcepts} ensures that concepts mentioned in a relation are defined in the schema.
%    Requirement~\ref{eqn:isasIntroduceConcepts} ensures that the specialization relation works on concepts of $\concepts$.
%    The following is for wide tables only
%    Requirement~\ref{eqn:subrelations} ensures that all specializations of a relation exist.
%    Requirement~\ref{eqn:isasPartialOrder} ensures that the specialization relation contains no cycles.
%    Requirement~\ref{eqn:consistentRules} ensures that every rule is consistent.

   The type system of Ampersand~\cite{vdWoude2011} ensures that every information system it generates satisfies this requirement.
   Note that these requirements do not depend on any particular data set,
   so they can be checked at compile time.
   This is also known as static typing,
   which has well established advantages for the software engineering process~\cite{HanenbergKRTS14,Petersen2014}.

%    For every rule $u$ in the schema, we assume there is a function $\id{viol}_u : \Dataset\rightarrow\powerset{\Pair{\Atoms}{\Atoms}}$
%    that represents the set of violations%
% \footnote{The definition of ``violation'' is not needed in this paper. We only use the absence of violations to show satisfaction of a constraint.}
%    of rule $u$ in any data set.
%    We also assume a function $\id{sign} : \Rules\rightarrow\Pair{\Concepts}{\Concepts}$
%    that represents the signature of a rule, i.e. the type of atoms in violations:
% \begin{eqnarray}
%    \sign{u}=\pair{A}{B}\ \wedge\ \pair{a}{b}\in\viol{u}{\dataset}&\Rightarrow&a\inst A\wedge b\inst B
%    \label{eqn:wellTypedEdged violations}
% \end{eqnarray}
%    To determine whether rule $u$ satisfies data set $\dataset$,
%    we define $\sat{u}{\Dataset}$:
% \begin{eqnarray}
%    \sat{u}{\dataset}&\Leftrightarrow&\viol{u}{\dataset}=\emptyset
%    \label{eqn:sat}
% \end{eqnarray}
%    Violations are used to produce meaningful error messages and to trigger changes in the data set to restore invariance of that rule.

   % The maintainance relation between roles and rules is used to determine who is allowed to change the data set.
   % For every rule $u$ in the schema, there is a function $\id{viol}_u : \Dataset\rightarrow\powerset{\Pair{\Atoms}{\Atoms}}$
   % that represents the set of violations.%
   
   % In practice, there are different ways of enforcing rules: automatic enforcement and manual enforcement.

   % Automatic enforcement is specified by the developer with a special syntax:
   % the {\em enforcement rule}.
   % An enforcement rule specifies not only the rule,
   % but it implicitly defines event $e'$ to restore the invariance.
   % The system must have an engine that restores invariance of all enforcement rules,
   % so users will experience that these rules are always satisfied.

   % Manual enforcement means that the system (temporarily) allows violations,
   % to let a user ``invent'' a reaction $e'$ that restores invariance.
   % That is why the term $(\exists o\in\roles:\ o\maintain u)$ is added to requirement~\ref{eqn:satisfaction}.
   % It allows the compiler to generate code for users with role $o$,
   % so they can restore the invariant manually.
   % In the Ampersand language, a developer explicitly assigns a rule $u$ to a role $o$,
   % to keep control over the type of persons that are allowed to ``restore broken rules''.
   % As a consequence, a mechanism is needed to notify those persons of the work to be done.

   % If a rule $u$ is not assigned to any role and it is not an enforcement rule,
   % the system must prevent any event $e$ from having any effect on the database.
   % So, if $e$ would violate rule $u$, the system must reject $e$.
   % This implements blocking behaviour, and must be accompanied by an error message.

\subsection{Information Systems}
\label{sct:Information Systems}
   An information system has a changing data set because of events that occur.
   Whenever the system ``observes'' an event, it attempts to change its data set accordingly.
   However, every rule in the schema represents a constraint,
   which the data set must invariably satisfy.
   That is why rules are also known as ``invariants''.

   Events are taken from an infinite set, $\Events$.
   An event is denoted as $\evt{d^-}{d^+}$, in which $d^-$ and $d^+$ are data sets.
   Every event is a function $\Dataset\rightarrow\Dataset$, so $\evt{d^-}{d^+}(\dataset)$ is a data set.
   Equation~\ref{eqn:event} defines the effect of an event on a data set.
\begin{eqnarray}
      \evt{d^-}{d^+}(\dataset)&=&\la(\triples_{\dataset}-\triples_{d^-})\cup\triples_{d^+},(\inst_{\dataset}-\inst_{d^-})\cup\inst_{d^+}\ra
\label{eqn:event}
\end{eqnarray}
   In English, we would say that an event $\evt{d^-}{d^+}$ removes the triples in $d^-$ from $\dataset$ and then inserts the triples in $d^+$ into the result.
   In the sequel, variables $e$, $e'$, $e''$, etc. represent events.
   As soon as a system $\infsys$ observes an event $e$,
   it applies $e$ to its data set $\dataset$.
   Yet, if $e(\dataset)$ does not satisfy a rule $u$ from schema $\schema_\infsys$,
   another event is needed, say $e'$, to ensure that $\sat{u}{e'(e(\dataset))}$.
   The phrase ``to restore an invariant'' or ``to restore invariance of a rule''
   means that the system will change $e(\dataset)$ by applying another event $e'$ to satisfy requirement~\ref{eqn:enforce}.
   The notation $\enf_{u,e}$ represents this combination of events $e$ and $e'$.
   So, $\enf_{u,e}$ must satisfy:
\begin{eqnarray}
   \sat{u}{\dataset}&\Rightarrow&\sat{u}{\enforce{u,e}{\dataset}}
\label{eqn:enforce}
\end{eqnarray}

   Let us now define the notion of an information system.
\begin{definition}[information system]
\label{def:information system}
\item An information system $\infsys$ is a tuple $\triple{\dataset}{\schema}{\enf}$, in which
\begin{itemize}
   \item data set $\dataset=\pair{\triples}{\inst}$ is defined as in section~\ref{sct:Data sets};
   \item schema $\schema=\triple{\concepts}{\rels}{\rules}$ is defined as in section~\ref{sct:Schemas};
   \item enforce function ${\enf}:\Rules\rightarrow\Events\rightarrow\Dataset$ satisfies requirement~\ref{eqn:enforce}.
\end{itemize}
\end{definition}
   % A \define{role} is a name that identifies a group of users.
   % It serves as a placeholder for a person or a machine (i.e. an actor) that works with the data set (i.e. create, read, update, or delete triples).
   % The purpose of a role is to mention an individual user (human) or an automated actor (bot) without knowing who that user is.

   The information system itself enforces the semantics by ensuring that the following requirements remain true at all times:
\begin{eqnarray}
   \triple{a}{\declare{n}{A}{B}}{b}\in\triples&\Rightarrow&\declare{n}{A}{B}\in\rels
   \label{eqn:define R}\\
   \forall u\in\rules&:&\sat{u}{\dataset}
   \label{eqn:satisfaction}
% TODO:
% De engine heeft een eigen rol. Alle regels die aan de engine zijn gekoppeld, worden automatisch onderhouden.
%    The following is for wide tables only
%    \pair{A}{B}\isa\pair{C}{D}\ \wedge\ \triple{a}{\declare{n}{A}{B}}{b}\in\triples&\Rightarrow&\triple{a}{\declare{n}{C}{D}}{b}\in\triples
%    \label{eqn:subpopulations}
\end{eqnarray}
   Requirement~\ref{eqn:define R} ensures that the information system only works with triples whose relation is defined in the schema.
   Requirement~\ref{eqn:satisfaction} ensures that the data set satisfies all rules.
%    The following is for wide tables only
%    Requirement~\ref{eqn:subpopulations} ensures that specialization works across relations.
   Note that these requirements apply at runtime.
   So, as long as a rule $u$ ``lives'' in an information system, the data set will satisfy that rule.

   As the system registers events, it inserts or deletes triples in its data set.
   If such an event would violate requirement~\ref{eqn:satisfaction},
   either the system stays in the state $\dataset$ or there is a reaction $e'$ that restores the affected invariants:
\begin{equation}
   \sat{u}{\dataset}\Rightarrow\sat{u}{e'_{d'}(e_{d}(\dataset))}
   \label{eqn:restoring}
\end{equation}
   In this way, the system can restore invariants and keep requirement~\ref{eqn:satisfaction} satisfied.
   The combination of events $e$ and $e'$ is called a {\em transaction}.

\subsection{Example}
\label{sct:Example existing IS}
   Having defined an information system in mathematical terms, let us discuss a small example.
   It is written in the language Ampersand to make it more appealing to read.
   Let us first define a data set of just a handful of triples and three relations.
\begin{verbatim}
RELATION takes[Student*Course] =
[ ("Peter", "Management")
; ("Susan", "Business IT")
; ("John", "Business IT")
]
\end{verbatim}
   This declaration introduces a relation with the name \verb#takes#,
   source concept \verb#Student#, and
   target concept \verb#Course#.
   The informal meaning of this relation is that it states which students are taking which courses.

   The example system also has a second relation that states which modules are part of which course.
\begin{verbatim}
RELATION isPartOf[Module*Course] =
[ ("Finance", "Management")
; ("Business Rules", "Business IT")
; ("Business Analytics", "Business IT")
; ("IT-Governance", "Management")
]
\end{verbatim}
   The third relation states which students are enrolled for which module.
   It is left without population for now.
\begin{verbatim}
RELATION isEnrolledFor[Student*Module]
\end{verbatim}

   A rule, {\tt EnrollRule} completes the script.
   It states that a student can enroll for any module that is part of a course she takes.
   % In Ampersand, which is a syntactically sugared form of relation algebra~\cite{JoostenRAMiCS2017},
   % each rule has a name and each rule has a role to maintain its invariance:
\begin{verbatim}
RULE EnrollRule: isEnrolledFor |- takes;isPartOf~
\end{verbatim}
   The Ampersand compiler defines $\sat{\tt EnrollRule}{\dataset}$ as:
\begin{equation}
   \begin{array}{l}
   \forall \pair{s}{m}\in\pop{\tt isEnrolledFor}{\dataset}\ \exists c\in\text{\tt Course}:\\
   s\ \text{\tt isEnrolledFor}\ m\ \rightarrow\ s\ \text{\tt takes}\ c\ \wedge\ m\ \text{\tt isPartOf}\ c
   \end{array}
\label{eqn:example isEnrolledFor}
\end{equation}
   As relation $\declare{\tt isEnrolledFor}{\tt Student}{\tt Module}$ is empty, rule {\tt EnrollRule} is satisfied in this example.

   Now let us check the requirements to verify that this example defines an information system.
   % Requirement~\ref{eqn:specialization} is satisfied because this example contains no specialization.
   The Ampersand compiler generates a data set $\dataset$, which contains a set of triples and a relation $\inst$.
   It defines the set of triples $\triples$ as:
\[\begin{array}[t]{l}
   \triple{\text{\tt "Peter"}}{\declare{\text{\tt takes}}{\text{\tt Student}}{\text{\tt Course}}}{\text{\tt "Management"}}\\
   \triple{\text{\tt "Susan"}}{\declare{\text{\tt takes}}{\text{\tt Student}}{\text{\tt Course}}}{\text{\tt "Business IT"}}\\
   \triple{\text{\tt "John"}}{\declare{\text{\tt takes}}{\text{\tt Student}}{\text{\tt Course}}}{\text{\tt "Business IT"}}\\
   \triple{\text{\tt "Finance"}}{\declare{\text{\tt isPartOf}}{\text{\tt Module}}{\text{\tt Course}}}{\text{\tt "Management"}}\\
   \triple{\text{\tt "Business Rules"}}{\declare{\text{\tt isPartOf}}{\text{\tt Module}}{\text{\tt Course}}}{\text{\tt "Business IT"}}\\
   \triple{\text{\tt "Business Analytics"}}{\declare{\text{\tt isPartOf}}{\text{\tt Module}}{\text{\tt Course}}}{\text{\tt "Business IT"}}\\
   \triple{\text{\tt "IT-Governance"}}{\declare{\text{\tt isPartOf}}{\text{\tt Module}}{\text{\tt Course}}}{\text{\tt "Management"}}
\end{array}\]
The relation $\inst$ contains the pairs:
\[\begin{array}{l}
   \pair{\tt "Finance"}{\tt Module}\\
   \pair{\tt "Business Rules"}{\tt Module}\\
   \pair{\tt "Business Analytics"}{\tt Module}\\
   \pair{\tt "IT-Governance"}{\tt Module}\\
   \pair{\tt "Management"}{\tt Course}\\
   \pair{\tt "Business IT"}{\tt Course}\\
   \pair{\tt "Peter"}{\tt Student}\\
   \pair{\tt "Susan"}{\tt Student}\\
   \pair{\tt "John"}{\tt Student}
\end{array}\]
   The pair $\pair{\triples}{\inst}$ satisfies requirement~\ref{eqn:wellTypedEdge} so this is a data set $\dataset$ as introduced in section~\ref{sct:Data sets}.

   The Ampersand compiler generates a schema $\schema$, which contains concepts, a specialization relation, relations, and rules.
   It defines the sets of concepts, relations, and rules to obtain a schema $\schema$:
\[\begin{array}{rcl}
   \concepts&=&\{ {\tt Module}, {\tt Course}, {\tt Student}\}\\
   \rels&=&\{\begin{array}[t]{@{}l@{}}
               \declare{\tt takes}{\tt Student}{\tt Course},\\
               \declare{\tt isPartOf}{\tt Module}{\tt Course},\\
               \declare{\tt isEnrolledFor}{\tt Student}{\tt Module}\}
             \end{array}\\
   \rules&=&\{ {\tt EnrollRule}\}
  \end{array}
\]
   % The user has defined no specializations.
   % This means that $\isa$ equals the identity relation on $\concepts$ and requirements~\ref{eqn:isasIntroduceConcepts} and~\ref{eqn:isasPartialOrder} are satisfied.
   % The script defines the set of rules $\rules$ to contain just one rule: \verb-EnrollRule-.
   % The (nonempty) data set satisfies this rule, which meets requirement~\ref{eqn:consistentRules}.
   So, the schema $\schema=\triple{\concepts}{\rels}{\rules}$ satisfies all requirements from section~\ref{sct:Schemas}.

   Now let us check the definition of information system.
   Requirement~\ref{eqn:satisfaction} is satisfied because the only rule, {\tt EnrollRule} is satisfied.
   This satisfies definition~\ref{def:information system}, which establishes that $\infsys$ is an information system.
   % The set of roles is empty and so is the relation $\maintain$.
   % This meets all requirements for $\infsys=\triple{\dataset}{\schema}{\roles}{\maintain}$ from definition~\ref{def:information system}.
   % This concludes the argument that $\infsys$ is an information system.

%    Now suppose that the script also contains the following line:
% \begin{verbatim}
% ROLE Administrator MAINTAINS EnrollRule
% \end{verbatim}
%    Ampersand generates a set of roles $\roles=\{{\tt Administrator}\}$.
%    The relation $\maintain$ contains one pair only, $\pair{{\tt Administrator}}{{\tt EnrollRule}}$.
%    Requirement~\ref{eqn:satisfaction} is still satisfied because now there is a role to maintain rule {\tt EnrollRule}.
%    The run-time engine will no longer enforce satisfaction of rule {\tt EnrollRule}, but leave it up to an administrator.

\subsection{Process of Data Migration}
   Sections~\ref{sct:Information Systems} and~\ref{sct:Example existing IS} introduce a single information system.
   The transition from an existing system $\infsys$ to the desired one $\infsys'$ is the topic of this section.
   Let us first sketch the steps to migrate system $\infsys$ to $\infsys'$.
\begin{enumerate}
   \item Let the existing information system $\infsys$ have data set $\dataset$ and schema $\schema$.
         A developer defines a new information system, $\infsys'$, which contains a data set of its own, $\dataset'$, a schema $\schema'$, and some functionality\footnote{In this paper, we only consider the functionality captured by rules.} as illustrated in figure~\ref{fig:post-migration}.
         After the migration, the resulting system will be $\infsys'$, enhanced with the triples from $\dataset$ that satisfy $\schema'$,
         and with additional changes made by the migration engineer (see step~\ref{item3}) and by users who have been using the system during the migration.
         $\dataset'$ may contain some new data,
         for instance, to initialize new features at the moment of transition (MoT).
         If schemas $\schema$ equals $\schema'$, the data migration is unnecessary, so in the sequel we assume they are different.
   \item The migration engineer uses a software generator to generate a migration script, $\migrsys$,
         This is a system that includes both $\infsys$ and $\infsys'$.
         $\migrsys$ is meant to migrate the data and define the semantics during the migration phase.
         It contains enforcement rules that migrate existing data to the desired data set.
         It also contains data structures and rules that are needed for the duration of the migration phase.
%         The generated script must be free of type errors, to ensure that $\sat{u}{\dataset}$ exists for every rule $u$ and data set $\dataset$.
%         The proof is available \href{https://www.isa-afp.org/}{here}
   \item\label{item3}
         In many cases, the schema $\schema_{\migrsys}$ will be enough to perform the migration.
         However, a migration engineer may have specific requirements that call for changes in $\migrsys$.
         For that purpose, the generator generates the migration script as an Ampersand script,
         so the migration engineer can accommodate such requirements.
         This can be necessary, for example, to resolve data pollution that violates a constraint that was not in schema $\schema_{\infsys}$.
         She can either implement enforcement rules or use the business to resolve the issues.
         She can test the resulting migration script, $\migrsys'$ separately before taking the desired system to production.
         The migration engineer alters $\schema$ nor $\schema'$ while adapting $\migrsys'$ to the specific needs of the current migration.
   \item Then, the migration engineer deploys system $\migrsys'$.
         This event starts the copying of data from $\dataset$ to $\dataset'$, as specified in $\migrsys'$ (figure~\ref{fig:migration phase}).
         During this phase, the ingress is still referring users to the existing system,
         so they will hardly notice that the desired system is being fired up and loaded with data.
         Users are still changing $\dataset$ (both inserts and deletes),
         but these changes are being transferred to $\dataset'$ as well.
         This step is complete when all data from $\dataset$ has been copied to $\dataset'$ and all invariants of the migration system are true.
   \item At this moment, the ingress switches the incoming traffic from $\infsys$ to $\migrsys'$,
         so $\migrsys'$ effectively takes over.
         This event marks the MoT.
         Since $\infsys$ may still have some violations due to last-minute edits,
         it may take some (short but finite) time until the invariants of $\infsys$ are true.
         After that, dataset $\dataset$ will no longer change.
   \item From this moment, users experience the functionality of the migration system $\migrsys'$.
         This system must ensure that all invariants of $\schema'$ are true before the migration is complete.
         It no longer ensures the rules of $\schema$, which govern the existing system.
         Since $\dataset$ is no longer changing, the migration engineer can now focus on the remaining violations of $\schema'$.
         Users of the migration system will deal with process rules in the migration system that are still violated.
         Once all invariants of $\migrsys'$ are true, the migration is complete.
         This state marks the moment of cleanup (MoC)
   \item After the MoC, the ingress switches the incoming traffic from $\migrsys'$ to $\infsys'$.
         The migration system is no longer needed and can be taken out of production, together with the existing system $\infsys$.
         The desired system remains in production and the migration is finished.
         This establishes the post-migration situation (figure~\ref{fig:post-migration}).
\end{enumerate}

%SJC on Formalization: it's often better to put these inline where we first use them. Leaving this as a comment for easy copy-paste.
%    
%    \subsection{Formalization}
%    Let us define the instruments needed to describe the derivation of $\schema_{\migrsys}$.
%    \begin{definition}[union of data sets]
%    \label{def:graph_union_wellTyped_if_parts_wellTyped}
%    \[\pair{\triples_1}{\inst_1}\cup\pair{\triples_2}{\inst_2}\ =\ \pair{\triples_1\cup\triples_2}{\inst_1\cup\inst_2}\]
%    \end{definition}
%    
%    \begin{lemma}
%    \label{lemma:graph_union_wellTyped_if_parts_wellTyped}
%    If $\dataset_1$ and $\dataset_2$ are data sets, then $\dataset_1\cup\dataset_2$ is a data set.
%    \end{lemma}
%    
%    \begin{definition}[map relation names in data sets]
%    \label{def:map_labels_preserves_wellTypedness}
%    Let $h:\Rels\rightarrow\Rels$, then
%    \[\begin{array}{rcl}
%      \maprel{h}{\pair{\triples}{\inst}}&=&\pair{\maprel{h}{\triples}}{\inst}\\
%      \maprel{h}{\triple{a}{r}{b}}&=&\triple{a}{h(r)}{b}
%    \end{array}\]
%    \end{definition}
%    
%    \begin{lemma}
%    \label{lemma:map_labels_preserves_wellTypedness}
%    If $\dataset$ is a data set and $h:\Rels\rightarrow\Rels$ is an injective function, then $\maprel{h}{\dataset}$ is a data set.
%    \end{lemma}
%    
%    \begin{definition}[disjoint union of data sets]
%    \begin{eqnarray}
%    \pair{\triples}{\inst}\sqcup\pair{\triples'}{\inst'}&=&\pair{\triples\uplus\triples'}{\inst\uplus^2\inst'}\\
%      X\uplus Y&=&\{(x,0)\mid\ x\in X\}\ \cup\ \{(y,1)\mid\ y\in Y\}\\
%      X\uplus^2 Y&=&\begin{array}[t]{@{}l}\{((x_1,0),(x_2,0))\mid\ (x_1,x_2)\in X\}\ \cup\\ \{((y_1,1),(y_2,1))\mid\ (y_1,y_2)\in Y\}\end{array}
%    \end{eqnarray}
%    \end{definition}
%    In a disjoint union, the instances of the two data sets are relabeled to avoid name clashes.
%    In practice, we will not use 0 and 1 as labels for $x$ and $y$, but rather extend their names by prefixing.
%    \begin{lemma}
%    If $\dataset$ and $\dataset'$ are data sets, then so is $\dataset\sqcup\dataset'$.
%    \end{lemma}
%    This lemma is being proven in Isabelle/HOL~\cite{Isabelle}. The proof is published \href{location.domain}{here}
%    
%    \begin{definition}[disjoint union of schemas]
%    \begin{eqnarray}
%    \triple{\concepts}{\rels}{\rules}\sqcup\triple{\concepts'}{\rels'}{\rules'}&=&\triple{\concepts\uplus\concepts}{\rels\uplus\rels'}{\rules\uplus\rules'}
%    \end{eqnarray}
%    \end{definition}
%    % \begin{definition}[disjoint union of data sets]
%    % \begin{eqnarray}
%    %    \omit\rlap{$\la\atoms,\concepts,\inst,\isa,\rels,\dataset\ra\sqcup\la\atoms',\concepts',\inst',\isa',\rels',\dataset'\ra$}\notag\\
%    %    &=&\la\atoms\uplus\atoms',\ \concepts\uplus\concepts',\instuplus^2\inst',\ \isa\uplus^2\isa',\ \rels\uplus^3\rels',\ \dataset\uplus^5\dataset'\ra\notag\\
%    %       X\uplus Y&=&\{(x,0)\mid\ x\in X\}\ \cup\ \{(y,1)\mid\ y\in Y\}\\
%    %       X\uplus^2 Y&=&\begin{array}[t]{@{}l}\{((x_1,0),(x_2,0))\mid\ (x_1,x_2)\in X\}\ \cup\\ \{((y_1,1),(y_2,1))\mid\ (y_1,y_2)\in Y\}\end{array}\\
%    %       X\uplus^3 Y&=&\begin{array}[t]{@{}l}\{\declare{(n,0)}{(A,0)}{(B,0)}\mid\ \declare{n}{A}{B}\in X\}\ \cup\\ \{\declare{(n,1)}{(A,1)}{(B,1)}\mid\ \declare{n}{A}{B}\in Y\}\end{array}\\
%    %       X\uplus^5 Y&=&\begin{array}[t]{@{}l}\{\triple{(a,0)}{\declare{(n,0)}{(A,0)}{(B,0)}}{(b,0)}\mid\ \triple{a}{\declare{n}{A}{B}}{b}\in X\}\ \cup\\ \{\triple{(a,1)}{\declare{(n,1)}{(A,1)}{(B,1)}}{(b,1)}\mid\ \triple{a}{\declare{n}{A}{B}}{b}\in Y\}\end{array}
%    % \end{eqnarray}
%    % \end{definition}
%    \begin{lemma}
%    If $\schema$ and $\schema'$ are schemas, then so is $\schema\sqcup\schema'$.
%    \end{lemma}
%    The proof of this lemma is published \href{location.domain}{here}
%    
%    To implement the disjoint union, the following relabelling functions are needed:
%    \begin{definition}[relabel concepts]
%    \[\subst{C}{D}{A}\ =\ \text{\bf if}\ C=A\ \text{\bf then}\ D\ \text{\bf else}\ A\]
%    \end{definition}
%    \begin{definition}[relabel concepts in triples]
%    \[\subst{C}{D}{\triple{a}{r}{b}} = \triple{a}{\subst{C}{D}{r}}{b}\]
%    \end{definition}
%    \begin{definition}[relabel concepts in relations]
%    \[\subst{C}{D}{(\declare{n}{A}{B})} = \declare{n}{\subst{C}{D}{A}}{\subst{C}{D}{B}}\]
%    \end{definition}
%    \begin{definition}[relabel concepts in $\inst$]
%    \[a\ \subst{C}{D}{\inst}\ \subst{C}{D}{A}\ \Leftrightarrow\ a\inst A\]
%    \end{definition}
%    \begin{definition}[relabel concepts in data sets]
%    \[\begin{array}{rcl}
%      \rlap{$\subst{C}{D}{\pair{\triples}{\inst}}$}\\
%      &=&\pair{\{\subst{C}{D}{t}\mid t\in\triples\}}{\subst{C}{D}{\inst}}
%    \end{array}\]
%    \end{definition}
%    The relabeling of concepts in rules must satisfy:
%    \begin{equation}
%    \sat{u}{\dataset}\ \Leftrightarrow\ \sat{\subst{C}{D}{u}}{\subst{C}{D}{\dataset}}
%    \end{equation}
%    \begin{definition}[relabel concepts in schemas]
%    \begin{eqnarray}
%      \rlap{$\subst{C}{D}{\triple{\concepts}{\rels}{\rules}}$}\notag\\
%      &=&\begin{array}[t]{l@{}l}
%         \la&\{\subst{C}{D}{c}\mid c\in\concepts\}\\
%         ,&\{\subst{C}{D}{r}\mid r\in\rels\}\\
%         ,&\{\subst{C}{D}{u}\mid u\in\rules\}\ \ra\notag
%         \end{array}
%    \end{eqnarray}
%    \end{definition}
%    \begin{definition}[relabel relations]
%    \[\subst{\declare{m}{C}{D}}{m'}{{\declare{n}{A}{B}}}\ =\ \text{\bf if}\ n=m\wedge A=C\wedge B=D\ \text{\bf then}\ \declare{m'}{A}{B}\ \text{\bf else}\ \declare{m}{A}{B}\]
%    \end{definition}
%    \begin{definition}[relabel relations in triples]
%    \[\subst{r'}{m'}{\triple{a}{r}{b}} = \triple{a}{\subst{r'}{m'}{r}}{b}\]
%    \end{definition}
%    \begin{definition}[relabel relations in data sets]
%    \[\begin{array}{rcl}
%      \rlap{$\subst{r'}{m'}{\pair{\triples}{\inst}}$}\\
%      &=&\pair{\{\subst{r'}{m'}{t}\mid t\in\triples\}}{\inst}
%    \end{array}\]
%    \end{definition}
%    The relabeling of relation names in rules must satisfy:
%    \begin{equation}
%    \sat{u}{\dataset}\ \Leftrightarrow\ \sat{\subst{r'}{m'}{u}}{\subst{r'}{m'}{\dataset}}
%    \end{equation}
%    \begin{definition}[relabel relations in schemas]
%    \begin{eqnarray}
%      \rlap{$\subst{r'}{m'}{\triple{\concepts}{\rels}{\rules}}$}\notag\\
%      &=&\triple{\concepts}{\{\subst{r'}{m'}{r} \mid r\in\rels\}}{\{\subst{r'}{m'}{u} \mid u\in\rules\}}\notag
%    \end{eqnarray}
%    \end{definition}
%    
%    The relabeling of concepts is not trivial because of specialization.
%    To illustrate this point, suppose $D\isa A$.
%    Then the substitution $[C\rightarrow D]$ means that every atom $a$ that used to be of concept $C$ in the existing system, is now not only a $D$ but also an $A$.
%    For example, by relabeling the concept {\tt Hotel} to {\tt Motel}, in a situation where ${\tt Motel}\isa{\tt Parking}$,
%    all atoms that represent hotels might suddenly be expected to have parking lots.
%    So, whereas such examples may fly in a technical sense, the developer must use renaming with care to preserve the intended semantics.

\section*{Generating a Migration Script}
\subsection{Loading Data}
   Before explaining how to migrate data from $\dataset$ to $\dataset'$, we need to explain {\tt ENFORCE} rules.
   In Ampersand, an enforcement rule comes in three flavors:
\begin{eqnarray}
   \text{\tt ENFORCE}\ r\ \text{\tt :<}\ \id{term}\label{enforce del}\\
   \text{\tt ENFORCE}\ r\ \text{\tt :=}\ \id{term}\label{enforce eq}\\
   \text{\tt ENFORCE}\ r\ \text{\tt >:}\ \id{term}\label{enforce ins}
\end{eqnarray}
   The run time environment contains an engine that enforces such rules.
   The engine will change the population of relation $r$ by the least set of pairs needed to satisfy the rule.
   An enforcement rule with operator {\tt :<} (alternative~\ref{enforce del}) keeps the population of $r$ smaller than or equal to \id{term}.
   When the population of this term is losing pairs that are in $r$, the engine will remove such pairs from $r$ to keep $r$ smaller than or equal to \id{term}.
   An enforcement rule with operator {\tt >:} (alternative~\ref{enforce ins}) keeps the population of $r$ larger than or equal to \id{term}.
   When the population of this term is growing with pairs that are not in $r$, the engine will insert such pairs in $r$ to keep $r$ greater than or equal to \id{term}.
   An enforcement rule with operator {\tt :=} (alternative~\ref{enforce eq}) keeps the population of $r$ equal to \id{term} by inserting and deleting the right pairs in $r$.
   Summarizing, an enforcement rule can be read as an invariant that keeps one of $r\leq\id{term}$, $r=\id{term}$, or $r\geq\id{term}$ satisfied by changing the population of $r$.

   Now, let us study how to migrate data from $\dataset$ to $\dataset'$.
   It would be tempting to state that a relation from the example system, say {\tt takes}, could be copied by the following code fragment%
\footnote{The prefix {\tt old.} refers to concepts, relations, and rules from the existing system (section~\ref{sct:Example existing IS}).
The prefix {\tt new.} refers to items from the desired information system.}:
\begin{verbatim}
   ENFORCE new.takes >: old.takes
\end{verbatim}
   The relation `old.takes' resides in the existing system and will not change anymore.
   
   This means, however, that all pairs from {\tt old.takes} will be in {\tt new.takes} forever.
   This is undesirable because data in the existing system may still change.
   An extra relation (\ref{extraRelation}) fixes this problem by remembering which pairs have been copied:
\begin{eqnarray}
   &&\verb#RELATION copied_takes[Student*Course]#\label{extraRelation}\\
   &&\verb#ENFORCE new.takes >: old.takes - copied_takes#\label{difference}\\
   &&\verb#ENFORCE copied_takes := new.takes#\label{fill copies with new.takes}\\
   &&\verb#ENFORCE copied_takes :< old.takes#\label{del copies from old.takes}
\end{eqnarray}
   Understanding what the engine does to enforce these rules might help the reader, so let us look what happens at runtime.
   Initially, when {\tt copied\_takes} is still empty, the difference between {\tt old.takes} and {\tt copied\_takes} equals {\tt old.takes}.
   So, {\tt new.takes} will start filling up with the pairs of {\tt old.takes} by rule~\ref*{difference}.
   Then, the engine starts filling {\tt copied\_takes} with pairs from {\tt new.takes} because of rule~\ref*{fill copies with new.takes}.
   As a result, the difference between {\tt old.takes} and {\tt copied\_takes} gets smaller and smaller.
   Once {\tt old.takes} and {\tt copied\_takes} are equal, rule~\ref*{difference} will no longer insert pairs in {\tt new.takes}.
   While this copying process is going on, {\tt old.takes} may accumulate more pairs or lose pairs.

% Het volgende is wellicht iets teveel van het goede. Het lijkt me dat de lezer het voorgaande al wel snapt.
%    The following table summarizes what happens when pairs are inserted or deleted from {\tt old.takes} and {\tt new.takes}.
% \begin{itemize}
% \item If pair $p$ is inserted in {\tt old.takes} and $p$ is not in {\tt copied\_takes},
%       then rule~\ref{difference} causes the engine to insert $p$ in {\tt new.takes}.
%       Subsequently, rule~\ref{fill copies with new.takes} causes the engine to insert $p$ in {\tt copied\_takes} as well.
% \item If pair $p$ is inserted in {\tt old.takes} and $p$ is already in {\tt copied\_takes},
%       then nothing happens.
% \item If pair $p$ is deleted from {\tt old.takes} and $p$ is in {\tt copied\_takes},
%       then rule~\ref{del copies from old.takes} causes the engine to delete $p$ from {\tt copied\_takes}.
% \item If pair $p$ is deleted from {\tt old.takes} and $p$ is not in {\tt copied\_takes},
%       then nothing happens.
% \item If pair $p$ is inserted in {\tt new.takes},
%       rule~\ref{fill copies with new.takes} causes the engine to insert $p$ into {\tt copied\_takes}.
% \item If pair $p$ is deleted from {\tt new.takes},
%       rule~\ref{fill copies with new.takes} causes the engine to delete $p$ from {\tt copied\_takes}.
%       If this pair is in {\tt old.takes}, rule~\ref{difference} causes the engine to insert $p$ back into {\tt new.takes}.
% \end{itemize}

\subsection{Example}
   Let us proceed to specify a desired system, $\infsys'$, to illustrate a (toy) migration.
   The example in section~\ref{sct:Example existing IS}, called $\infsys$, serves as the existing system.
   Its population reflects the state of the system just before the migration.

   Information system $\infsys'$, the desired system, is specified by:
\begin{verbatim}
   RELATION takes[Student*Course]
   RELATION isPartOf[Module*Course] [UNI]
   RELATION isEnrolledFor[Student*Module]
   RULE EnrollRule: isEnrolledFor |- takes;isPartOf~
   
   RELATION course[ExamReg*Course] [UNI] =
      [ ("ER1", "Management")
      ; ("ER2", "Business IT")
      ; ("ER3", "Business IT")
      ]
   RELATION student[ExamReg*Student] [UNI] =
      [ ("ER1", "Peter")
      ; ("ER2", "Susan")
      ]
   RULE ExamRule1: student~;course |- takes
   RULE ExamRule2: student~;course;isPartOf~ |- isEnrolledFor
\end{verbatim}
   In this example, the relations and the rule of the existing system remain part of the desired system%
\footnote{Removal or changing of relations is allowed as well.}
   because the developer has copied these items into the script of the desired system.
   However, she has imposed the restriction \verb-UNI- on the relation {\tt isPartOf}, stating that this relation is univalent.
   This means that every \verb-Module- is part of at most one \verb-Course-.
   Besides, two new relations and two rules have appeared.

   The desired system, $\infsys'$, contains exam registrations (concept \verb-ExamReg-), which is new.
   In an exam registration, a student registers for the examination of a course.
   $\infsys'$ contains two extra rules: \verb-ExamRule1- and \verb-ExamRule2-.
   The first one says that an exam registration requires that a student actually takes the course.
   In logic, the constraint $p_{\tt ExamRule1}$ is written as:
\[\begin{array}{l}\forall s\in\text{\tt Student}, e\in\text{\tt ExamReg}, c\in\text{\tt Course}:\\
   e\ \text{\tt student}\ s\ \wedge\ \ e\ \text{\tt course}\ c\ \rightarrow\ s\ \text{\tt takes}\ c\\
\end{array}\]
   Another requirement, \verb-ExamRule2-, is that the student is enrolled for every module that is part of the course.
   Its constraint, $p_{\tt ExamRule2}$, is written in logic as:
\[\begin{array}{l}\forall s\in\text{\tt Student}, e\in\text{\tt ExamReg}, m\in\text{\tt Module}, c\in\text{\tt Course}:\\
   e\ \text{\tt student}\ s\ \wedge\ \ e\ \text{\tt course}\ c\ \wedge\ m\ \text{\tt isPartOf}\ c\ \rightarrow\ s\ \text{\tt isEnrolledFor}\ m
\end{array}\]
   Both rules are invariants.
   So, a student can register for an exam only if she is taking the course for which she registers and if she is enrolled for every module of that course.
   % Ampersand also derives the following violation sets:
% \[\begin{array}{l}
   % \viol{\text{\tt ExamRule1}}{\dataset}\\
   % \hspace{1cm}=\{\pair{s}{c}\mid\exists e:\ e\ \text{\tt student}\ s\ \wedge\ \ e\ \text{\tt course}\ c\ \wedge\neg(s\ \text{\tt takes}\ c)\}\\
   % \viol{\text{\tt ExamRule2}}{\dataset}\\
   % \hspace{1cm}=\{\pair{s}{m}\mid\exists e,c:\ e\ \text{\tt student}\ s\ \wedge\ \ e\ \text{\tt course}\ c\ \wedge\ m\ \text{\tt isPartOf}\ c\ \wedge\neg(s\ \text{\tt isEnrolledFor}\ m)\}
% \end{array}\]

\subsection{Migration script}
   The generated migration script, $\migrsys$, consists of the schema of the existing system $\infsys$,
   the migration system, and the desired system $\infsys'$.
   The migration system copies the data of the existing system to the desired system by the following rules:
\begin{verbatim}
-- The existing system (everything is prefixed with "old.")
   RELATION old.takes[Student*Course]
   RELATION old.isPartOf[Module*Course]
   RELATION old.isEnrolledFor[Student*Module]

   RULE old.EnrollRule:
      old.isEnrolledFor |- old.takes;old.isPartOf~
      
-- Migration system
   RELATION copied_takes[Student*Course]
   ENFORCE new.takes >: old.takes - copied_takes
   ENFORCE copied_takes >: new.takes

   RELATION copied_isPartOf[Module*Course]
   ENFORCE new.isPartOf >: old.isPartOf - copied_isPartOf
   ENFORCE copied_isPartOf >: new.isPartOf

   RELATION copied_isEnrolledFor[Student*Module]
   ENFORCE new.isEnrolledFor >:
           old.isEnrolledFor - copied_isEnrolledFor
   ENFORCE copied_isEnrolledFor >: new.isEnrolledFor

-- The desired system (everything is prefixed with "new.")
   RELATION new.takes[Student*Course]
   RELATION new.isPartOf[Module*Course] [UNI]
   RELATION new.isEnrolledFor[Student*Module]
   RULE new.EnrollRule:
      new.isEnrolledFor |- new.takes;new.isPartOf~
      
   RELATION new.course[ExamReg*Course] [UNI] =
      [ ("ER1", "Management")
      ; ("ER2", "Business IT")
      ; ("ER3", "Business IT")
      ]
   RELATION new.student[ExamReg*Student] [UNI] =
      [ ("ER1", "Peter")
      ; ("ER2", "Susan")
      ]

   RULE new.ExamRule1 :
      new.student~;new.course |- new.takes
   RULE new.ExamRule2 :
      new.student~;new.course;new.isPartOf~ |- new.isEnrolledFor
\end{verbatim}

\subsection{Preparing for the Moment of Transition}
   Just before starting the migration,
   the data in the existing system typically differs from the data at its inception (section~\ref{sct:Example existing IS}).
   Let us assume the actual population at the start of the migration is:
\begin{verbatim}
   POPULATION takes[Student*Course] CONTAINS
      [ ("Peter", "Management")
      ; ("Susan", "Business IT")
      ; ("John", "Business IT")
      ]
   POPULATION isPartOf[Module*Course] CONTAINS
      [ ("Finance", "Management")
      ; ("Business Rules", "Business IT")
      ; ("Business Analytics", "Business IT")
      ; ("IT-Governance", "Management")
      ; ("IT-Governance", "Business IT")
      ]
   POPULATION isEnrolledFor [Student*Module] CONTAINS
      [ ("Susan", "Business Analytics")
      ; ("Susan", "IT-Governance")
      ; ("Susan", "Business Rules")
      ]
\end{verbatim}

   Upon initialization, the migration system evaluates all enforce rules in an attempt to satisfy all invariants.
   In this example, it leaves two violations behind that it cannot resolve by enforce rules.
   The pair {\tt ("Peter", "Management")} is the first one; it violates rule {\tt new.ExamRule2}.
   This is because Peter has registered for an examination of the course Management,
   which contains the module Finance for which Peter is not enrolled.
   The other is {\tt "IT-Governance"}, which violates the univalence of relation {\tt new.isPartOf}.
   Since rule {\tt new.ExamRule2} and the univalence of {\tt new.isPartOf} are both defined as invariants,
   the desired system cannot be taken into production until these violations are resolved.
   Until that moment, the migration system remains operational to allow a user to resolve this violation. 
   The moment of cleanup is marked by the event that all invariants of the Migration system and the desired system are satisfied.
   At that moment, the migration system is no longer needed and can be removed.

\subsubsection{Strategies for restoring invariance}

We allow ExamRule2 to be broken during the transition to the desired system, and expect people with the Administrator role to deal with them.

However, the violations that arise when taking the (non-disjoint) union of the two scripts include violations of rules that the system should maintain.
This means that if we cannot generate software based on the disjoint union as it is.
To resolve this, the simplest solution is to change the roles assigned to the rules that are violated in the disjoint union:
\begin{verbatim}
   RELATION isPartOf[Module*Course]
   RULE isPartOfUNI: isPartOf;isPartOf~ |- I
   ROLE migrationHelper MAINTAINS isPartOfUNI
\end{verbatim}

Naturally, this solution requires effort from people with the role migrationHelper.
This in itself is not an issue: problems need to be solved one way or another.
However, a problem that may occur is that regular users of the system, who are used to the existing system, might be adding data to the system that causes violations faster than they can be solved.
Two solutions can mitigate this.

One solution is to state that the only violations that may occur in the relation \verb=isPartOfUNI= are those that were present when the migration started:
\begin{verbatim}
   RELATION isPartOf[Module*Course]
   RULE isPartOfUNI: isPartOf;isPartOf~ |- I
   ROLE migrationHelper MAINTAINS isPartOfUNI
   
   RELATION isPartOfUNI_violations[Module*Module]
     = [("IT-Governance", "IT-Governance")]
   RULE isPartOfUNI_progress: -(isPartOf;isPartOf~) /\ I
         |- isPartOfUNI_violations
   SYSTEM MAINTAINS isPartOfUNI_progress
   
   ENFORCE isPartOfUNI_violations :< -(isPartOf;isPartOf~) /\ I  
\end{verbatim}

In this first solution, the first rule is the same as with the simple solution.
The second rule prevents any new violation from occurring.
Existing violations are recorded in the \verb=isPartOfUNI_violations= relation, and the rule \verb=isPartOfUNI_progress= prevents the set of violations to \verb=isPartOfUNI= from growing beyond this.
The \verb=ENFORCE= statement at the end of this code snippet states that the relation recording existing violations should be shrunk whenever violations are solved.
This way, violations cannot re-occur.
As a whole, this means that \verb=migrationHelper= has a finite task.

As a second solution, the data in the designed system does not have any violations.
Consequentially, the cause of the violations comes from the triples in the existing system.
Rather than taking the union of all triples, the disjoint union is taken instead;
\begin{verbatim}
   RELATION isPartOf_old[Module*Course] [UNI]=
      [ ("Finance", "Management")
      ; ("Business Rules", "Business IT")
      ; ("Business Analytics", "Business IT")
      ; ("IT-Governance", "Management")
      ]
   RELATION isPartOf[Module*Course] [UNI]=
      [ ("IT-Governance", "Business IT") ]
   RELATION isPartOf_ignored[Module*Course]
   
   RULE isPartOf_isUnion:
         isPartOf_old |- isPartOf \/ isPartOf_ignored
   ROLE migrationHelper MAINTAINS isPartOf_isUnion
   
   ENFORCE isPartOf_ignored :> isPartOf
\end{verbatim}

\begin{verbatim}
   RELATION isPartOf_old[Module*Course] [UNI]=
      [ ("Finance", "Management")
      ; ("Business Rules", "Business IT")
      ; ("Business Analytics", "Business IT")
      ; ("IT-Governance", "Management")
      ]
   RELATION isPartOf[Module*Course] [UNI]=
      [ ("IT-Governance", "Business IT") ]
   RELATION isPartOf_ignored[Module*Course]

   ENFORCE isPartOf :> isPartOf_old - isPartOf_ignored
   ENFORCE isPartOf_ignored :> isPartOf
\end{verbatim}
In this second solution, we immediately move to a system in which the \verb=UNI= rule is maintained by the system.
The work for the \verb=migrationHelper= is again finite: the pairs in \verb=isPartOf_old= need to be copied over or explicitly ignored.
Once a pair is in \verb=isPartOf=, the pair is copied over to \verb=isPartOf_ignored= by the \verb=ENFORCE= statement, such that regular users can delete it without adding work to anyone with the \verb=migrationHelper= role.
Of course, since this solution starts with fewer pairs in the \verb=isPartOf= relation, other rules could be violated as a result of choosing this solution.
Indeed, ExamRule2 initially has more violations in this scenario.

\begin{definition}[migration data set]
   \begin{eqnarray}
      &&\begin{array}[t]{@{}l}
         \{ {\tt ENFORCE\ }\declare{(nm',1)}{(A',1)}{(B',1)}\\\quad{\tt\ :=\ }\ident{(A',1)};\declare{(nm,0)}{(A,0)}{(B,0)};\ident{(B',1)}\\
            \mid\ \begin{array}[t]{@{}l}
               \declare{nm}{A}{B}\in\rels\wedge\declare{nm'}{A'}{B'}\in\rels'\wedge\id{nm}=\id{nm'}\ \wedge\\
               \{\pair{A}{A'},\pair{B}{B'}\}\subseteq\isa\cup\isa'\cup\flip{\isa}\cup\flip{\isa'}\}
               \end{array}
           \end{array}\\
           &\cup&\begin{array}[t]{@{}l}
            \{ {\tt CLASSIFY\ }(C',1){\tt\ IS\ }(C,0)\mid\ C\in\concepts,C'\in\concepts',C=C'\}\\
           \end{array}
   \end{eqnarray}
\end{definition}

The ${\tt CLASSIFY\ }(C',1){\tt\ IS\ }(C,0)$ statement adds two pairs to the $\isa$ relation:
$\pair{(C',1)}{(C,0)}$ and $\pair{(C,0)}{(C',1)}$.
Note that if $\dataset$ is a data set, and $\dataset'$ is that data set with pairs added%
\footnote{TODO: het woord `add' is informeel, dit kan beter} to the $\isa$ relation, then the result is again a data set.
The ${\tt ENFORCE\ }r{\tt\ :=\ }e$ statements add triples $(x,r,y)$ for all $(x,y)\in e$.
The terms $t$ in these statements are such that $x\ (\inst \compose \kleenestar{\flip{\isa}})\ (A',1)$ and $y \ (\inst \compose \kleenestar{\flip{\isa}})\ (B',1)$, thus preserving the property that the result is a data set.

\begin{definition}[]
   \begin{eqnarray}
      \infsys\sqcup\infsys'&=&\triple{\roles'}{\rules'}{\maintain'}{\dataset\sqcup\dataset'}
   \end{eqnarray}
\end{definition}

\subsection{Changes}
% The purpose of this section is to explain why a data set is structured the way it is.
   In the migration of a data set we deal with changes to the elements that are not data:
   $\inst$, $\concepts$, and $\rels$.
   Such changes have further reaching consequences, however.
   Changes to $\inst$, $\concepts$, $\rels$ change the data structure in a data set.
   We define a \define{migration of a data set} $\la\atoms,\concepts,\inst,\rels,\dataset\ra$ as a change in which one or more of $\inst$, $\concepts$, or $\rels$ change.
   To satisfy the definition of data sets, $\dataset$ must change too,
   but the migration should try to preserve the maximal amount of data.

\section{Validation}
   To validate the theory, we have to prove that:
\begin{itemize}
   % -> het migratie-systeem is typefout-vrij (opmerking: als we de type-checker buiten beschouwing willen laten, kunnen we dit niet beschrijven)
   \item the migration script $\migrsys$ is free of type errors.
   % -> het migratie-systeem is vrij van overtredingen op regels die het migratie-systeem moet bewaken
   \item the migration system $\migrsys$ is an information system (definition~\ref{def:information system}),
         which implies that its population yields no violations.
   % -> het migratie-systeem bevat alle oude data, ihb nog steeds na het toepassen van de enforce regels
   \item $\migrsys$ contains all data of the existing system. So, $\dataset_\infsys\subseteq\dataset_\migrsys$.
   % -> er is een pad naar ingebruikname van het nieuwe systeem (vanaf de initiÃ«le toestand van het migratie-systeem)
   \item there is a well-founded progress measure towards releasing the desired system.
   % -> corollary: op het moment van ingebruikname van het nieuwe systeem, is het migratie-systeem vrij van overtredingen op regels die het nieuwe systeem moet bewaken
   \item When the progress measure indicates that further steps are not possible, the desired system $\infsys'$ is void of violations.
   The invariants of $\infsys'$ are cork rules in $\migrsys$.
   So, once all these rules are satisfied (and secured with a cork),
   all invariants in $\infsys'$ are satisfied and ready for deployment.
   % -> optioneel: na een begrensd aantal 'voortgangs-stappen' kan het nieuwe systeem in gebruik genomen worden
\end{itemize}
% Bewijsverplichtingen:

\section{Software architecture}
   Since functionality is excluded from this paper, we discuss the software architecture of the back-end of the migration system.
   For this research, we have used the Ampersand compiler to generate the system(s).
   Figure~\ref{fig:initial} shows the starting point of the migration.
\begin{figure}
   \centering
   \includegraphics[width=0.8\textwidth]{figures/Before migration.png}
   \caption{Initial state of the migration}
   \label{fig:initial}
\end{figure}
   For the purpose of data migration, the existing system must make its schema available through its API.
   For this purpose, the back-end offers a service ``retrieve schema'', which provides the schema as Ampersand source-code.
   As a result, the migration engineer can rely on the schema of any generated application to be accurate.
   For this reason, we can safely assume that the schema this service provides has not been changed since it was deployed.
   Its presence allows the generator to build various front-ends and migration scripts during the lifetime of the existing system%

   After the migration engineer has produced and tested a migration script, the Ampersand compiler produces a migration system.
   It includes the schemas of both the existing system and the desired system, besides having a schema of its own.
   This is implemented by Ampersand's INCLUDE mechanism, which boils down to taking the disjoint union of the schemas.
\begin{figure}
   \centering
   \includegraphics[width=0.8\textwidth]{figures/During migration.png}
   \caption{During the migration}
   \label{fig:during}
\end{figure}
   Deployment of the migration system (figure~\ref{fig:during}) leaves the existing system intact.
   It enhances the system with the data set needed by the migration system, which includes the data set needed by the desired system.
   So, users will notice no difference in the existing system when the migration system is deployed.

   When all rules in the migration script have been satisfied, the migration system is ready to be deployed.
   At this point, the ingress switches traffic from the existing system to the migration system.
   Until this moment, users have been updating data in the existing system, which is continuously being migrated to the desired system.
   After the ingress switch is made, users have the new functionality at their disposal and make their changes in the desired system.
   If for some reason, there is remaining migration work, the migration system will execute this.
   Since there is no more input in the existing system, the migration system will eventually be consistent.
   At this point, the existing system can be removed, which leaves the migration system in place.


   The main purpose of the migration script is to define the migration rules that are used to migrate data from the existing system to the desired system.
   During this process, users continu to use the existing system.

\begin{figure}
   \centering
   \includegraphics[width=0.8\textwidth]{figures/After migration.png}
   \caption{After the migration}
   \label{fig:after}
\end{figure}

\section{Conclusions}
\begin{itemize}
   \item the part of the data that does not change can be migrated automatically;
   \item the part of the migration that can be automated is usually not sufficient;
         it takes additional human creativity to complete the migration specification;
\end{itemize}
\section{Bibliography}
\bibliographystyle{elsarticle-harv}
\bibliography{doc}


\end{document}
