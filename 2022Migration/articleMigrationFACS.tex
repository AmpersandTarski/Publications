% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%

\usepackage{hyperref}
%\usepackage{multicol}
%\usepackage{footmisc}
%\usepackage{amstext}
\usepackage{amsmath}
\usepackage{amssymb,stmaryrd}
\usepackage{mathrsfs}
\usepackage{mathtools}
%\usepackage{amsthm}
\usepackage[english]{babel}
%\usepackage[official,right]{eurosym}
\selectlanguage{english}
\hyphenation{ExecEngine DevOps}
%\newtheorem{lemma}{Lemma}


% Ampersand -----------------------------------------------------------

%\def\id#1{\text{\it #1\/}}
\newcommand{\xrightarrowdbl}[2][]{%
  \xrightarrow[#1]{#2}\mathrel{\mkern-14mu}\rightarrow
}
\newcommand{\id}[1]{\text{\it #1\/}}
\newcommand{\code}[1]{\text{\tt\small #1}}
\newcommand{\stmtText}[1]{``{\small\tt #1}''}
\newcommand{\dom}[1]{\id{dom}(#1)}
\newcommand{\cod}[1]{\id{cod}(#1)}
%\renewcommand{\int}[2]{\id{inter}(#1,#2)}
\newcommand{\relsIn}[1]{\id{relsIn}(#1)}    % maps a Term to a set of Relations
\newcommand{\popF}[1]{\id{pop}_{#1}}
\newcommand{\pop}[2]{\popF{#1}(#2)}
\newcommand{\maintain}{\mathbin{\id{maint}}}
\newcommand{\enf}{\mathbin{{\tt enforce}}}
\newcommand{\enforce}[2]{{\tt enforce}_{#1}(#2)}
\newcommand{\instance}{\mathbin{\id{inst}}}
\newcommand{\relname}[1]{\id{relname}(#1)}
\newcommand{\evt}[2]{\id{event}_{#1,#2}}
\newcommand{\src}[1]{\id{src}(#1)}
\newcommand{\tgt}[1]{\id{tgt}(#1)}
\newcommand{\viol}[2]{\violC{#1}(#2)}
\newcommand{\violC}[1]{\id{viol}_{#1}}
\newcommand{\sign}[1]{\id{sign}_{#1}}
\newcommand{\enfRel}[1]{\id{enf}_{#1}}
\newcommand{\signature}[2]{\langle{#1},{#2}\rangle}
\newcommand{\powerset}[1]{\cal{P}\{#1\}}
\newcommand{\la}{\langle}
\newcommand{\ra}{\rangle}
\newcommand{\full}{V}
\newcommand{\declare}[3]{\id{#1}_{\pair{#2}{#3}}}
\newcommand{\subst}[3]{#3_{[#1\rightarrow #2]}}
\newcommand{\fullt}[2]{V_{\pair{#1}{#2}}}
\newcommand{\iden}{I}
\newcommand{\ident}[1]{I_{\id{#1}}}
\newcommand{\expr}[3]{(#1)_{#2\times #3}}
\newcommand{\pair}[2]{\langle{#1},{#2}\rangle}
\newcommand{\maprel}[2]{{\tt maprel}_{#1}({#2})}
\newcommand{\Pair}[2]{#1\times#2}
\newcommand{\pairs}[1]{\id{pairs}(#1)}
\newcommand{\triple}[3]{\langle{#1},{#2},{#3}\rangle}
\newcommand{\quintuple}[5]{\langle{#1},{#2},{#3},{#4},{#5}\rangle}
\newcommand{\atom}[1]{{\tt\small #1}}
\newcommand{\atoms}{\mathcal{A}}
\newcommand{\Atoms}{\mathbb{A}}
%\newcommand{\events}{\mathcal{E}}
%\newcommand{\Events}{\mathbb{E}}
\newcommand{\concept}[1]{{\tt\small #1}}
\newcommand{\concepts}{\mathcal{C}}
\newcommand{\Concepts}{\mathbb{C}}
\newcommand{\decls}{\mathcal{D}}  %% names of relations
\newcommand{\rels}{\mathcal{R}}   %% all relations
\newcommand{\Rels}{\mathbb{R}}   %% all relations
\newcommand{\relations}{\mathcal{M}} % representing terms. M is a subset of R.
\newcommand{\triples}{\mathcal{T}}
\newcommand{\Triples}{\mathbb{T}}
\newcommand{\Triple}[3]{#1\times#2\times#3}
\newcommand{\vertices}{N}
\newcommand{\rules}{\mathcal{U}}
\newcommand{\transactions}{\mathcal{E}}
\newcommand{\busConstraints}{\mathcal{B}}
\newcommand{\Constraints}{\mathbb{U}}
\newcommand{\specrules}{\mathcal{S}}
\newcommand{\roles}{\mathcal{O}}
\newcommand{\dataset}{\mathscr{D}}
\newcommand{\Dataset}{\mathbb{D}}
\newcommand{\schema}{\mathscr{Z}}
\newcommand{\functionality}{\mathscr{F}}
\newcommand{\select}[2]{\id{select}_{#1}\{{#2}\}}
\newcommand{\migrsys}{\mathscr{M}}
\newcommand{\infsys}{\mathscr{S}}
\newcommand{\tf}[1]{\mathscr{T}(#1)}
\newcommand{\ptf}[1]{\mathscr{T}'(#1)}
\newcommand{\ti}[1]{\mathscr{I}(#1)}
\newcommand{\tic}[1]{I_{\cal C}(#1)}
\newcommand{\relAdd}{\dagger}
\newcommand{\flip}[1]{{#1}^\smallsmile} %formerly:  {#1}^\backsim
\newcommand{\kleeneplus}[1]{{#1}^+}
\newcommand{\kleenestar}[1]{{#1}^*}
\newcommand{\cmpl}[1]{\overline{#1}}
\newcommand{\rel}{\times}
\newcommand{\compose}{;}
\newcommand{\subs}{\subseteq}%{\models}
\newcommand{\fun}{\rightarrow}
\newcommand{\isa}{\preceq}
%\newcommand{\isaClos}{\sqsubseteq}
\newcommand{\typetest}{?}
\newcommand{\meet}{\sqcap}
\newcommand{\join}{\sqcup}
\newcommand{\Meet}{\bigsqcap}
\newcommand{\Moin}{\bigsqcup} % because LaTeX has already defined command \Join.
\newcommand{\order}{\ominus}
\newcommand{\anything}{\top}
\newcommand{\nothing}{\bot}
\newcommand{\rewriteto}{\rightarrow}
\newcommand{\calc}{\implies}
\newcommand{\alland}{\bigwedge}
\newcommand{\mph}[3]{#1_{#2\times #3}}
\newcommand{\mphu}[1]{#1_{\univ\times\univ}}

%-----------------------------------------
\newcommand{\kse}{\hspace*{1.7em}}
\newcommand{\ksf}{\hspace*{1em}}
\newcommand{\ksg}{\hspace*{1em}}
\newenvironment{derivation}{\begin{tabbing}\kse \= \ksf \= \ksg \= \kill}{\end{tabbing}}
%\newtheorem{definition}{Definition}
\newcommand{\term}[1]{\>\>\(#1\)\\[1ex]}
\newcommand{\rela}[2]{\>\(#1\)\>\>\{ \ #2 \ \}\\[1ex]}
\newcommand{\weg}[1]{}

\def\define#1{\label{dfn:#1}{\em #1}\index{#1}}
\def\definem#1{\label{dfnm:#1}{\em #1}\index{#1}\marge{#1}}
\newcommand{\marg}[1]{\index{#1}\marge{#1}}



\usepackage{color}
\renewcommand\UrlFont{\color{blue}\rmfamily}
%



\begin{document}
%

\title{Data Migration under a Changing Schema in Ampersand}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Sebastiaan Joosten\inst{1}\orcidID{0000-0002-6590-6220}\\ \and
Stef Joosten\inst{2,3}\orcidID{0000-0001-8308-0189}}
%
\authorrunning{S. Joosten}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{University of Minnesota, Minneapolis, USA \and Open Universiteit, Heerlen, the Netherlands\\ 
\email{stef.joosten@ou.nl} \and
Ordina NV, Nieuwegein, the Netherlands}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
   Software generators can help increase the frequency of releases and their reliability.
   They save on time spent on development and fixing human-induced mistakes by compiling a specification into a working information system.
   However, many generators do not support data migrations.
   Data migration is necessary when an incremental deployment changes the system's schema.
   Consequently, developers tend to avoid migrations or migrate data ``by hand''.

   To address this problem, this paper proposes a theory for data migrations aimed at automating the migration process.
   The problem at large is how to preserve the semantics of that data under a changing schema.
   This paper proposes a theory for deploying incremental change.
   The theory is applicable in general but will be implemented in a software generator called Ampersand.

   This paper defines a migration process that preserves the semantics of data by means of invariants (constraints)
   with zero downtime for users.
   The migration process is based on assumptions and requirements that aim to capture a large class of data migrations in practice.
   This paper focuses on correctness of the migration. Efficiency of the migration is outside its scope.
\keywords{Generative software \and Incremental software deployment \and Data migration \and Relation algebra \and Ampersand \and Schema change. \and Invariants \and Zero downtime.}
\end{abstract}

\section{Introduction}
\label{sct:Introduction}
   In practice, information systems%
   \footnote{In the sequel, the word ``system'' refers to the phrase ``information system'', to simplify the language a little.}
   may live for many years.
   After they are built, they need to be updated regularly to keep up with changing requirements in a dynamically evolving environment.
   Roughly half of the DevOps~\cite{BassWeberZhu15} teams that responded in a worldwide survey in 2023~\cite{HumanitecDevOps2023} are deploying software more frequently than once per day.
   Obviously, these deployments are mostly updates of existing systems.
   Schema changes cannot always be avoided when updating software, so a {\em schema changing data migration} (SCDM) will be neccessary from time to time.
   For example, adding or removing a column to a table in a relational database adds to the complexity of migrating data.
   Even worse, if a system invariant changes, some of the existing data in the system may violate the new invariant.
   The risk and effort of such data migrations explains why development teams try to avoid schema changes at all costs.

   Data migration for other purposes than schema change has been described in the literature.
   For instance, if a data migration is done for switching to another platform or to different technology,
   e.g.~\cite{Gholami2016,Bisbal1999},
   migration engineers can and will avoid schema changes and functionality changes to avoid introducing new errors in an otherwise error-prone migration process.
   Another example, Ataei, Khan, and Walkingshaw~\cite{Ataei2021,Walkingshaw2014}, defines a migration as a variation between two data structures.
   They show how to unify databases with slight variations by preserving all variations in one comprehensive database.
   This does not cater for schema changes, however.
   Then there are SCDMs in situations without a schema or an implicit schema, e.g.~\cite{Hillenbrand2022}.
   If the schema is not available at compile time, the work will have to be done at runtime.
   This requires versioned storage of production data and an overhead in performance.
   This paper focuses on SCDMs in situations with an explicit schema that changes with the data migration.
   The prototypical use case for that is to release updates of information systems in production,
   where the semantic integrity of data must be preserved across schema changes.
   Another use case is application integration for multiple dispersed data sources with explicit schemas.

   In practice, data migrations typically follow Extract-Transform-Load (ETL) patterns~\cite{Theodorou2017},
   for which many tools are available.
   However, invariants that change require writing code in the transform phase,
   for which ETL tools typically provide little support.
   This yields extra work for software engineers, and it increases the risk of software errors.
   The ubiquitous pressure to decrease deploy times calls for further automation of SCDMs.
   With the release frequencies increasing and deploy times decreasing, any downtime for the sake of migration becomes less and less acceptable.
   That is why our research aims for zero downtime.

   Another practical problem is that of data quality.
   Migrations typically suffer from a backlog of deteriorated data, incurring work to clean it up.
   Some of that work must be done before the migration; some can wait till after the migration.
   We can capture part of the data quality problem as a requirement to satisfy semantic constraints.
   E.g. the constraint that the combination of street name, house number, postal code, and city occurs in
   a registration of valid addresses can be checked automatically.
   In a formalism like Ampersand, which allows us to express such constraints, we can add constraints for data quality to the schema.
   This allows us to signal the data pollution at runtime.
   Some forms of data pollution cannot be detected in this way, however.
   An example is when a person has deliberately specified a false name without violating any constraint in the system.

   The next section analyzes SCDMs with an eye on zero downtime and data quality.
   It sketches the outline of a procedure for SCDMs.
   Section~\ref{sct:Definitions} formalizes the concepts that we need to define the procedure.
   Section~\ref{sct:Generating} defines the algoritm for generating a migration system, to automate SCDMs.
   This algorithm has been proven correct in Isabelle/HOL.
   The outline of this proof is discussed in section~\ref{sct:Proof} and the proof itself is published on the internet.

   For experimental validation we have used the language Ampersand~\cite{JoostenRAMiCS2017,Joosten-JLAMP2018}
   because its syntax and semantics correspond directly to the definitions in section~\ref{sct:Definitions}.
   In this way, prototypes built in Ampersand have helped to refine the theory.

\section{Analysis}
\label{sct:Analysis}
   This section analyzes information systems qualitatively, to prepare for a formal treatment in section~\ref{sct:Definitions}.
   The current section yields a procedure for migrating data from one system to another.
\subsection{Information Systems}
   The purpose of an information system is to store and disclose data in a way that is meaningful to its users.
   Multiple users, working from different locations and on different moments, constitute what we will loosely call ``the business''.
   The data in the system constitutes the collective memory of the business,
   which relies on the semantics of the data to draw the right conclusions and carry out their tasks.
\begin{figure}[bht]
   \begin{center}
     \includegraphics[scale=.45]{figures/datamigration-Pre-migration.png}
   \end{center}
\caption{Anatomy of an information system}
\label{fig:pre-migration}
\end{figure}

   Actors (both users and computers) are changing the data in a system continually.
   The state of the system is represented by a data set, typically represented in some form of persistent store such as a database.
   Events that the system detects may cause the state to change.
   To keep our theory technology independent, we assume data sets to contain triples.
   This makes our theory valid for every kind of database that triples can represent,
   including SQL databases, object-oriented databases, graph databases, triple stores, and other no-SQL databases.

   We assume that constraints implement the business semantics of the data.
   Constraints represent business concerns formally, so they can be checked automatically and be used to generate software.
   Figure~\ref{fig:pre-migration} depicts them as rules.
   Some constraints must be satisfied through human intervention, while others are being satisfied by the system.
   So, we distinguish different kinds of constraint:
\begin{enumerate}
\item Blocking invariant\\
   A \define{blocking invariant} is a constraint that is always true in a system.
   It serves to constrain the data set at runtime.
   When it is about to be violated, the system produces an error message and refuses the transaction.
\item Transaction\\
   The classical database transaction can be seen as an invariant,
   which the system keeps satisfied by adding or deleting triples to the dataset.
   As soon as the data violates this constraint, the system restores it without human intervention.
   So, the outside world experiences a constraint that is always true, i.e.~an invariant.
\item Business constraint\\
   A \define{business constraint} is a constraint that users can violate temporarily until someone restores it.
   Example: ``An authorized manager has to sign every purchase order.''
   Every violation requires some form of human action to satisfy the business constraint (e.g. "sign the purchase order").
   That takes some time, during which the constraint is violated.
   So, we do not consider business constraints to be invariant.
\end{enumerate}
   Summarizing, in our notion of information systems, concepts, relations, and constraints carry the business semantics.
   Of the three types of constraint, only blocking invariants and transactions are invariants. 

\subsection{Ampersand}
   Ampersand is a language that specifies information systems as a system of concepts, relations, and constraints.
   It features the types of constraints that we need for this paper, so we have used it to try our theory in practice.
   Ampersand has a compiler that generates the information system from a script.
   A developer specifies constraints in heterogeneous relation algebra~\cite{Hattensperger1999,Alloy2006}.
   The generated system keeps the invariants satisfied and signals violations of business constraints to a user.
   Having all invariants explicit in the Ampersand source code makes Ampersand a suitable platform on which to implement our theory.
   The absence of imperative code in an Ampersand script makes it easier to reason about the system.
   The type system of Ampersand~\cite{vdWoude2011} features static typing,
   which has well established advantages for the software engineering process~\cite{HanenbergKRTS14,Petersen2014}.
   Constraints carry the business semantics, so they allow us to be explicit about ``preserving the meaning as much as possible''.
   An Ampersand script contains just enough information to generate a complete system,
   which means that a classical database schema (i.e.\ data structure plus constraints) can be extracted from the Ampersand script.

\subsection{Zero downtime}
   To make the case for zero downtime, consider this problem:
   Suppose we have an invariant, $u$, in the {\em desired system}, which is not part of the {\em existing system}.
   In the sequel, let us call this a {\em new invariant}.
   Now, suppose the data in the existing system does not satisfy $u$.
   If $u$ is a transaction, the desired system will restore it automatically.
   But if it is a blocking invariant, the desired system cannot spin up because all of its invariants must be satisfied.
   To avoid downtime, we must implement new blocking invariants initially as a business constraint,
   to let users do the work.

   For this purpose, we define an intermediate system, the \define{migration system},
   which contains all concepts, relations, and constraints of both the existing and the desired system.
   However, it implements the blocking invariants of the desired system as business constraints.
   This migration system must also ensure that every violation that is fixed is blocked from recurring.
   In this way, the business constraint gradually turns into a blocking invariant, satisfying the specification of the desired system.
   Since the number of violations is finite, the business can resolve these violations in finite time.
   In this way, the migration system bridges the gap and users get a zero downtime SCDM.

   Summarizing, the following requirements apply to SCDMs
\begin{enumerate}
\item users must experience zero downtime, to enable more frequent SCDMs.
\item users must be able to fix invariants in the new system.
   So, we need an intermediate ``migration system'' that implements blocking constraints of the desired system as business constraints,
   in order to deliver zero downtime.
\item the number of violations that users must fix is finite and decreases (monotonically) over time,
   to ensure that the migration system does not need to live ever after.
\end{enumerate}

\subsection{Data Migrations}
   Data migration occurs when a desired system replaces an existing one,
   while preserving the meaning of the present data as much as possible~\cite{Spivak2012}.
   %Just copying the set of data from the existing system to the desired system is obviously wrong if the schemas of both systems differ.
   %
   In practice, data migrations typically deploy the existing system and the desired system side-by-side,
   while transferring data in a controlled fashion, as shown in Figure~\ref{fig:migration phase}.
\begin{figure}[bht]
   \begin{center}
     \includegraphics[scale=.35]{figures/datamigration-Migration-phase.png}
   \end{center}
\caption{Migration phase}
\label{fig:migration phase}
\end{figure}
   To automate the migration as much as possible and to achieve zero downtime,
   we must deploy a third system: the migration system.
   This system has its own schema.
   It comprises the schemas of both the existing system and the desired system,
   so the migration system can copy the data from the existing to the desired system.
   It uses transactions to copy the data and to resolve some forms of data pollution.
   Not all migration work, however, can be automated.
   Data pollution, new business rules, or known issues in the existing system
   may occasionally require tailoring the migration script to specific needs
   that require action of users in production.
   For that purpose, the migration engineer specifies business constraints in the migration system.
   In Ampersand, a developer can use business constraints to implement such needs.

   Before a migration starts we assume that the existing system is up and running and all of its invariants are satisfied.
   The ingress is directing traffic to the existing system.
   The data migration has three distinct steps:
\begin{description}
\item[Pre-deploy]
      The migration engineer deploys both the migration system and the desired system with their initial data,
      to allow both systems to satisfy their initial invariants before going live.
      The existing data is still in the existing system and the ingress still directs all traffic to the existing system.
      So, users notice no difference yet.
      The migration system starts copying data from the existing system.
\item[moment of transition (MoT)]
      After the migration system is done copying data, the migration engineer switches all traffic to the migration system
      and deploys the new functionality at the same time.
      This is the moment users will notice the difference.
      Even though the migration system may look like the desired system in the eyes of an average user,
      it relaxes the blocking invariants and keeps the system operational.
      Since the existing system receives no more traffic, its activity will come to a halt and its data will become static.
      The migration system stays active until all invariants of the desired system are satisfied and it can take over the work from the migration system.
\item[moment of completion (MoC)]
      Once the invariants of the desired system are satisfied, the migration engineer switches all traffic to the desired system.
      After that, she can safely remove both the migration system and the existing system, including their data.
\end{description}

   Transactions in the existing system that happen during the time that the migration system is copying data cause no problem,
   because their changes are copied by the migration system too.
   However, after the MoT there must be no new changes in the existing system
   to avoid violations of new invariants that the migration system has already fixed.

   The following section introduces the definitions required to migrate data from one system to another.

\section{Definitions}
\label{sct:Definitions}
   An {\em information system} is a combination of data set, schema, and functionality.
   For the purpose of this paper, we ignore functionality captured in user interfaces because it does not impact the migration.
   Section~\ref{sct:Data sets} describes how we define data sets.
   Section~\ref{sct:Constraints} defines constraints and their violations.
   Schemas are treated in section~\ref{sct:Schemas}.
   Then section~\ref{sct:Information Systems} defines information systems.

\subsection{Data sets}
\label{sct:Data sets}
   A data set $\dataset$ describes a set of structured data, which is typically stored persistently in a database of some kind.
   The notation $\dataset_{\infsys}$ refers to the data set of a particular system $\infsys$.
   The purpose of a data set is to describe the data of a system at one point in time. 
   Before defining data sets, let us first define the constituent notions of atom, concept, relation, and triple.
   
   Atoms serve as data elements.
   %Atoms are values without internal structure of interest, meant to represent atomic data elements (e.g. dates, strings, numbers, etc.) in a database.
   %From a business perspective, atoms represent concrete items of the world,
   %such as \atom{Peter}, \atom{1}, or \atom{the king of France}.
   %By convention throughout the remainder of this paper, variables $a$, $b$, and $c$ represent \emph{atoms}.
   All atoms are taken from an infinite set called $\Atoms$.
   %
   Concepts are names that group atoms of the same type.
   All concepts are taken from an infinite set $\Concepts$.
   %$\Concepts$ and $\Atoms$ are disjoint.
   For example, a developer might choose to classify \atom{Peter} and \atom{Melissa} as \concept{Person},
   and \atom{074238991} as a \concept{TelephoneNumber}.
   In this example, \concept{Person} and \concept{TelephoneNumber} are concepts.
   Moreover, \atom{Peter}, \atom{Melissa} and \atom{074238991} are atoms.
   In the sequel, variables $A$, $B$, $C$, $D$ will represent concepts, and variables $a$, $b$, and $c$ represent \emph{atoms}.
   %
   The relation $\instance:\Pair{\Atoms}{\Concepts}$ relates atoms to concepts.
   The term $a\instance C$ means that atom $a$ is an \emph{instance} of concept $C$.
   %This relation is used in the type system, in which $\instance$ assigns one or more concepts to every atom in the data set.
   %Since $\instance$ is a relation and every relation is a set of pairs,
   %set operators $\cup$, $\cap$, and $-$ can be used on $\instance$.

   Relations serve to organize and store data, allowing developers to represent facts.
   In this paper, variables $r$, $s$, and $d$ represent relations\footnote{Some readers might like to read `relation symbol' where we write `relation'.}.
   All relations are taken from an infinite set $\Rels$.
   $\Rels$ is disjoint from $\Concepts$ and $\Atoms$.
   Every relation $r$ has a name, a source concept, and a target concept.
   The notation $r=\declare{n}{A}{B}$ denotes that relation $r$ has name $n$, source concept $A$, and target concept $B$.
   The part $\pair{A}{B}$ is called the {\em signature} of the relation.
   
   Triples serve to represent data.
   A triple %\footnote{Please note that this paper uses the word {\em triple} in a more restricted way than in natural language.}
   is an element of $\Triple{\Atoms}{\Rels}{\Atoms}$.
   For example, $\triple{\text{\atom{Peter}}}{\declare{\id{phone}}{\tt Person}{\tt TelephoneNumber}}{\text{\atom{074238991}}}$ is a triple.
   
   \begin{definition}[Data set]
   A data set $\dataset$ is a tuple $\pair{\triples}{\instance}$ with finite $\triples \subseteq {\Triple{\Atoms}{\Rels}{\Atoms}}$ and $\instance\subseteq\Pair{\Atoms}{\Concepts}$ that satisfies:
\begin{eqnarray}
   \triple{a}{\declare{n}{A}{B}}{b}\in\triples&\Rightarrow&a\instance A\ \wedge\ b\instance B
\label{eqn:wellTypedEdge}
\end{eqnarray}
\end{definition}
   Looking at the example,
   equation~\ref{eqn:wellTypedEdge} says that \atom{Peter} is an instance of {\tt Person} and \atom{074238991} is an instance of {\tt TelephoneNumber}.
   In practice, users can say that the Person Peter has telephone number 074238991.
   So, the ``thing'' that \atom{Peter} refers to (which is Peter) has \atom{074238991} as a telephone number.
   The notations $\triples_{\dataset}$ and $\instance_{\dataset}$ are used to disambiguate $\triples$ and $\instance$ when necessary.
   To save writing in the sequel, the notation $a\ r\ b$ means that $\triple{a}{r}{b}\in\triples$.

   A relation $r$ can serve as a container of pairs,
   as defined by the function $\popF{r}:\Dataset\rightarrow\powerset{\Pair{\Atoms}{\Atoms}}$.
   It defines a set of pairs, also known as the population of $r$:
\begin{equation}
   \pop{r}{\dataset}\ =\ \{ \pair{a}{b}\mid\ \triple{a}{r}{b}\in\triples_{\dataset}\}
\label{eqn:pop-rel}
\end{equation}
%   Equation~\ref{eqn:wellTypedEdge} implies that for every data set $\dataset$:
%\[\pair{a}{b}\in\pop{\declare{n}{A}{B}}{\dataset}\ \Rightarrow\ a\instance_{\dataset}A\ \wedge\ b\instance_{\dataset}B\]
%   For a developer, this means that the type of an atom depends only on the relation in which it resides; not on the actual population of the database.
%
   We overload the notation $\popF{}$ so we can use it on concepts $\popF{C}:\Dataset\rightarrow\powerset{\Atoms}$
   and expressions. We also define the difference of populations, in equation~\ref{eqn:pop-expr}, both for relations and concepts:
\begin{align}
   \pop{C}{\dataset}&= \{ x\mid\ x\ \instance_{\dataset}\ C\}
\label{eqn:pop-concept}\\
   \pop{x-y}{\dataset}&= \pop{x}{\dataset} - \pop{y}{\dataset}
\label{eqn:pop-expr}
\end{align}

\subsection{Constraints}
\label{sct:Constraints}
   Every constraint is an element of an infinite set called $\Constraints$.
   In this paper, variables $u$ and $v$ represent all three types of constraints.
   We say that a constraint, $u$, is satisfied when $u$ is true for all triples in the data set.
   For every constraint $u$, function $\violC{u}:\Dataset\rightarrow\powerset{\Pair{\Atoms}{\Atoms}}$ produces the violations of $u$,
   and $\sign{u}:\Pair{\Concepts}{\Concepts}$ yields the signature of $u$.
   The definition of $\violC{u}$ implies the assumption that we represent each violation as a pair.
   Every constraint must satisfy:
\begin{equation}
   \pair{a}{b}\in\viol{u}{\dataset}\ \wedge\ \sign{u}=\pair{A}{B}\ \Rightarrow\ a\instance A\ \wedge\ b\instance B
\label{eqn:wellTypedViolation}
\end{equation}
   Note that $\viol{u}{\dataset}=\emptyset$ means that $\dataset$ satisfies $u$.

   In case $u$ is a transaction,
   the system will keep it satisfied by adding the violations to a specific relation $\declare{n}{A}{B}$.
\begin{equation}
   \begin{array}[t]{l}
      \viol{u}{\pair{\triples\cup\{\triple{a}{\declare{n}{A}{B}}{b}\mid\pair{a}{b}\in\viol{u}{\dataset}\}}{\instance\cup\instance'}}=\emptyset\\
      \text{\bf where}\ \instance'=\{ \pair{a}{A}\mid\pair{a}{b}\in\viol{u}{\dataset}\}\cup\{ \pair{b}{B}\mid\pair{a}{b}\in\viol{u}{\dataset}\}
   \end{array}
\label{eqn:transaction}
\end{equation}
   It is obvious that not every conceivable constraint can satisfy this equation.
   So, we assume that the compiler restricts the set of transactions to those that satisfy equation~\ref{eqn:transaction}.
   As $\declare{n}{A}{B}$ is specific for $u$, we can write $\enfRel{u}$ for it.
   We call this the {\em enforced relation} of transaction $u$:
\begin{equation}
   \enfRel{u}=\declare{n}{A}{B}
\end{equation}
   The language Ampersand has more types of transactions than just this one,
   but to define this one only is sufficient for this paper.

\subsection{Schemas}
\label{sct:Schemas}
   Schemas serve to capture the semantics of a system~\cite{Spivak2012}.
   They define concepts, relations, and constraints.
   We assume that a software engineer defines a schema on design time, and a compiler checks whether the semantics are consistent.
   Errors the compiler detects are prohibitive for generating code,
   to prevent a substantial class of mistakes to ever reach end users.

   We describe a schema $\schema$ as a tuple $\quintuple{\concepts}{\rels}{\rules}{\transactions}{\busConstraints}$,
   in which $\concepts\subseteq \Concepts$ is a finite set of concepts,
   $\rels\subseteq\Rels$ is a finite set of relations,
   $\rules\subseteq\Constraints$ is a finite set of blocking invariants,
   $\transactions\subseteq\Constraints$ is a finite set of transactions,
   and $\busConstraints\subseteq\Constraints$ is a finite set of business constraints.

   Let us denote a transaction as $r \mapsfrom u$ or equivalently $r \mapsfrom \lambda \dataset.~ u(\dataset)$,
   in which $r=\enfRel{u}$.
   
   \begin{definition}[Schema]
   A schema is a tuple $\quintuple{\concepts}{\rels}{\rules}{\transactions}{\busConstraints}$ that satisfies:
\begin{align}
   \declare{n}{A}{B}\in\rels&~\Rightarrow~ A\in\concepts\,\wedge\, B\in\concepts
   \label{eqn:relationsIntroduceConcepts}\\
   u\in\rules\cup\transactions\cup\busConstraints\ \wedge\ \sign{u}=\pair{A}{B}&~\Rightarrow~A\in\concepts\,\wedge\, B\in\concepts\label{eqn:invariant-has-type}\\
   (\declare{n}{A}{B}\mapsfrom t)\in\mathcal\transactions&~\Rightarrow~ \declare{n}{A}{B}\in \rels\label{eqn:enforcement-has-type}
\end{align}
   \end{definition}
   Requirements~\ref{eqn:relationsIntroduceConcepts} and~\ref{eqn:invariant-has-type} ensure that concepts mentioned in relations and in the signature of rules are part of the schema.
   Requirement~\ref{eqn:enforcement-has-type} ensures the enforced relation of a transaction is declared in the schema. 
   When clarity is needed, we write $\concepts_{\schema}$, $\rels_{\schema}$, $\rules_{\schema}$, $\transactions_{\schema}$, $\busConstraints_{\schema}$
   for $\concepts$, $\rels$, $\rules$, $\transactions$, $\busConstraints$ corresponding to $\schema = \quintuple{\concepts}{\rels}{\rules}{\transactions}{\busConstraints}$.

\subsection{Information Systems}
\label{sct:Information Systems}
   Let us now define information systems by enumerating the requirements.
\begin{definition}[information system]
\label{def:information system}
\item An information system $\infsys$ is a tuple $\pair{\dataset}{\schema}$, in which
\begin{itemize}
\item $\dataset=\pair{\triples}{\instance}$ is a data set (so it must satisfy equation~\ref{eqn:wellTypedEdge}).
   We write $\triples_\infsys = \triples$ and $\instance_\infsys = \instance$ if needed;
\item $\schema=\quintuple{\concepts}{\rels}{\rules}{\transactions}{\busConstraints}$ is a schema (so it must satisfy equations~\ref{eqn:relationsIntroduceConcepts} thru~\ref{eqn:enforcement-has-type}).
\item Triples in the data set must have their relation mentioned in the schema:
   \begin{eqnarray}
      \triple{a}{\declare{n}{A}{B}}{b}\in\triples&\Rightarrow&\declare{n}{A}{B}\in\rels
   \label{eqn:define R}
   \end{eqnarray}
\item All violations must have a type, which follows from  (\ref{eqn:wellTypedViolation}).
\item Transactions remain satisfied by adding violations to a specific relation (\ref{eqn:transaction}).
\item All blocking invariants and transactions must remain satisfied:
   \begin{align}
      \forall u\in\rules\cup\transactions&.~\viol{u}{\dataset}=\emptyset
   \label{eqn:satisfaction}
   \end{align}
\end{itemize}
\end{definition}

\section{Generating a Migration Script}
\label{sct:Generating}
   The complexity of data migration results in a reasonable chance of making mistakes in writing code for data migration.
   For that reason, we want to generate this code.
   To ensure the correctness of the generator is even more challenging.
   That is why we have proven the correctness of the generator in Isabelle/HOL.
   
   This section starts with a presentation of the algorithm.
   Then we discuss its proof of correctness.

\subsection{Algorithm for generating a migration script}
   In the migration system, we need to refer to the items (concepts, relations, and constraints) of both the existing system and the desired system.
   We have to relabel items with prefixes to avoid name clashes in the migration system.
   We use a left arrow to denote relabeling by prefixing the name of the item with ``old.''.
\begin{equation}
   \begin{array}[m]{rcl}
      \overleftarrow{\pair{\dataset}{\schema}}&=&\pair{\overleftarrow{\dataset}}{\overleftarrow{\schema}}\\
      \overleftarrow{\pair{\triples}{\instance}}&=&\pair{\overleftarrow{\triples}}{\instance}\\
      \overleftarrow{\quintuple{\concepts}{\rels}{\rules}{\transactions}{\busConstraints}}&=&\quintuple{\concepts}{\overleftarrow{\rels}}{\overleftarrow{\rules}}{\overleftarrow{\transactions}}{\overleftarrow{\busConstraints}}\\
      \overleftarrow{\triples}&=&\{\triple{a}{\overleftarrow{r}}{b}\mid\triple{a}{r}{b}\in\triples\}\\
      \overleftarrow{\declare{n}{A}{B}}&=&\declare{old.n}{A}{B}\\
      \overleftarrow{X}&=&\{\overleftarrow{x}\mid x\in X\}\\
      \viol{\overleftarrow{u}}{\overleftarrow{\dataset}}&=&\viol{u}{\dataset}\\
      \sign{\overleftarrow{u}}&=&\sign{u}\\
      \enfRel{\overleftarrow{u}}&=&\overleftarrow{\enfRel{u}}
   \end{array}
\end{equation}
   The notation $\overrightarrow{\infsys}$ is defined similarly, using prefix ``new.'' instead of ``old.''.

   Then we define the migration system $\migrsys$ as follows:
   Let $\pair{\dataset}{\schema}$ be the existing system.
   Let $\pair{\dataset'}{\schema'}$ be the desired system in its initial state.
\begin{enumerate}
\item We take a disjoint union of the data sets by relabeling relation names:
   \begin{align}
      \dataset_\migrsys={}&\overleftarrow{\dataset}\cup\overrightarrow{\dataset'}
   \end{align}
\item We create transactions to copy the population of relations from $\dataset$ to $\dataset'$:
For every relation $r$ shared by the existing and desired systems, we generate a helper relation: ${\tt copy}_r$, and a transaction to produce its population.
We also generate a transaction to populate the relation $\overrightarrow{r}$ (in the desired system).
% \begin{verbatim}
%    RELATION copied_r[A*B]
%    RELATION new_r[A*B] [UNI]
%    ENFORCE copied_r >: new_r /\ old_r
%    ENFORCE new_r >: old_r - copied_r
% \end{verbatim}
   \begin{align}
      \rels_1={}&\{{\tt copy}_r\mid r \in \rels_{\schema'}\cap\rels_{\schema}\}\\
      \transactions_1={}&\{{\tt copy}_r\mapsfrom \popF{\overrightarrow{r}\cap\overleftarrow{r}} \mid r \in\rels_1\}\cup\{\overrightarrow{r}\mapsfrom \popF{\overleftarrow{r}-{\tt copy}_r} \mid r \in\rels_1\}
   \end{align}
   The helper relation is needed to ensure that the copying process terminates,
   which is the case when:
   \begin{align}
      \forall r\in\rels_1.~\overleftarrow{r}={\tt copy}_r\label{eqn:copyingTerminates}
   \end{align}

\item The new blocking invariants are $\rules_{\schema'}-\rules_{\schema}$.
   For each new blocking invariant $u$, we generate a helper relation: ${\tt fixed}_u$, to register all violations that are fixed.
   We also need a transaction to produce its population:
% \begin{verbatim}
%    RELATION fixed_TOTr[A*A]
%    ENFORCE fixed_TOTr >: I /\ new_r;new_r~
% \end{verbatim}
   \begin{align}
      \rels_2={}&\{{\tt fixed}_u\mid u \in \rules_{\schema'}-\rules_{\schema}\}\\
      \transactions_2={}&\{{\tt fixed}_u\mapsfrom\lambda\dataset.~\viol{\overleftarrow{u}}{\dataset}-\viol{\overrightarrow{u}}{\dataset}\mid u\in\rules_{\schema'}-\rules_{\schema}\}\label{eqn:enforceForRules}
   \end{align}
   The helper relation is needed to ensure that the fixing of violations terminates,
   which is the case when:
   \begin{align}
      \forall u\in\rules_{\schema'}-\rules_{\schema}.~\viol{\overrightarrow{u}}{\dataset}={\tt fixed}_u\label{eqn:fixingTerminates}
   \end{align}
\item For each new blocking invariant $u$, we generate another blocking invariant $v$ in the migration system that blocks fixed violations from recurring:
% \begin{verbatim}
%    RULE Block_TOTr : fixed_TOTr |- new_r;new_r~
%    MESSAGE "Relation r[A*B] must remain total."
%    VIOLATION (TXT "Atom ", SRC I, TXT " must remain paired with an atom from B.")
% \end{verbatim}
   \begin{align}
      \rules_\text{block}={}&\{v\ 
      \begin{array}[t]{l}
         \text{\bf where}\\
         \sign{v}=\sign{u}\\
         \viol{v}{\dataset}=\viol{\overrightarrow{u}}{\dataset}\cap{\tt fixed}_u
      \end{array}\\
      &\mid u\in\rules_{\schema'}-\rules_{\schema}\}\notag
   \end{align}
   Note that the blocking invariants from $\schema'\cap\schema$ are also used in $\migrsys$ to ensure continuity.
\item To signal users that there are violations that need to be fixed, we generate a business constraint for each new blocking invariant $u$:
% \begin{verbatim}
%    ROLE User MAINTAINS TOTr
%    RULE TOTr : I - fixed_TOTr |- new_r;new_r~
%    MESSAGE "Please, make relation r[A*B] total."
%    VIOLATION (TXT "Fix ", SRC I, TXT " by pairing it with an (any) atom from B.")
% \end{verbatim}
   \begin{align}
      \busConstraints_\text{fix}={}&\{v\ 
      \begin{array}[t]{l}
         \text{\bf where}\\
         \sign{v}=\sign{u}\\
         \viol{v}{\dataset}=\viol{\overrightarrow{u}}{\dataset}-{\tt fixed}_u
      \end{array}\\
      &\mid u\in\rules_{\schema'}-\rules_{\schema}\}\notag
   \end{align}
   In some cases, a migration engineer can invent ways to satisfy these invariants automatically.
   For this purpose, the generator should produce source code (as opposed to compiled code) to allow the migration engineer to replace a business constraint with transactions of her own making.
\item Let us combine the above into a single migration schema:
   \begin{align}
      \schema_\migrsys=\langle{}&\concepts_\dataset\cup\concepts_{\dataset'},\\
      &\overleftarrow{\rels_{\schema}}\cup\overrightarrow{\rels_{\schema'}}\cup\rels_1\cup\rels_2,\notag\\
      &\rules_\text{block}\cup(\rules_{\schema}\cap\rules_{\schema'}),\notag\\
      &\transactions_1\cup\transactions_2\cup\transactions_{\schema'},\notag\\
      &\busConstraints_\text{fix}\cup\busConstraints_{\schema'}\rangle\notag
   \end{align}
\end{enumerate}
   The migration engineer can switch all traffic to the desired system
   after resolving the violations that prohibit deploying the desired system.
   That is the case when violations of new invariants on the old population have all been fixed:
% \begin{verbatim}
%    ROLE User MAINTAINS CleanUp_TOTr
%    RULE CleanUp_TOTr : V[ONE*A] ; (I - fixed_TOTr) ; V[A*ONE]
%    MESSAGE "Now you can remove the migration system because relation r[A*B] is total."
% \end{verbatim}
\begin{equation}
   \forall u\in\rules_{\schema'}-\rules_{\schema}.\ \viol{\overleftarrow{u}}{\dataset_\migrsys}={\tt fixed}_u
\end{equation}
   After this, the migration engineer can remove the migration system and the old system.

\subsection{Proof}
\label{sct:Proof}
   A theorem prover, Isabelle/HOL, is used to establish the correctness of the presented algorithm.
   This section discusses the proof obligations. The proof is available online.
   Assume that $\infsys$ and $\infsys'$ are information systems, so they satisfy definition~\ref{def:information system}.
   This means that equations~\ref{eqn:wellTypedEdge}, \ref{eqn:wellTypedViolation}, \ref{eqn:transaction}, \ref{eqn:relationsIntroduceConcepts} thru~\ref{eqn:enforcement-has-type}, \ref{eqn:define R}, \ref{eqn:satisfaction} hold for both systems.
\begin{enumerate}
\item Prove that $\migrsys$ is an information system,
      so the migration engineer can deploy the migration system.
\item For every relation $r\in\rels_{\schema}\cap\rels_{\schema'}$, prove that $\overrightarrow{r}$ contains all triples from the existing data set in a finite time after deploying $\migrsys$,
      to ensure that the existing data is migrated to the desired system.
\item Prove that equation~\ref{eqn:satisfaction} holds for $\migrsys$ when the copying of relations is done (just before MoT),
      to ensure that a valid information system is presented to users at the moment of transition.
\item Prove that equation~\ref{eqn:copyingTerminates} represents the condition for terminating the copying process.
\item Prove that equation~\ref{eqn:fixingTerminates} represents the condition for terminating the fixing of violations.
\item Prove that the migration engineer can switch the ingress from the existing system to the migration system
      when equations~\ref{eqn:copyingTerminates} and~\ref{eqn:fixingTerminates} are true,
      so the computer can let the migration engineer know that it is safe to switch.
\item Assuming that the business can fix each violation in finite time,
      prove that the number of violations to be fixed by the business is finite,
      and that it decreases monotonically, so the business can reach the MoC in finite time.
\item Prove that $\rules_\text{block}$ is free of violations, so it is redundant after the MoC.
\item Prove that $\transactions_1$ will never more add any violations to any ${\tt copy}_u$ at the MoC, so $\rels_1$ and $\transactions_1$ are redundant after the MoC.
\item Prove that $\transactions_2$ will never more add any violations to any ${\tt fixed}_u$ at the MoC, so that $\rels_2$ and $\transactions_2$ are redundant after the MoC.
\item Prove that $\busConstraints_\text{fix}$ is free of violations, so it is redundant after the MoC.
\item Prove that $\concepts_{\schema}$, $\rels_{\schema}$, and $\rules_{\schema}$ are redundant after the MoC.
\item Prove that, given the redundancies proven above, the migration system is equivalent to $\infsys'$ after the MoC,
      so the migration engineer can switch the ingress to the desired system and remove the existing system and the migration system.
\end{enumerate}
\section{Validation}
   As the system responds to events, it changes its data set, while the schema remains the same.
   In this paper, it suffices to define an event as a pair of systems for which the schema stays constant.
   Events are categorized by the part of the data set that changes.

\begin{definition}[Event]
   Let $R \subseteq \Rels$ and $I \subseteq \Rels$ be sets of relations,
   and let $\infsys=\pair{\schema}{\dataset}$ and  $\infsys'=\pair{\schema}{\dataset'}$ be information systems,
t  hen $\infsys\xrightarrow[I]{R} \infsys'$ is an event if and only if:
\begin{align}
   \triple{a}{r}{b}\in\triples_{\dataset}-\triples_{\dataset'}&\Rightarrow\ r\in R\label{eqn:eventUnchanged}\\
   \triple{a}{r}{b}\in\triples_{\dataset'}-\triples_{\dataset}&\Rightarrow\ r\in(R \cup I)\label{eqn:eventInsert}
\end{align}
% was:
% \begin{align}
%    \triples_{\dataset} - (\Triple{\Atoms}{(R \cup I)}{\Atoms}) &= \triples_{\dataset'} - (\Triple{\Atoms}{(R \cup I)}{\Atoms})
% \label{eqn:eventUnchanged}\\
%    \triples_{\dataset} - (\Triple{\Atoms}{R}{\Atoms}) &\subseteq \triples_{\dataset'} - (\Triple{\Atoms}{R}{\Atoms})
% \label{eqn:eventInsert}
% \end{align}
   We say that event $\infsys\xrightarrow[I]{R} \infsys'$ brings the system $\infsys$ to $\infsys'$ by inserting pairs in relations from $I$ and deleting pairs in relations from $R$.
\end{definition}

   Equation~\ref{eqn:eventUnchanged} and~\ref{eqn:eventInsert} state that the triples of those for relations in $I$ or $R$ are the only ones that can be changed, and that only triples in $R$ can be removed.
%If $I$ or $R$ is the empty set, we omit it from the arrow, so $\infsys \xrightarrow{R} \infsys'$ is a notation for $\infsys \xrightarrow[\emptyset]{R} \infsys'$.

\begin{definition}[Inserting event]
   We write $\infsys \xrightarrowdbl[I]{R} \infsys'$ to state that some triple was inserted whose relation was in $I$, that is:
   $t \in \triples_{\dataset'} \cap (\Triple{\Atoms}{I}{\Atoms})$ and $t \not\in \triples_{\dataset}$ for some triple $t$.
\end{definition}

W  e use inserting events to model the effect of transactions.
%   If $R$ is a set of relations that covers those that occur in the rule, then changes to an system $\infsys$ can be described by the event $\infsys \xrightarrow[\{r\}]{R} \infsys'$.


   We first observe that $\migrsys = \pair{\dataset_\migrsys}{\schema_\migrsys}$ is a system after application of the transactions:
   the only rules are those introduced in equation~\ref{eqn:migrsysrules}, and transactions from equation~\ref{eqn:enforceForRules} ensure that $\popF{{\tt viols}_u} = u$ initially, while $\popF{{\tt fixed}_u}$ is empty.
   Hence, the MoT can happen after the transactions have been applied, giving us $\migrsys'$.
   The event $\migrsys \xrightarrow[\rels_2 \cup \rels_3]{\overrightarrow\rels_{\schema'}}\migrsys'$ represents the MoT.
   
   The MoC is reached for data set $\dataset'$ whenever $\pop{{\tt viols}_u - {\tt fixed}_u}{\dataset'}=\emptyset$ for all rules.
   This should happen after a finite number of inserting events: let $I = \{{\tt fixed}_u \mid u\in\rules_{\schema'}\}$ be the relations that keep track of which violations have been fixed, then there is no infinite chain of systems $\migrsys' = \migrsys_0, \migrsys_1,\ldots$ such that $\migrsys_i\xrightarrowdbl[I]{\overrightarrow\rels_{\schema'}}\migrsys_{i+1}$.
   This shows that when users get rid of violations, there is real progress: they can only do this a finite number of times.
   Next we ask ourselves: does this mean we always reach the MoC?
   
   Suppose there is a system in which all violations have been fixed, that is, suppose there is a $\migrsys^C$ such that $\pop{{\tt fixed}_u}{\dataset_{\migrsys^C}}=\pop{{\tt viols}_u}{\dataset_{\migrsys^C}}$ and $\migrsys'\xrightarrow[I]{\overrightarrow\rels_{\schema'}} \migrsys^C$.
   In that case, after any steps done by users (both those with and without the migration helper role), we can reach it, that is: for any $\migrsys_i$, $\migrsys'\xrightarrow[I]{\overrightarrow\rels_{\schema'}} \migrsys_i$ implies $\migrsys_i\xrightarrow[I]{\overrightarrow\rels_{\schema'}} \migrsys^C$.
   This implies that if the MoC is initially reachable through $\xrightarrow[I]{\overrightarrow\rels_{\schema'}}$ events, then it will remain reachable.
   So is the MoC initially reachable?
   
   For this last question, we need to add a condition:
   There needs to be a system that has the desired schema $\schema'$.
   If rules in $\schema'$ are such that they cannot all be satisfied, regardless of the data set, no such system exists and we have no hope of reaching it.
   We assume that the rules of $\schema'$ are intended to prevent data pollution, such that a data set $\dataset'$ describing the `real world' would be one that satisfies them.
   So let $\pair{\dataset'}{\schema'}$ be a system, then we can get to it through our migration system.
   To show this, we construct a migration system $\migrsys^C$ satisfying $\pop{{\tt fixed}_u}{\dataset_{\migrsys^C}}=\pop{{\tt viols}_u}{\dataset_{\migrsys^C}}$ and $\migrsys'\xrightarrow[I]{\overrightarrow\rels_{\schema'}} \migrsys^C$.
   We can do so by simply stating its triples, since the schema is determined by $\migrsys'$:
   
\begin{align}
   \begin{aligned}
   \triples_{\migrsys^C} &= \triples_\migrsys 
   \cup  \{\triple{a}{{\tt viols}_u}{b} \mid \pair{a}{b}\in \viol{u}{\dataset}, u\in \rules_{\schema'} \} \\
   &\cup \{\triple{a}{{\tt fixed}_u}{b} \mid \pair{a}{b}\in \viol{u}{\dataset}, u\in \rules_{\schema'} \}
   \cup \{\triple{a}{\overrightarrow{r}}{b} \mid \triple{a}{r}{b}\in\triples_{\dataset'}\}   
   \end{aligned}
\end{align}
   
   Collectively, this shows that the MoC is and remains reachable after a finite number of inserting events by users in the migration helper role.
   
\section{Conclusions}
\label{sct:Conclusions}
   In this paper, we describe the data migration as going from an existing system to a desired one, where the schema changes.
   As Ampersand generates information systems, creating a new system can be a small task, allowing for incremental deployment of new features.
   We describe the parts of a system that have an effect on data pollution.
   We assume that the existing system does not violate any constraints of its schema, but address other forms of data pollution:
   constraints that are not in the schema but are in the desired schema are initially relaxed such that the business can start using the migration system, after which this form of data pollution needs to be addressed by human intervention.
   We propose a method for doing migration such that only a finite amount of human intervention is needed.
   Our method allows a system similar to the desired system to be used while the intervention takes place.

   Our proposed migration is certainly not the only approach one could think of.
   However, we have not come across other approaches that allow changing the schema in the presence of constraints.
   As such, we cannot compare our approach against other approaches.
   We envision that one day there will be multiple approaches for migration under a changing schema to choose from.
   For now, our next step is to implement the approach shown here into Ampersand.

   This work does not consider what to do about (user) interfaces.
   Instead, it models events by assuming that any change to the data set can be achieved.
   In practice, such changes need to be achieved through interfaces.
   Most Ampersand systems indeed allow the users of the system to edit the data set quite freely through the interfaces.
   However, some interfaces may require certain constraints to be satisfied, which means that interfaces of the desired system might break.
   In the spirit of the approach outlined here, we hope to generate migration interfaces that can replace any broken interfaces until the Moment of Transition.
   How to do this is future work.

%\section{Bibliography}
\bibliographystyle{splncs04}
\bibliography{doc}


\end{document}
