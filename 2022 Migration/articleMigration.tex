\documentclass{elsarticle}
\usepackage{graphicx}
%\usepackage{multicol}
%\usepackage{footmisc}
\usepackage{amstext}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage[english]{babel}
%\usepackage[official,right]{eurosym}
\selectlanguage{english}
\hyphenation{ExecEngine}
\newtheorem{lemma}{Lemma}
\begin{document}
\include{preambleMigrations}

% TODO Algemeen:
% - type van triple-set en atom-set definieren zodat viol_u ?->P(?x?)
%   (Bas: ik heb foute types weggehaald, maar dit nog niet gedefinieerd)
% - nieuwe populatie na de disjoint union zou de populatie met ENFORCE regels toegepast moeten zijn
%  (en in het bijzonder de ISA regels)

% TODO Stef:
% - stukje literatuuronderzoek
% https://ieeexplore.ieee.org/abstract/document/7445334?casa_token=ECzi6XeV2ncAAAAA:KhWzB8XBFOUJ0C6AD-XjX_ryuA9ARvTd3gm6RR-ZNiR8sZ1858FJpQ7zKQhkAZDlv8IjPdgD
% https://ieeexplore.ieee.org/abstract/document/8549944?casa_token=9qiGqNzh2Q0AAAAA:C-cYogExB35nGxQdxLcdBh4JoLNvM0OHedAMhCbB5V4kb4_6nzHUvc23xSJbeoBu67LSiz-Y
% https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.651.9298&rep=rep1&type=pdf
% https://www.scirp.org/html/4-7800724_106592.htm
% https://www.researchgate.net/profile/Ranjana-Badre/publication/318665687_GUI_for_Data_Migration_and_Query_Conversion/links/5b45bbea0f7e9b1c722386e5/GUI-for-Data-Migration-and-Query-Conversion.pdf
% https://journal3.uin-alauddin.ac.id/index.php/literatify/article/view/12567

% Bewijsverplichtingen:
% -> het migratie-systeem is typefout-vrij (opmerking: als we de type-checker buiten beschouwing willen laten, kunnen we dit niet beschrijven)
% -> het migratie-systeem is vrij van overtredingen op regels die het migratie-systeem moet bewaken
% -> het migratie-systeem bevat alle oude data, ihb nog steeds na het toepassen van de enforce regels
% -> er is een pad naar ingebruikname van het nieuwe systeem (vanaf de initiÃ«le toestand van het migratie-systeem)
% -> corollary: op het moment van ingebruikname van het nieuwe systeem, is het migratie-systeem vrij van overtredingen op regels die het nieuwe systeem moet bewaken
% -> optioneel: na een begrensd aantal 'voortgangs-stappen' kan het nieuwe systeem in gebruik genomen worden
% -> optioneel: er bestaat een migratie-systeem dat de oude functionaliteit behoudt (mogelijk uitbreid) totdat het nieuwe systeem in gebruik genomen is?

\title{It's all about Meaning!\\{\normalsize A Theory for Data Migration during Software Development}}
\author[ou,ordina]{Stef Joosten\fnref{fn1}}
\ead{stef.joosten@ou.nl}
\author[umn]{Sebastiaan Joosten\fnref{fn2}}
\address[ou]{Open Universiteit Nederland, Heerlen, the Netherlands}
\address[ordina]{Ordina NV, Nieuwegein, the Netherlands}
\address[umn]{University of Minnesota, Minneapolis, USA}
\fntext[fn1]{ORCID 0000-0001-8308-0189}
\fntext[fn2]{ORCID 0000-0002-6590-6220}

\begin{abstract}
   The Ampersand project has provided the theory and tools to generate information systems from an algebraic specification.
   However, information systems in practice may change repeatedly after their maiden deployment.
   Changes that affect the data model typically result in a data migration.
   In such cases, simply regenerating the system is not enough.
   That would reset the database to its initial state, losing all data gathered so far.
   To prevent that, a migration engineer must transfer the old data to the new system by hand.

   In this contribution we develop a theory for reliable data migration to help the migration engineer
   to transfer the data and preserve the semantics as much as possible.
   We aim to automate the data migration,
   to prevent mistakes and enable more frequent migrations.
   The target is to generate a migration script from two specifications: the old specification and the new specification.
   A software generator that embodies this theory is subject of future research.
\end{abstract}

\begin{keyword}
relation algebra\sep software development\sep data migration\sep software migration\sep Ampersand
\end{keyword}
\maketitle

\section{Introduction}
\label{sct:Introduction}
   To automate the datamigrations that come with incrementally developing and deploying information systems.
   That is the purpose of the theory proposed in this paper.

   We believe in generating information systems%
\footnote{In the sequel, the word ``system'' refers to the phrase ``information system''. This simplifies the language a little. }
   to prevent human programming errors.
   For, a mistake not made can never propagate into production.
   We also believe that information systems should be released into production in small increments,
   to obtain shorter release cycles, more frequent releases, and thus a more agile software development process%
   \footnote{These effects can be measured in terms of DevOps metrics such as
   deployment frequency,
   reliability of deployments, and
   change failure rate~\cite{DevOps2021}.}.
   By installing upgrades automatically, frequently, in small increments, and without down-time,
   the users experience a system that evolves organically, without even noticing the updates.
   Thus, we believe our theory will help to automate data migrations correctly and reliably.
   The benefits work in two directions.
   Automation data migrations saves effort and errors,
   contributing to more frequent and smaller releases.
   Smaller and faster releases also mean smaller migrations
   and therefore a smaller difference between the old system and the new system.
   The smaller that difference, the simpler the migration and the more of it can be automated.

   In each increment, we distinguish an ``old system'' and a ``new system''.
   The difference between the old system and the new system is not just a difference in two schemas.
   When deploying the increment, the data from the old system needs to migrate to the new system as well.
   Incremental evolution poses specific requirements to data migration,
   which sets it apart from data migrations for other purposes.
   For instance, if a data migration is done for switching to another platform or to different technology,
   e.g.~\cite{Gholami2016,Bisbal1999},
   migration engineers may exclude new functionality to avoid introducing new errors in an otherwise error-prone migration process.
   In our work, however, introducing new functionality and removing obsolete functionality is the very purpose of an increment.
   Data migration literature sometimes uses a different notion of increment.
   For example, Ataei, Khan, and Walkingshaw~\cite{Ataei2021,Walkingshaw2014} define an increment as a variation between two data structures.
   They show how to unify databases with slight variations by preserving all variations in one comprehensive database.
   In our work, however, we want to get rid of the old system and not retain it.
   So, we need to look at a different approach.
   Examples like these have convinced us to define a new method for data migration that is specifically meant for systems in production that are
   continually evolving in small increments.
   
   The contribution of this paper is to derive a migration schema from two artifacts: an existing system
   (which has its own schema and dataset) and a new schema (which specifies the new system),
   which allows us to facilitate the migration.
   The purpose of our theory is to develop tools.
   These tools must satisfy such practical requirements as
   zero down-time updates and ways to cope with data pollution.
   In practice, more frequent updates require that updates come with zero down-time.
   For our theory, zero down-time means that all human activity for the data migration can be done without taking the system out of production.
   We consider any delay caused by automatically switching from the old system to the new one negligible,
   assuming a modern deployment platform that does this with zero down-time.
   
   Our theory is based on the following assumptions and requirements:
\begin{itemize}
   \item the data migration is meant to deploy a software increment in production;
   \item the old data set may be polluted, but it satisfies its schema;
   \item the data migration may require human interaction, which may take time;
   \item the meaning of data must be preserved;
   \item the business continues during the migration without interruption;
   \item there is a compiler to generate an information system from a given schema.
\end{itemize}


\section{Analysis}
   Information systems are typically used by distributed actors (both users and computers) who continually work with data.
   Every system contains a dataset, which represents the state of the system.
   Events that the system detects may cause the state to change.
   To keep our theory technology independent, we will assume that a dataset contains triples.
   This makes our theory valid for any kind of database that triples can represent,
   such as SQL databases, object-oriented databases, graph databases, triple stores, and other no-SQL databases.

   Data migration occurs when a new system replaces an old one,
   while preserving the meaning of the data from the old system as much as possible.
   Just copying the set of data from the old system to the new is obviously wrong if the schemas of both systems differ.

   We will assume that the schema of a system contains constraints on its dataset.
   These constraints represent the semantics of the system, similar to the theory developed by Spivak~\cite{Spivak2012}.
   This allows us to be explicit about ``preserving the meaning as much as possible''.
   In contrast with Spivak, however, our approach does not work with functions (functors)
   but with relations similar to Alloy~\cite{Alloy2006} and allegories%
\footnote{Allegories are a specific type of categories, which the reader does not need to understand for the purpose of this paper}~\cite{Zielinski2013}.

   In this work we use the Ampersand compiler~\cite{Joosten-JLAMP2018},
   to generate information systems.
   A script in the Ampersand language represents the system's semantics by constraints only.
   We don't have to formalize the semantics from specifications or from source code
   because the constraints are already explicitly available in the Ampersand script.
   A system generated by the Ampersand compiler will keep these constraints satisfied at all times.
   This makes Ampersand an obvious platform for validating our theory.
   An Ampersand script contains just enough information to generate a complete system,
   which means that a classical database schema (i.e. data structure plus semantics) can be extracted from the Ampersand script.
   For the purpose of this paper we can equate an Ampersand script with the schema of the generated system.

   For data migration, complications arise from accumulated data pollution in the old system.
   We distinguish data pollution in the following categories:
\begin{itemize}
   \item data pollution that cannot be captured in constraints on the dataset,
   such as a street address that has become obsolete because the system never registered the change of address.
   \item data pollution that violates a constraint in the schema.
   \item data pollution that violates a constraint that is not in the schema.
   \item data pollution that is a consequence of an erroneous constraint in the schema.
\end{itemize}
   The first category of pollution must be dealt with by working procedures, to prevent such pollution as much as possible.
   In some cases, constraints on data can be formulated,
   so the system can help the user and simplify (or eliminate) the corresponding working procedure.
   So, such constraints may help to move this type of pollution to one of the other categories.
   The second category is about violations of constraints from the schema.
   Such violations do not occur in our theory because the old system was generated by Ampersand,
   so that system will keep all constraints from the schema satisfied at all times.
   This sets our approach apart from other approaches to formalize data migration, e.g.~\cite{Thalheim2013}.
   The third category, data pollution that is not captured by any of the constraints mentioned in the schema,
   can be dealt with by adding constraints in the new system.
   In that case, a migration engineer must ensure that the new system can start with data that satisfies all constraints in its schema.
   So some human intervention may be necessary.
   The last category of data pollution, erroneous constraints, must be fixed by replacing them by the correct constraints in the new system.

\begin{figure}[bht]
   \begin{center}
     \includegraphics[scale=.35]{Migration.png}
   \end{center}
\caption{Data migration}
\label{fig:event flow}
\end{figure}
   This illustrates that automating data migrations has its limits.
   Some effort of a migration engineer can never be excluded.
   If the semantics encoded in the new schema differs from the old schema,
   a migration engineer must make choices.
   If the interpretation of data changes,
   a migration engineer must remove obsolete constraints from the old system and introduce new constraints in the new system.
   The consequences of such changes may require changes in data as well.
   The migration engineer may even want to adapt existing data of which the constraints in the schema have not changed.
   All of this requires a third schema, which specifies the migration itself.
   This migration schema preserves data from the old system, it may introduce new data automatically (generated from the old dataset),
   and it may require users to introduce new data (which will take time). 
   It must contain the old schema, but it also contains the new schema and possibly some new data, which is specified by the migration engineer.
   As the migration schema must distinguish between the old dataset and the new dataset,
   the migration must be based on the disjoint union of the two.
   So the idea of a migration schema is to compute the disjoint union of the old and the new schemas
   and to present it to the migration engineer in the form of source code,
   so the migration engineer can change everything she wants to suit the particulars of the data migration.

%    A naive migration might proceed as follows:
%    Launch the new system in parallel to the old one, copy data from the old to the new system, and have everyone use the new system.
%    However, numerous issues might impede that plan.
%    The following examples illustrate the practical issues that may occur:
% \begin{enumerate}
% \item Data required in the new system is missing in the old system.
%    There may be no way in the old system to enter that data.
%    An example could be that every reimbursement form needs to have an address associated to it to mail the check to, but address information is not stored in the old system:
%    The old system required the reimbursement office to look up employee's addresses from a hand-written list they had on their desk.

%    This issue will require somebody to insert the missing addresses in the new system.
% \item Data in the old system is wrong but cannot be corrected there due to how the old system was designed.
%    An example is if the old system only allows approvals to be entered as the current user, and the CEO has always insisted that her administrative staff enters the approvals into the system for her.
%    This may result in approvals being entered as admin staff, where it was really the CEO making the approval.

%    This issue will require that the incorrect registration of staff members is corrected in the new system.
% \item Data in the old system does not satisfy invariants of the new system.
%    There may be no way in the old system of making the data satisfy those invariants.
%    We can use the same example as in the previous bullet point,
%    but add the requirement (in the new system) that every purchase above a certain amount needs to be approved by the CEO.

%    This issue will require somebody to change the data to satisfy the broken invariants in the new system.
% \item A way of entering data into the old system is missing in the new system.
%    People or automated processes might rely on these ways of entering data.
%    An example could be that when employees turned their computers on or off,
%    an ad-hoc script would automatically check them in- and out to determine the number of hours they worked.
%    A handful of employees still relies on this.

%    This issue will require that users of obsolete functionality are informed and given a way to cope with the missing functionality where appropriate.
% \item Data present in the old system cannot be stored in the new system.
%    An example could be that references to physical locations where original receipts are kept are stored in the old system,
%    but the new system relies on scans of receipts and allows the originals to be destroyed or not submitted.

%    This issue will require that the old data is kept until the original receipts have been scanned.
% \end{enumerate}

% (@Bas, het volgende zit al in de oplossingen sfeer. Willen we dat hier al doen?)
%    To mitigate these issues, we:
   
%    \begin{enumerate}
%    \item Allow `missing' triples requirements to be ignored during migration.
%    \item Allow triples to be migrated while being marked as needing correction.
%    \item Allow invariants in the new system to be ignored for certain triples during migration.
%    \item Allow continued use of interfaces of the old system, data entered into the old system via those interfaces needs to be continuously copied to the new system.
%    \item Retain data in the old system until it can be marked as ready to be phased out.
%    \end{enumerate}   

   In the following section we introduce the definitions required to migrate data from one system to another.

\section{Terminology}
\label{sct:Terminology}
   First,
   we define ``information system'' as a combination of dataset, schema, and functionality.
   We start with datasets in section~\ref{sct:Datasets}, then we define schemas in section~\ref{sct:Schemas},
   and then we can define information systems.

\subsection{Datasets}
\label{sct:Datasets}
   A dataset $\dataset$ describes a set of structured data, which is typically stored persistently in a database of some kind.
   We write $\dataset_{\infsys}$ to refer to the dataset of a particular information system $\infsys$.
   The purpose of a dataset is to describe the data of a system at one point in time. 
   Before defining datasets, we must first define the constituent notions of atom, concept, relation, and triple.
   
   Atoms serve as data elements.
   They are values without internal structure of interest, meant to represent data elements in a database.
   From a business perspective, atoms represent concrete items of the world,
   such as \atom{Peter}, \atom{1}, or \atom{the king of France}.
   By convention throughout the remainder of this paper, variables $a$, $b$, and $c$ represent \emph{atoms}.
   All atoms are taken from an infinite set called $\Atoms$.
   
   Concepts are names that group atoms of the same type.
   All concepts are taken from an infinite set $\Concepts$.
   $\Concepts$ and $\Atoms$ are disjoint.
   For example, a developer might choose to classify \atom{Peter} and \atom{Melissa} as \concept{Person},
   and \atom{074238991} as a \concept{TelephoneNumber}.
   In this example, \concept{Person} and \concept{TelephoneNumber} are concepts.
   We will use variables $A$, $B$, $C$, $D$ to represent concepts.

   The relation $\inst:\Pair{\Atoms}{\Concepts}$ is the instance relation between atoms and concepts.
   The expression $a\ \inst\ C$ means that atom $a$ is an \emph{instance} of concept $C$.
   This relation is used in the type system.
   In the type system, every atom must have a type,
   which makes $\inst$ a total relation:
\begin{eqnarray}
   \forall a\in\atoms\ \exists C\in\Concepts: a\ \inst\ C
\label{eqn:concepts}
\end{eqnarray}

   Relations serve to organize and store data, to allow a developer to represent facts.
   In this paper, variables $r$, $s$, and $d$ represent relations.
   All relations are taken from an infinite set $\Rels$.
   $\Rels$ is disjoint from $\Concepts$ and $\Atoms$.
   Every relation $r$ has a name, a source concept, and a target concept.
   We write $r=\declare{n}{A}{B}$ to denote that relation $r$ has name $n$, source concept $A$, and target concept $B$.

   Triples serve to represent data.
   A triple is an element of $\Triple{\Atoms}{\Rels}{\Atoms}$.
   A dataset $\dataset$ is a tuple $\pair{\triples}{\inst}$ that satisfies:
\begin{eqnarray}
   \forall\triple{a}{\declare{n}{A}{B}}{b}\in\triples&:&a\ \inst\ A\ \wedge\ b\ \inst\ B
   \label{eqn:type}
\end{eqnarray}
   For example, $\triple{\text{\atom{Peter}}}{\declare{\id{phone}}{\tt Person}{\tt TelephoneNumber}}{\text{\atom{074238991}}}$ is a triple.
   Equation~\ref{eqn:type} says that \atom{Peter} is an instance of {\tt Person} and \atom{074238991} is an instance of {\tt TelephoneNumber}.
   In practice, users can say that Peter has telephone number 074238991.
   So, the ``thing'' that \atom{Peter} refers to (which is Peter) has \atom{074238991} as a telephone number.
   This ``meaning from practice'' has no consequences in the formal world.
   We leave it entirely up to a user to attach a practical meaning to a triple.
   In this paper, the informal meaning is interesting only for the sake of illustration.

   $\triples$ is a finite set of triples and $\inst$ is the instance relation.
   In case we need to be explicit about the dataset,
   we will write $\triples_{\dataset}$ and $\inst_{\dataset}$ to refer to $\triples$ and $\inst$ respectively.
   Every dataset is an element of an infinite set called $\Dataset$.
   To save writing in the sequel, we will write $a\ r\ b$ to denote that $\triple{a}{r}{b}\in\triples$.
   Every atom has the type of the relation in which it resides:

   A relation $r$ can serve as a container of pairs,
   as defined by the function $\id{pop}_r:\Dataset\rightarrow\powerset{\Pair{\Atoms}{\Atoms}}$.
   It defines a set of pairs, which we call the population of $r$:
\begin{equation}
   \pop{r}{\dataset}\ =\ \{ \pair{a}{b}\mid\ \triple{a}{r}{b}\in\triples_{\dataset}\}
\label{eqn:pop}
\end{equation}
   Equation~\ref{eqn:type} implies that for every dataset $\dataset$:
\[\pair{a}{b}\in\pop{\declare{n}{A}{B}}{\dataset}\ \Rightarrow\ \inst\ A\ \wedge\ b\ \inst\ B\]
   For a developer, this means that the type of an atom depends only on the relation in which it resides; not on the actual population of the database.
   This allows for static typing, which has well established advantages for the software engineering process~\cite{HanenbergKRTS14,Petersen2014}.

\subsection{Schemas}
\label{sct:Schemas}
   To design an information system, a developer defines a schema in which she defines concepts, relations, and rules.
   The purpose of a schema is to capture the semantics of an information system.
   Rules serve the purpose of invariants,
   i.e.\ properties that the data in the database must satisfy at all times.
   In this paper, variables $u$ and $v$ represent rules.

   A schema $\schema$ is a triple $\triple{\concepts}{\rels}{\rules}$,
   in which $\concepts$ is a finite set of concepts,
   $\rels$ is a finite set of relations,
   and $\rules$ is a finite set of rules.
   We write $\schema_{\infsys}$ to refer to the schema of information system $\infsys$.
   We write $\concepts_{\schema}$, $\rels_{\schema}$, and $\rules_{\schema}$ to refer to $\concepts$, $\rels$, and $\rules$ respectively.
%  Every schema is an element of an infinite set called $\Schema$.

   In order to do static type checking,
   all concepts and relations must be ``known'' in the schema:
\begin{eqnarray}
   \declare{n}{A}{B}\in\rels&\Rightarrow&A\in\concepts\ \wedge\ B\in\concepts
   \label{eqn:relationsIntroduceConcepts}\\
   \triple{a}{\declare{n}{A}{B}}{b}\in\triples&\Rightarrow&\declare{n}{A}{B}\in\rels\ \wedge\ a\ \inst\ A\wedge b\ \inst\ B
   \label{eqn:define R}
\end{eqnarray}

   A rule in a schema serves to constrain the dataset,
   to ensure the semantic integrity of the dataset.
   For every rule $u$ in the schema, we assume there is a function $\id{viol}_u : \Dataset\rightarrow\powerset{\Pair{\Atoms}{\Atoms}}$
   that represents the set of violations of rule $u$ in any dataset.
   We also assume a function $\Sign{u} : \Dataset\rightarrow\Pair{\Concepts}{\Concepts}$
   that represents the signature of a rule, i.e. the type of atoms in violations:
\begin{eqnarray}
   \sign{u}{\dataset}=\pair{A}{B}\ \wedge\ \pair{a}{b}\in\viol{u}{\dataset}&\Rightarrow&a\ \inst\ A\wedge b\ \inst\ B
   \label{eqn:typed violations}
\end{eqnarray}
   To determine whether rule $u$ is satisfied in dataset $\dataset$,
   we define $\sat{u}{\Dataset}$:
\begin{eqnarray}
   \sat{u}{\dataset}&\Leftrightarrow&\viol{u}{\dataset}=\emptyset
   \label{eqn:sat}
\end{eqnarray}
   Violations are used to produce meaningful error messages and to trigger changes in the dataset to restore satisfaction of that rule.

   We assume a special kind of rule that is used for specialization.
   The rule $A\isa B$ (pronounce: $A$ is a $B$) states that any instance of $A$ is an instance of $B$ as well.
\begin{equation}
   \label{eqn:specialization}
   \sat{A\isa B}{\dataset}\ \Leftrightarrow\ \forall a: a\ \inst\ A\rightarrow a\ \inst\ B
\end{equation}
   We call this {\em specialization}, but it is also known as {\em generalization} or {\em subtyping}.
   Specialization is needed to allow statements such as: ``An employee is a person'' or ``A human is a mammal''.
   All specialization rules in a schema together form a partial order $\isa : \Pair{\concepts}{\concepts}$.
   Specialization rules are special for two reasons:
   Firstly, the system keeps $A\isa B$ satisfied automatically by changing $\inst$ whenever needed.
   This means that a system that contains rule $A\isa B$ can never be brought into a state in which $\id{sat}_{A\isa B}$ is False.
   This allows for certain run-time optimizations, which are beyond the scope of this paper.
   A second property that sets specialization rules apart, is that they play a role in the Ampersand compiler.
   For example, code that determines equality between atoms only works for atoms that are an instance of the same concept.
   To ensure that this does not result in problems at run-time, users are required to write all specialization rules explicitly.

   %These rules reflect the meaning (semantics) that is shared among users.

\subsection{Functionality}
   The functionality of an information system is an integral part of it.
   It consists of software.
   In this paper, we are not interested in the precise nature of the functionality.

\subsection{Information Systems}
\label{sct:Information Systems}
   The purpose of an information system is to make data meaningful to its users.
   Users have their own tasks and responsibilities
   and may work from different locations and on different moments.
   This collective use serves a purpose which we loosely call ``the business''.
   As a consequence, the data in a system changes continually.
   To preserve meaning, users are trying to maintain semantic constraints on the data,
   amidst of all changes that are going on around them.

   This section introduces the notion of information system.
   As before, we will suffix the elements of this definition if the situation requires it.
\begin{definition}[information system]
\label{def:information system}
\item An information system $\infsys$ is a tuple $\la\dataset,\schema,\functionality,\roles,\maintain\ra$, in which
\begin{itemize}
   \item dataset $\dataset$ is defined as in section~\ref{sct:Datasets};
   \item schema $\schema$ is defined as in section~\ref{sct:Schemas};
   \item $\functionality$ represents the functionality of the system, which is typically software;
   \item $\roles$ is a set of roles;
   \item $\maintain : \roles\times\rules$ is the maintainance relation between roles and rules from $\schema$.
\end{itemize}
\end{definition}
   A \define{role} is a name that identifies a group of users.
   It serves as a placeholder for a person or a machine (i.e. an actor) who can change the dataset.
   The purpose of a role is to mention an individual user (human) or an automated actor (bot) without knowing who that user is.
   In the sequel, when we talk about a role we actually mean an actor who fulfills that role in the system.

   Every rule in a schema is meant to be kept satisfied invariably.
   The only way to break this invariance is to remove the rule from the schema.
   So the "normal" state of a system is that its dataset $\dataset$ contains no violations of any rule in the schema:
   $\forall u\in\rules:\ \sat{u}{\dataset}$.
   However, the system continually registers events that insert or delete triples in its dataset,
   which may cause rules to be satisfied no longer.
   This is a temporary breach of the invariance of the affected rules,
   which requires a reaction to restore invariance.
   Example: if a rule says that every request must be acknowledged, the receipt of a request will violate this rule.
   Sending a message that acknowledges this receipt will cause the rule to be satisfied again.

   In this paper we assume two ways of enforcing satisfaction of rules: automatic or manual.
   Manual enforcement is specified by the developer by assigning a rule $u$ to a role $o$.
   Any actor with role $o$, whether person or machine,
   can restore invariance of rule $u$ by inserting or deleting the right triples to compensate for the violations.
   For this reason, we assume that every rule is assigned to at least one role by means of the relation $\maintain$.
\begin{equation}
   \forall u\in\rules\ \exists o\in\roles:\ o\ \maintain\ u\label{eqn:maintain}
\end{equation}
   $o\ \maintain\ u$ means that role $o$ must keep rule $u$ satisfied, i.e.\ to keep $\sat{u}{\dataset}$ true.
   Not all ways of restoring invariance can be anticipated by an algorithm, which makes manual enforcement a necessary feature.
   As long as rule $u$ is not satisfied, its violations won't go away, stimulating the actor to do something about it.

   Automatic enforcement is specified by the developer with a special syntax,
   which describes not only the rule, but also the way to restore invariance.
   Whenever the dataset violates an enforce rule,
   the system will make other changes to the dataset to satisfy that rule.
   Hence, the violation of the rule exists during a small, finite amount of time in which the system restores its invariance.

   The fact that every rule has at least one role in the relation $\maintain$, means that all roles together are keeping all rules satisfied.

\subsection{Example}
\label{old IS}
   Having defined an information system in mathematical terms, let us discuss a small example.
   For this purpose we use the language Ampersand
   because it makes the examples more appealing to read.
   Let us first define a dataset of seven triples and three relations.
\begin{verbatim}
RELATION takes[Student*Course] =
[ ("Peter", "Management")
; ("Susan", "Business IT")
; ("John", "Business IT")
]
\end{verbatim}
   This declaration introduces a relation with name \verb#takes#,
   source concept \verb#Student#, and
   target concept \verb#Course#.
   It also introduces three triples:
\[\begin{array}{l}
   \triple{\text{\tt "Peter"}}{\declare{\text{\tt takes}}{\text{\tt Student}}{\text{\tt Course}}}{\text{\tt "Management"}}\\
   \triple{\text{\tt "Susan"}}{\declare{\text{\tt takes}}{\text{\tt Student}}{\text{\tt Course}}}{\text{\tt "Business IT"}}\\
   \triple{\text{\tt "John"}}{\declare{\text{\tt takes}}{\text{\tt Student}}{\text{\tt Course}}}{\text{\tt "Business IT"}}
\end{array}\]
   The informal meaning of this relation is that it states which students are taking which courses.
   So, there is a student, Peter, taking the course Management and two students, Susan and John, taking the course Business IT.

   The example system also has a second relation that states which modules are part of which course.
   It is constrained by \verb-[UNI]- to be univalent.
   This means that every \verb-Module- is part of at most one \verb-Course-.
\begin{verbatim}
RELATION isPartOf[Module*Course] [UNI] =
[ ("Finance", "Management")
; ("Business Rules", "Business IT")
; ("Business Analytics", "Business IT")
; ("IT-Governance", "Management")
]
\end{verbatim}
   The third and last relation states which students are enrolled for which module.
   We leave it empty for now.
\begin{verbatim}
RELATION isEnrolledFor[Student*Module]
\end{verbatim}
   We also define a rule, {\tt EnrollRule}, that states that a student can enroll for any module that is part of a course that student takes.
   In Ampersand, which is a syntactically sugared form of relation algebra,
   we give each rule a name and declare a role to preserve its invariance:
\begin{verbatim}
RULE EnrollRule: isEnrolledFor |- takes;isPartOf~
ROLE Administrator MAINTAINS EnrollRule
\end{verbatim}
   The actual constraint, \verb#isEnrolledFor |- takes;isPartOf~#, is written in logic as:
\[\begin{array}{l}\forall s\in\text{\tt Student}, m\in\text{\tt Module}\ \exists c\in\text{\tt Course}:\\
s\ \text{\tt isEnrolledFor}\ m\ \rightarrow\ s\ \text{\tt takes}\ c\ \wedge\ m\ \text{\tt isPartOf}\ c
\end{array}\]

   Summarizing, this example contains a dataset with seven triples
   and a schema with three concepts, three relations, and one rule.

   Now let us check the requirements to verify that this example defines an information system.
   Requirement~\ref{eqn:pop} results in a set $\triples$ that contains seven triples.
   Requirements~\ref{eqn:instance} and~\ref{eqn:type} define the relation $\id{inst}$ to contains the pairs:
   \[\begin{array}{l}
      \pair{\tt "Finance"}{\tt Module}\\
      \pair{\tt "Business Rules"}{\tt Module}\\
      \pair{\tt "Business Analytics"}{\tt Module}\\
      \pair{\tt "IT-Governance"}{\tt Module}\\
      \pair{\tt "Management"}{\tt Course}\\
      \pair{\tt "Business IT"}{\tt Course}\\
      \pair{\tt "Peter"}{\tt Student}\\
      \pair{\tt "Susan"}{\tt Student}\\
      \pair{\tt "John"}{\tt Student}
   \end{array}\]
   Requirements~\ref{eqn:concepts} and~\ref{eqn:relationsIntroduceConcepts} both say that 
   \[\concepts=\{ {\tt Module}, {\tt Course}, {\tt Student}\}\]
   Requirement~\ref{eqn:define R} says that 
   \[\begin{array}{rcl}
      \rels&=&\{\begin{array}[t]{l}
                  \declare{\tt takes}{\tt Student}{\tt Course},\\
                  \declare{\tt isPartOf}{\tt Module}{\tt Course},\\
                  \declare{\tt isEnrolledFor}{\tt Student}{\tt Module}\ \}
                \end{array}
     \end{array}
   \]
   The set of rules $\rules$ contains just one rule: \verb-EnrollRule-.
   So $\concepts$, $\relations$, and $\rules$ complete the schema $\schema$ of this example.
   The dataset $\dataset$ contains the set $\triples$ and the relation $\inst$.
   The set of roles, $\roles$, contains one role: ${\tt Administrator}$
   and the relation $\maintain$ relates ${\tt Administrator}$ to rule\verb-EnrollRule-.
   Requirement~\ref{eqn:typed violations} and~\ref{eqn:sat} are satisfied because $\viol{\tt EnrollRule}{\dataset}$ is empty.
   Requirement~\ref{eqn:specialization} is satisfied because this example contains no specialization.
   So we can conclude that this example satisfies definition~\ref{def:information system}.

%    The language Ampersand is typed, to prevent a substantial amount of programming mistakes at compile-time.
%    This means that every atom is an instance of a concept:
% \begin{eqnarray}
%    \forall a\in\atoms\ \exists A\in\concepts&:&a\ \inst\ A\label{eqn:concepts}
% \end{eqnarray}
%    To do type checking based on the schema only,
%    all relations in triples from the dataset must be ``known'' in the schema:
% \begin{equation}
%    \forall a,r,b:\ \triple{a}{r}{b}\in\triples\ \Rightarrow\ r\in\rels\label{eqn:define R}
% \end{equation}

%    In a typed information system, atoms in a relation have a type as declared in a relation.
%    For this reason, every relation $r$ has not only a name, but also a source concept and a target concept.
%    We write $r=\declare{nm}{A}{B}$ to denote that relation $r$ has name \id{nm}, source concept $A$, and target concept $B$.
% \begin{eqnarray}
%    \forall\declare{n}{A}{B}\in\rels&:&A\in\concepts\ \wedge\ B\in\concepts\label{eqn:concepts}\\
%    \forall\triple{a}{\declare{n}{A}{B}}{b}\in\triples&:&a\ \inst\ A\ \wedge\ b\ \inst\ B\label{eqn:type}
% \end{eqnarray}
%    The type checker in the Ampersand compiler will spot violations and refuse to generate a system if they occur.

%    To compare atoms for equality,
%    the set $\rels$ contains an identity relation $\ident{C}$ for every concept $C$ in which all atoms occur.
%    It satisfies:
% \begin{equation}
%    \forall a,b:\ a\ \inst\ C\ \wedge\ b\ \inst\ C\ \Rightarrow\ (a\ \ident{C}\ b\ \Leftrightarrow\ a=b)
% \end{equation}
%    So, $\id{pop}_{\ident{C}}$ is defined by:
% \begin{eqnarray}
%    \forall C\in\concepts&:&\pop{\ident{C}}{\dataset}=\{\pair{a}{a}\mid a\ \inst\ C\}
% \end{eqnarray}

%    Having one rule, one role, a relation $\maintain$ that contains one pair, and a dataset,
%    we have now defined a small system.

\subsection{Data migration}
   To migrate system $\infsys$ to $\infsys'$, we take the following steps:
\begin{enumerate}
   \item Let the old system $\infsys$ have state $\dataset$ and schema $\schema$.
         A developer defines a schema $\schema'$ of the new system as a first step in the deployment of the increment.
   \item The generator generates a migration system $\migrsys$ based on $\schema$ and $\schema'$,
         which gives the user the full functionality of the old system and the new system,
         i.e.\ $\schema_{\migrsys}=(\schema\sqcup\schema')\cup\id{migrationrules}$.
         The schema $\id{migrationrules}$ represents the enforce rules that migrate the existing data to the new dataset.
         TODO: $\id{migrationrules}$ bevat alleen tijdelijke datastructuur, indien nodig.
   \item A migration engineer changes $\id{migrationrules}$ to accommodate the non-standard requirements of the migration,
         yielding migration system $\migrsys'$.
         TODO: de migration engineer kan alleen maar in $\dataset'$ dingen (laten) wijzigen, zodat je altijd terug kunt naar het oude systeem.
   \item The production environment switches instantly from $\infsys$ to $\migrsys'$,
         while $\dataset_{\infsys}\subseteq\dataset_{\migrsys'}$.
         This means that the data of the old system remains intact.
   \item After a finite span of time, $\dataset_{\migrsys'}$ satisfies all its enforce rules.
         This implies that the generated rules plus the changes made by the migration engineer ensure the preservation of the meaning of the old data.
   \item Users may have to perform manual migration steps.
         These are specified as process rules in $\id{migrationrules}$.
         % Dit zijn "kurk" regels.
         The system ensures that the amount of work needed to perform these steps is finite.
   \item The patron of the system can decide whether to carry on with the migration or to roll back to the old system.
         She can do that incrementally by switching off the interfaces one-by-one.
   \item The system retains the data, the schema, and the functionality that is covered by $\schema'$, dropping everything else.
\end{enumerate}

   Let us define the instruments needed to describe the derivation of $\schema_{\migrsys}$.
\begin{definition}[disjoint union of datasets]
\begin{eqnarray}
   \omit\rlap{$\pair{\triples}{\inst}\sqcup\pair{\triples'}{\inst'}$}\notag\\
   &=&\pair{\triples\uplus\triples'}{\inst\uplus^2\inst'}\notag\\
      X\uplus Y&=&\{(x,0)\mid\ x\in X\}\ \cup\ \{(y,1)\mid\ y\in Y\}\\
      X\uplus^2 Y&=&\begin{array}[t]{@{}l}\{((x_1,0),(x_2,0))\mid\ (x_1,x_2)\in X\}\ \cup\\ \{((y_1,1),(y_2,1))\mid\ (y_1,y_2)\in Y\}\end{array}\\
\end{eqnarray}
\end{definition}

\begin{definition}[disjoint union of schemas]
\begin{eqnarray}
   \triple{\concepts}{\rels}{\rules}\sqcup\triple{\concepts'}{\rels'}{\rules'}&=&\triple{\concepts\uplus\concepts}{\rels\uplus\rels'}{\rules\uplus\rules'}
\end{eqnarray}
\end{definition}
% \begin{definition}[disjoint union of datasets]
% \begin{eqnarray}
%    \omit\rlap{$\la\atoms,\concepts,\inst,\isa,\rels,\dataset\ra\sqcup\la\atoms',\concepts',\inst',\isa',\rels',\dataset'\ra$}\notag\\
%    &=&\la\atoms\uplus\atoms',\ \concepts\uplus\concepts',\ \inst\uplus^2\inst',\ \isa\uplus^2\isa',\ \rels\uplus^3\rels',\ \dataset\uplus^5\dataset'\ra\notag\\
%       X\uplus Y&=&\{(x,0)\mid\ x\in X\}\ \cup\ \{(y,1)\mid\ y\in Y\}\\
%       X\uplus^2 Y&=&\begin{array}[t]{@{}l}\{((x_1,0),(x_2,0))\mid\ (x_1,x_2)\in X\}\ \cup\\ \{((y_1,1),(y_2,1))\mid\ (y_1,y_2)\in Y\}\end{array}\\
%       X\uplus^3 Y&=&\begin{array}[t]{@{}l}\{\declare{(n,0)}{(A,0)}{(B,0)}\mid\ \declare{n}{A}{B}\in X\}\ \cup\\ \{\declare{(n,1)}{(A,1)}{(B,1)}\mid\ \declare{n}{A}{B}\in Y\}\end{array}\\
%       X\uplus^5 Y&=&\begin{array}[t]{@{}l}\{\triple{(a,0)}{\declare{(n,0)}{(A,0)}{(B,0)}}{(b,0)}\mid\ \triple{a}{\declare{n}{A}{B}}{b}\in X\}\ \cup\\ \{\triple{(a,1)}{\declare{(n,1)}{(A,1)}{(B,1)}}{(b,1)}\mid\ \triple{a}{\declare{n}{A}{B}}{b}\in Y\}\end{array}
% \end{eqnarray}
% \end{definition}
Note that if $\dataset$ and $\dataset'$ are datasets, then so is $\dataset\sqcup\dataset'$.

\subsection{Example}
   Let us proceed to specify a new system, $\infsys'$, to illustrate a (toy) migration.
   We will use the example in section~\ref{old IS} as the old system
   and call that $\infsys$ in this section.
   Its population reflects the state of the system just before the migration.

   Information system $\infsys'$, the new system, is specified by:
\begin{verbatim}
   RELATION takes[Student*Course]
   RELATION isPartOf[Module*Course] [UNI]=
      [ ("IT-Governance", "Business IT") ]
   RELATION isEnrolledFor [Student*Module] =
      [ ("Susan", "Business Analytics")
      ; ("Susan", "IT-Governance")
      ; ("Susan", "Business Rules")
      ]
   RULE EnrollRule: isEnrolledFor |- takes;isPartOf~
   ROLE Administrator MAINTAINS EnrollRule
   
   RELATION course[ExamReg*Course] [UNI] =
      [ ("ER1", "Management")
      ; ("ER2", "Business IT")
      ; ("ER3", "Business IT")
      ]
   RELATION student[ExamReg*Student] [UNI] =
      [ ("ER1", "Peter")
      ; ("ER2", "Susan")
      ]
   RULE ExamRule1: student~;course |- takes
   RULE ExamRule2: student~;course;isPartOf~ |- isEnrolledFor
\end{verbatim}
   This specification shows that $\infsys'$ gets to keep all three relations and the rule from $\infsys$.
   Some new population, two new relations, and two new rules have been added to $\infsys'$.
   $\infsys'$ contains exam registrations (concept \verb-ExamReg-), which is new.
   In an exam registration, a student registers for the examination of a course.
   $\infsys'$ contains two extra rules: \verb-ExamRule1- and \verb-ExamRule2-.
   The first one says that an exam registration requires that a student actually takes the course.
   In logic, the constraint $p_{\tt ExamRule1}$ is written as:
\[\begin{array}{l}\forall s\in\text{\tt Student}, e\in\text{\tt ExamReg}, c\in\text{\tt Course}:\\
   e\ \text{\tt student}\ s\ \wedge\ \ e\ \text{\tt course}\ c\ \rightarrow\ s\ \text{\tt takes}\ c\\
\end{array}\]
   Another requirement, \verb-ExamRule2-, is that the student is enrolled for every module that is part of the course.
   Its constraint, $p_{\tt ExamRule2}$, is written in logic as:
\[\begin{array}{l}\forall s\in\text{\tt Student}, e\in\text{\tt ExamReg}, m\in\text{\tt Module}, c\in\text{\tt Course}:\\
   e\ \text{\tt student}\ s\ \wedge\ \ e\ \text{\tt course}\ c\ \wedge\ m\ \text{\tt isPartOf}\ c\ \rightarrow\ s\ \text{\tt isEnrolledFor}\ m
\end{array}\]
   Ampersand also derives the following violation sets:
\[\begin{array}{l}
   \viol{\text{\tt ExamRule1}}{\dataset}\\
   \hspace{1cm}=\{\pair{s}{c}\mid\exists e:\ e\ \text{\tt student}\ s\ \wedge\ \ e\ \text{\tt course}\ c\ \wedge\neg(s\ \text{\tt takes}\ c)\}\\
   \viol{\text{\tt ExamRule2}}{\dataset}\\
   \hspace{1cm}=\{\pair{s}{m}\mid\exists e,c:\ e\ \text{\tt student}\ s\ \wedge\ \ e\ \text{\tt course}\ c\ \wedge\ m\ \text{\tt isPartOf}\ c\ \wedge\neg(s\ \text{\tt isEnrolledFor}\ m)\}
\end{array}\]

   So let us now turn to the migration.
   The intention of any migration is to preserve as much of the data from $\infsys$ as possible into $\infsys'$,
   while adding the new triples to $\infsys'$.
   So the first thing to do is to take the disjoint union of both systems.
   This yields:
\begin{verbatim}
   RELATION takes[Student*Course] =
      [ ("Peter", "Management")
      ; ("Susan", "Business IT")
      ; ("John", "Business IT")
      ]
   
   RELATION isPartOf[Module*Course] [UNI]=
      [ ("Finance", "Management")
      ; ("Business Rules", "Business IT")
      ; ("Business Analytics", "Business IT")
      ; ("IT-Governance", "Management")
      ; ("IT-Governance", "Business IT")
      ]
   
   RELATION isEnrolledFor [Student*Module] =
      [ ("Susan", "Business Analytics")
      ; ("Susan", "IT-Governance")
      ; ("Susan", "Business Rules")
      ]
   
   RULE EnrollRule: isEnrolledFor |- takes;isPartOf~
   ROLE Administrator MAINTAINS EnrollRule
   
   RELATION course[ExamReg*Course] [UNI] =
      [ ("ER1", "Management")
      ; ("ER2", "Business IT")
      ; ("ER3", "Business IT")
      ]
   RELATION student[ExamReg*Student] [UNI] =
      [ ("ER1", "Peter")
      ; ("ER2", "Susan")
      ]
   RULE ExamRule1: student~;course |- takes
   RULE ExamRule2: student~;course;isPartOf~ |- isEnrolledFor
\end{verbatim}
   If we check this for violations, we get the following results:
\begin{verbatim}
   There are 2 violations of RULE "ExamRule2":
      ("Peter", "IT-Governance")
      ("Peter", "Finance")
   ==============================
   There is one violation of RULE "UNI isPartOf[Module*Course]":
      ("IT-Governance", "IT-Governance")
\end{verbatim}
   Any data migration must anticipate any population in the old system.
   The only thing we know for sure about the ``old'' population is that it satisfies the rules in $\infsys$.

\subsubsection{Strategies for dealing with violations}

The violations of ExamRule2 don't pose a problem: they have a role (administrator) assigned to them.
We allow these violations to occur in the transition to the new system, and expect people with the Administrator role to deal with them.

However, the violations that arise when taking the (non-disjoint) union of the two scripts include violations of rules that the system should maintain.
This means that if we cannot generate software based on the disjoint union as it is.
To resolve this, the simplest solution is to change the roles assigned to the rules that are violated in the disjoint union:
\begin{verbatim}
   RELATION isPartOf[Module*Course]
   RULE isPartOfUNI: isPartOf;isPartOf~ |- I
   ROLE migrationHelper MAINTAINS isPartOfUNI
\end{verbatim}

Naturally, this solution requires effort from people with the role migrationHelper.
This in itself is not an issue: problems need to be solved one way or another.
However, a problem that may occur is that regular users of the system, who are used to the old system, might be adding data to the system that causes violations faster than they can be solved.
To mitigate this, we propose two solutions.

One solution is to state that the only violations that may occur in \verb=isPartOfUNI= are those that were present when the migration started:
\begin{verbatim}
   RELATION isPartOf[Module*Course]
   RULE isPartOfUNI: isPartOf;isPartOf~ |- I
   ROLE migrationHelper MAINTAINS isPartOfUNI
   
   RELATION isPartOfUNI_violations[Module*Module]
     = [("IT-Governance", "IT-Governance")]
   RULE isPartOfUNI_progress: -(isPartOf;isPartOf~) /\ I |- isPartOfUNI_violations
   SYSTEM MAINTAINS isPartOfUNI_progress
   
   ENFORCE isPartOfUNI_violations :< -(isPartOf;isPartOf~) /\ I  
\end{verbatim}

In this first solution, the first rule is the same as with the simple solution.
The second rule prevents any new violation from occurring.
Existing violations are recorded in the \verb=isPartOfUNI_violations= relation, and the rule \verb=isPartOfUNI_progress= prevents the set of violations to \verb=isPartOfUNI= from growing beyond this.
The \verb=ENFORCE= statement at the end of this code snippet states that the relation recording existing violations should be shrunk whenever violations are solved.
This way, we ensure that violations cannot re-occur.
As a whole, this means that \verb=migrationHelper= has a finite task.

As a second solution, we observe that the data in the designed system does not have any violations.
Consequentially, the cause of the violations comes from the triples in the old system.
Rather than taking the union of all triples, we can take the disjoint union instead;
\begin{verbatim}
   RELATION isPartOf_old[Module*Course] [UNI]=
      [ ("Finance", "Management")
      ; ("Business Rules", "Business IT")
      ; ("Business Analytics", "Business IT")
      ; ("IT-Governance", "Management")
      ]
   RELATION isPartOf[Module*Course] [UNI]=
      [ ("IT-Governance", "Business IT") ]
   RELATION isPartOf_ignored[Module*Course]
   
   RULE isPartOf_isUnion:  isPartOf_old :- isPartOf \/ isPartOf_ignored
   ROLE migrationHelper MAINTAINS isPartOf_isUnion
   
   ENFORCE isPartOf_ignored :> isPartOf
\end{verbatim}

\begin{verbatim}
   RELATION isPartOf_old[Module*Course] [UNI]=
      [ ("Finance", "Management")
      ; ("Business Rules", "Business IT")
      ; ("Business Analytics", "Business IT")
      ; ("IT-Governance", "Management")
      ]
   RELATION isPartOf[Module*Course] [UNI]=
      [ ("IT-Governance", "Business IT") ]
   RELATION isPartOf_ignored[Module*Course]

   ENFORCE isPartOf :> isPartOf_old - isPartOf_ignored
   ENFORCE isPartOf_ignored :> isPartOf
\end{verbatim}
In this second solution, we immediately move to a system in which the \verb=UNI= rule is maintained by the system.
The work for the \verb=migrationHelper= is again finite: the tuples in \verb=isPartOf_old= need to be copied over or explicitly ignored.
Once a tuple is in \verb=isPartOf=, the tuple is copied over to \verb=isPartOf_ignored= by the \verb=ENFORCE= statement, such that regular users can delete it without adding work to anyone with the \verb=migrationHelper= role.
Of course, since this solution starts with fewer pairs in the \verb=isPartOf= relation, other rules could be violated as a result of choosing this solution.
Indeed, ExamRule2 initially has more violations in this scenario.

\begin{definition}[migration dataset]
   \begin{eqnarray}
      &&\begin{array}[t]{@{}l}
         \{ {\tt ENFORCE\ }\declare{(nm',1)}{(A',1)}{(B',1)}\\\quad{\tt\ :=\ }\ident{(A',1)};\declare{(nm,0)}{(A,0)}{(B,0)};\ident{(B',1)}\\
            \mid\ \begin{array}[t]{@{}l}
               \declare{nm}{A}{B}\in\rels\wedge\declare{nm'}{A'}{B'}\in\rels'\wedge\id{nm}=\id{nm'}\ \wedge\\
               \{\pair{A}{A'},\pair{B}{B'}\}\subseteq\isa\cup\isa'\cup\flip{\isa}\cup\flip{\isa'}\}
               \end{array}
           \end{array}\\
           &\cup&\begin{array}[t]{@{}l}
            \{ {\tt CLASSIFY\ }(C',1){\tt\ IS\ }(C,0)\mid\ C\in\concepts,C'\in\concepts',C=C'\}\\
           \end{array}
   \end{eqnarray}
\end{definition}

The ${\tt CLASSIFY\ }(C',1){\tt\ IS\ }(C,0)$ statements add $((C',1),(C,0))$ and  $((C,0),(C',1))$ to the $\isa$ relation. Note that if $\dataset$ is a dataset, and $\dataset'$ is that dataset with pairs added\footnote{TODO: het woord `add' is informeel, dit kan beter} to the $\isa$ relation, then the result is again a dataset.
The ${\tt ENFORCE\ }r{\tt\ :=\ }e$ statements add triples $(x,r,y)$ for all $(x,y)\in e$.
The expressions $e$ in these statements are such that $x\ (\inst \compose \kleenestar{\flip{\isa}})\ (A',1)$ and $y \ (\inst \compose \kleenestar{\flip{\isa}})\ (B',1)$, thus preserving the property that the result is a dataset.

\begin{definition}[]
   \begin{eqnarray}
      \infsys\sqcup\infsys'&=&\la\roles',\rules',\maintain',\dataset\sqcup\dataset'\ra
   \end{eqnarray}
\end{definition}

\subsection{Changes}
% The purpose of this section is to explain why a dataset is structured the way it is.
   In the migration of a dataset we deal with changes to the elements that are not data:
   $\inst$, $\concepts$, and $\rels$.
   Such changes have further reaching consequences, however.
   Changes to $\inst$, $\concepts$, $\rels$ change the data structure in a dataset.
   We define a \define{migration of a dataset} $\la\atoms,\concepts,\inst,\rels,\dataset\ra$ as a change in which one or more of $\inst$, $\concepts$, or $\rels$ change.
   To satisfy the definition of datasets, $\dataset$ and $\atoms$ must change too,
   but the migration should try to preserve the maximal amount of data.

\section{Validation}
   To validate the theory, we have to prove that:
\begin{itemize}
   % -> het migratie-systeem is typefout-vrij (opmerking: als we de type-checker buiten beschouwing willen laten, kunnen we dit niet beschrijven)
   \item the migration system $\migrsys$ is free of type errors.
   % -> het migratie-systeem is vrij van overtredingen op regels die het migratie-systeem moet bewaken
   \item the migration system $\migrsys$ is an information system (definition~\ref{def:information system}),
         which implies that its population yields no violations.
   % -> het migratie-systeem bevat alle oude data, ihb nog steeds na het toepassen van de enforce regels
   \item $\migrsys$ contains the old data, insofar its enforce rules have not deleted it.
   % -> er is een pad naar ingebruikname van het nieuwe systeem (vanaf de initiÃ«le toestand van het migratie-systeem)
   \item there is a path to releasing the new system, starting with the initial state of the $\migrsys$.
   % -> corollary: op het moment van ingebruikname van het nieuwe systeem, is het migratie-systeem vrij van overtredingen op regels die het nieuwe systeem moet bewaken
   \item As a result, the new system $\infsys'$ is void of violations at the start of its production life.
   % -> optioneel: na een begrensd aantal 'voortgangs-stappen' kan het nieuwe systeem in gebruik genomen worden
   \item Optionally, the new system can be used in production after a limited number of steps.
   % -> optioneel: er bestaat een migratie-systeem dat de oude functionaliteit behoudt (mogelijk uitbreidt) totdat het nieuwe systeem in gebruik genomen is?
   \item Optionally, there is a migration systen that preserves the functionality of the old system until the new system has been put in production.
\end{itemize}
% Bewijsverplichtingen:

\section{Conclusions}
\begin{itemize}
   \item the part of the data that does not change can be migrated automatically;
   \item the part of the migration that can be automated is usually not sufficient;
         it takes additional human creativity to complete the migration specification;
\end{itemize}
\section{Bibliography}
\bibliographystyle{elsarticle-harv}
\bibliography{doc}


\end{document}
