\documentclass{elsarticle}
% \usepackage{graphicx}
\usepackage{hyperref}
%\usepackage{multicol}
%\usepackage{footmisc}
\usepackage{amstext}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{amsthm}
\usepackage[english]{babel}
%\usepackage[official,right]{eurosym}
\selectlanguage{english}
\hyphenation{ExecEngine}
\newtheorem{lemma}{Lemma}
\begin{document}
\include{preambleMigrations}

% TODO Algemeen:
% - type van triple-set en atom-set definieren zodat viol_u ?->P(?x?)
%   (Bas: ik heb foute types weggehaald, maar dit nog niet gedefinieerd)
% - nieuwe populatie na de disjoint union zou de populatie met ENFORCE regels toegepast moeten zijn
%  (en in het bijzonder de ISA regels)

% TODO Stef:
% - stukje literatuuronderzoek
% https://ieeexplore.ieee.org/abstract/document/7445334?casa_token=ECzi6XeV2ncAAAAA:KhWzB8XBFOUJ0C6AD-XjX_ryuA9ARvTd3gm6RR-ZNiR8sZ1858FJpQ7zKQhkAZDlv8IjPdgD
% https://ieeexplore.ieee.org/abstract/document/8549944?casa_token=9qiGqNzh2Q0AAAAA:C-cYogExB35nGxQdxLcdBh4JoLNvM0OHedAMhCbB5V4kb4_6nzHUvc23xSJbeoBu67LSiz-Y
% https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.651.9298&rep=rep1&type=pdf
% https://www.scirp.org/html/4-7800724_106592.htm
% https://www.researchgate.net/profile/Ranjana-Badre/publication/318665687_GUI_for_Data_Migration_and_Query_Conversion/links/5b45bbea0f7e9b1c722386e5/GUI-for-Data-Migration-and-Query-Conversion.pdf
% https://journal3.uin-alauddin.ac.id/index.php/literatify/article/view/12567

% Bewijsverplichtingen:
% -> het migratie-systeem is typefout-vrij (opmerking: als we de type-checker buiten beschouwing willen laten, kunnen we dit niet beschrijven)
% -> het migratie-systeem is vrij van overtredingen op regels die het migratie-systeem moet bewaken
% -> het migratie-systeem bevat alle oude data, ihb nog steeds na het toepassen van de enforce regels
% -> er is een pad naar ingebruikname van het nieuwe systeem (vanaf de initiÃ«le toestand van het migratie-systeem)
% -> corollary: op het moment van ingebruikname van het nieuwe systeem, is het migratie-systeem vrij van overtredingen op regels die het nieuwe systeem moet bewaken
% -> optioneel: na een begrensd aantal 'voortgangs-stappen' kan het nieuwe systeem in gebruik genomen worden
% -> optioneel: er bestaat een migratie-systeem dat de oude functionaliteit behoudt (mogelijk uitbreid) totdat het nieuwe systeem in gebruik genomen is?

\title{Data migration under a Changing Schema}
\author[ou,ordina]{Stef Joosten\fnref{fn1}}
\ead{stef.joosten@ou.nl}
\author[umn]{Sebastiaan Joosten\fnref{fn2}}
\address[ou]{Open Universiteit Nederland, Heerlen, the Netherlands}
\address[ordina]{Ordina NV, Nieuwegein, the Netherlands}
\address[umn]{University of Minnesota, Minneapolis, USA}
\fntext[fn1]{ORCID 0000-0001-8308-0189}
\fntext[fn2]{ORCID 0000-0002-6590-6220}

\begin{abstract}
   To support incremental software development with a software generator bears the potential of decreasing the time to deploy changes.
   However, an increment that changes the database schema causes a nontrivial data migration problem.
   The problem is how to preserve the semantics of the data as much as possible and satisfy elementary business requirements at the same time.

   To generate an information system in a non-incremental manner is already
   provided for by the Ampersand project.
   This paper is about changing an already generated system in production.
   It develops a theory for deploying an incremental change,
   making Ampersand more useful for agile software development.
   We propose a migration method that features zero down-time,
   and features an undo after deployment.
   It also leaves room for the business to finish ongoing work
   after the moment the change has been deployed.
\end{abstract}

\begin{keyword}
generative software\sep incremental software development\sep data migration\sep relation algebra\sep Ampersand\sep schema change
\end{keyword}
\maketitle

\section{Introduction}
\label{sct:Introduction}
   To automate the datamigrations that come with incrementally developing and deploying information systems;
   that is the purpose of the theory proposed in this paper.

   We believe in generating information systems%
\footnote{In the sequel, the word ``system'' refers to the phrase ``information system''. This simplifies the language a little. }
   to prevent human programming errors.
   For, a mistake not made can never propagate into production.
   We also believe that information systems should be released into production in small increments,
   to obtain shorter release cycles, more frequent releases, and thus a more agile software development process%
   \footnote{These effects can be measured in terms of DevOps metrics such as
   deployment frequency,
   reliability of deployments, and
   change failure rate~\cite{DevOps2021}.}.
   By installing upgrades automatically, frequently, in small increments, and without down-time,
   the users experience a system that evolves organically, without even noticing the updates.
   Thus, we believe our theory will help to automate data migrations correctly and reliably.
   The benefits work in two directions.
   Automating data migrations saves effort and errors,
   contributing to more frequent and smaller releases.
   Smaller and faster releases also mean a smaller difference between the existing system and the desired system.
   The smaller that difference, the simpler the migration can be and the more of it can be automated.

   In each increment, we distinguish an ``existing system'' and a ``desired system''.
   A difference of schemas complicates the migration of data between the existing system and the desired system.
   The purpose of incremental evolution poses specific requirements to data migration,
   which sets it apart from data migrations for other purposes.
   For instance, if a data migration is done for switching to another platform or to different technology,
   e.g.~\cite{Gholami2016,Bisbal1999},
   migration engineers may exclude new functionality to avoid introducing new errors in an otherwise error-prone migration process.
   In our work, however, introducing new functionality and removing obsolete functionality is the very purpose of an increment.
   Data migration literature sometimes uses a different notion of increment.
   For example, Ataei, Khan, and Walkingshaw~\cite{Ataei2021,Walkingshaw2014} define an increment as a variation between two data structures.
   They show how to unify databases with slight variations by preserving all variations in one comprehensive database.
   In our work, however, we want to get rid of the existing system and not retain it.
   So, we need to look at a different approach.
   Examples like these have convinced us to define a new method for data migration that is specifically meant for systems in production that are
   evolving incrementally.
   
   The contribution of this paper is to derive a migration schema from two artifacts: an existing system
   (which has its own schema and dataset) and a new schema (which specifies the desired system).
   The purpose of our theory is to develop tools.
   These tools must satisfy such practical requirements as
   zero down-time updates and ways to cope with data pollution.
   In practice, more frequent updates require that updates come with zero down-time.
   For our theory, zero down-time means that all human activity for the data migration can be done without taking the system out of production.
   We consider any delay caused by automatically switching from the existing system to the desired one negligible,
   assuming a modern deployment platform that does this with zero down-time.
   
   Our theory is based on the following assumptions and requirements:
\begin{itemize}
   \item the data migration is meant to deploy a software increment in production;
   \item the existing data set may be polluted, but it satisfies its schema;
   \item the data migration may require human interaction, which may take time;
   \item the meaning of data must be preserved;
   \item the business continues during the migration without interruption;
   \item there is a compiler to generate an information system from a given schema.
\end{itemize}
   The theory we develop focuses on the correctness of the migration.
   Efficiency of the migration is beyond the scope of this paper.

\section{Analysis}
   The purpose of an information system is to make data meaningful to its users.
   Users have their own tasks and responsibilities
   and may work from different locations and on different moments.
   This collective use by multiple users serves a purpose which we loosely call ``the business''.
   To preserve meaning, the business maintains semantic constraints on the data,
   amidst of all changes that are going on around them.
   
   Information systems are typically used by distributed actors (both users and computers) who continually work with data.
   As a consequence, the data in a system changes continually.
   In practice, actors ``talk to'' the system through an ingress mechanism, which connects each user to the right service(s).
   We assume that the ingress function is provided by the deployment platform, so it is beyond the scope of this paper.
   We also assume that every service runs independent of other services,
   meaning that the service can be stopped, (re)started and substituted without disrupting the other services.

   Every system contains a dataset, which represents the state of the system.
   Every service produces and consumes events that may change the state of the system.
   This state is represented in a persistent store, aka the database%
\footnote{Whether the database is local, remote, or globally distributed is immaterial for the theory in this paper.}.
   Events that the system detects may cause the state to change.
\begin{figure}[bht]
   \begin{center}
     \includegraphics[scale=.45]{datamigration-Pre-migration.drawio.png}
   \end{center}
\caption{Anatomy of an information system}
\label{fig:pre-migration}
\end{figure}
   To keep our theory technology independent, we will assume that a dataset contains triples.
   This makes our theory valid for any kind of database that triples can represent,
   such as SQL databases, object-oriented databases, graph databases, triple stores, and other no-SQL databases.
   We will also assume that the system's semantics are represented as constraints,
   which can be implemented most database management systems as integrity rules that keep the constraints satisfied.

   In this work, we use the Ampersand compiler~\cite{Joosten-JLAMP2018},
   to validate our theory.
   Ampersand is a tool to generate information systems.
   It works with relations in a way similar to Alloy~\cite{Alloy2006} and allegories%
\footnote{Allegories are a specific type of categories, which the reader does not need to understand for the purpose of this paper}~\cite{Zielinski2013}.
   Each rule in Ampersand is a constraint (aka invariant), which the Ampersand run-time engine keeps satisfied as long as that rule lives%
\footnote{The dotted lines in the diagrams show which set of rules the engine keeps satisfied.}.
   So, the user spends no effort to extract constraints from specifications or (even more laborious) from code
   because all constraints are explicitly available as rules in the code.
   This, and the absence of imperative code in an Ampersand script, makes Ampersand a suitable platform to validate the theory.
   It also allows us to be explicit about ``preserving the meaning as much as possible''.
   An Ampersand script contains just enough information to generate a complete system,
   which means that a classical database schema (i.e.\ data structure plus semantics) can be extracted from the Ampersand script.
   For the purpose of this paper we can equate an Ampersand script with the schema of the generated system.

   Data migration occurs when a desired system replaces an existing one,
   while preserving the meaning of the present data as much as possible~\cite{Spivak2012}.
   Just copying the set of data from the existing system to the desired is obviously wrong if the schemas of both systems differ.

   Complications may arise as a result of data pollution,
   which may require human interventions to resolve.
   We distinguish data pollution in the following categories:
\begin{itemize}
   \item data pollution that violates a constraint in the schema.
   \item data pollution that violates a constraint that is not in the schema.
   \item data pollution that cannot be captured in constraints on the dataset,
   such as a street address that has become obsolete because someone has moved without notification.
   \item data pollution that is a consequence of an erroneous constraint in the schema.
\end{itemize}
   The first category is about violations of constraints from the schema.
   Such violations do not occur in our theory because the existing system was generated by Ampersand,
   so the existing system has kept all constraints from the schema satisfied.
   This sets our approach apart from other approaches to formalize data migration, e.g.~\cite{Thalheim2013}.
   The second category, data pollution that is not captured by any of the constraints mentioned in the schema,
   can be dealt with by adding constraints in the desired system.
   In that case, a migration engineer must ensure that the desired system can start with data that satisfies all constraints in its schema.
   So some human intervention may be necessary in specific cases.
   The third category of pollution must be dealt with by working procedures, to prevent such pollution as much as possible.
   In some cases, constraints on data can be formulated,
   so the system can help the user and simplify (or eliminate) the corresponding working procedure.
   So, such constraints may help to move this type of pollution to one of the other categories.
   The last category of data pollution, erroneous constraints, must be fixed by replacing them by the correct constraints in the desired system.

   This illustrates that automating data migrations may require user intervention,
   either by a migration engineer or by end-users.
   In our approach, such user interventions are given some time by
   defining two distinct moments:
\begin{enumerate}
   \item the moment of transition (the MoT), i.e. the moment the desired system is made available to users;
   \item the moment of no return (the MoN), i.e. the moment that the migration cannot be undone other than by a backup-procedure%
\footnote{Backup mechanisms are outside the scope of this paper.}.
\end{enumerate}

   During the time between the MoT and MoN,
   we propose to keep the existing system and the desired system alive, side by side, as shown in figure~\ref{fig:migration phase}.
\begin{figure}[bht]
   \begin{center}
     \includegraphics[scale=.35]{datamigration-Migration phase.drawio.png}
   \end{center}
\caption{Migration phase}
\label{fig:migration phase}
\end{figure}

   The migration requires a third schema, which specifies the migration itself.
   The migration schema comprises the schemas of both the existing system and the desired system.
   The rules of the migration schema will cause the correct data to be transported into the desired system.
   This migration schema preserves data from the existing system, it may introduce new data automatically (generated from the existing dataset),
   and it may require users to introduce new data (which will take time). 
   So the idea of a migration schema is to compute the disjoint union of the existing and the desired schemas
   and to present it to the migration engineer in the form of source code,
   so the migration engineer can change everything she wants to suit the particulars of the data migration.

%    A naive migration might proceed as follows:
%    Launch the desired system in parallel to the existing one, copy data from the existing to the desired system, and have everyone use the desired system.
%    However, numerous issues might impede that plan.
%    The following examples illustrate the practical issues that may occur:
% \begin{enumerate}
% \item Data required in the desired system is missing in the existing system.
%    There may be no way in the existing system to enter that data.
%    An example could be that every reimbursement form needs to have an address associated to it to mail the check to, but address information is not stored in the existing system:
%    The existing system required the reimbursement office to look up employee's addresses from a hand-written list they had on their desk.

%    This issue will require somebody to insert the missing addresses in the desired system.
% \item Data in the existing system is wrong but cannot be corrected there due to how the existing system was designed.
%    An example is if the existing system only allows approvals to be entered as the current user, and the CEO has always insisted that her administrative staff enters the approvals into the system for her.
%    This may result in approvals being entered as admin staff, where it was really the CEO making the approval.

%    This issue will require that the incorrect registration of staff members is corrected in the desired system.
% \item Data in the existing system does not satisfy invariants of the desired system.
%    There may be no way in the existing system of making the data satisfy those invariants.
%    We can use the same example as in the previous bullet point,
%    but add the requirement (in the desired system) that every purchase above a certain amount needs to be approved by the CEO.

%    This issue will require somebody to change the data to satisfy the broken invariants in the desired system.
% \item A way of entering data into the existing system is missing in the desired system.
%    People or automated processes might rely on these ways of entering data.
%    An example could be that when employees turned their computers on or off,
%    an ad-hoc script would automatically check them in- and out to determine the number of hours they worked.
%    A handful of employees still relies on this.

%    This issue will require that users of obsolete functionality are informed and given a way to cope with the missing functionality where appropriate.
% \item Data present in the existing system cannot be stored in the desired system.
%    An example could be that references to physical locations where original receipts are kept are stored in the existing system,
%    but the desired system relies on scans of receipts and allows the originals to be destroyed or not submitted.

%    This issue will require that the existing data is kept until the original receipts have been scanned.
% \end{enumerate}

% (@Bas, het volgende zit al in de oplossingen sfeer. Willen we dat hier al doen?)
%    To mitigate these issues, we:
   
%    \begin{enumerate}
%    \item Allow `missing' triples requirements to be ignored during migration.
%    \item Allow triples to be migrated while being marked as needing correction.
%    \item Allow invariants in the desired system to be ignored for certain triples during migration.
%    \item Allow continued use of interfaces of the existing system, data entered into the existing system via those interfaces needs to be continuously copied to the desired system.
%    \item Retain data in the existing system until it can be marked as ready to be phased out.
%    \end{enumerate}   

   During the migration phase, transactions in the existing system are allowed because the migration engine will transport them to the desired system.
   This allows the business to finish transactions in the existing system while the desired system is already up and running.
   Similarly, the existing system must execute transactions from the desired system, just in case the business calls off the migration.
   Rules in the migration system that require user interaction are given time by requiring that the migration phase ends
   only after all migration rules are at rest.
   At the MoN, the migration engineer takes down everything but the desired system,
   leaving the business with a successful migration.
\begin{figure}[bht]
   \begin{center}
     \includegraphics[scale=.35]{datamigration-Post-migration.drawio.png}
   \end{center}
\caption{The system after the data migration}
\label{fig:post-migration}
\end{figure}

   In the following section we introduce the definitions required to migrate data from one system to another.

\section{Terminology}
\label{sct:Terminology}
   An {\em information system} is a combination of dataset, schema, and functionality.
   For the purpose of this paper, we ignore the functionality because it does not impact the migration.
   We start with datasets in section~\ref{sct:Datasets}, then we define schemas in section~\ref{sct:Schemas},
   and then we can define information systems.

\subsection{Datasets}
\label{sct:Datasets}
   A dataset $\dataset$ describes a set of structured data, which is typically stored persistently in a database of some kind.
   We write $\dataset_{\infsys}$ to refer to the dataset of a particular information system $\infsys$.
   The purpose of a dataset is to describe the data of a system at one point in time. 
   Before defining datasets, we must first define the constituent notions of atom, concept, relation, and triple.
   
   Atoms serve as data elements.
   Atoms are values without internal structure of interest, meant to represent atomic data elements (e.g. dates, strings, numbers, etc.) in a database.
   From a business perspective, atoms represent concrete items of the world,
   such as \atom{Peter}, \atom{1}, or \atom{the king of France}.
   By convention throughout the remainder of this paper, variables $a$, $b$, and $c$ represent \emph{atoms}.
   All atoms are taken from an infinite set called $\Atoms$.
   
   Concepts are names that group atoms of the same type.
   All concepts are taken from an infinite set $\Concepts$.
   $\Concepts$ and $\Atoms$ are disjoint.
   For example, a developer might choose to classify \atom{Peter} and \atom{Melissa} as \concept{Person},
   and \atom{074238991} as a \concept{TelephoneNumber}.
   In this example, \concept{Person} and \concept{TelephoneNumber} are concepts.
   We will use variables $A$, $B$, $C$, $D$ to represent concepts.

   The relation $\inst:\Pair{\Atoms}{\Concepts}$ is the instance relation between atoms and concepts.
   The expression $a\ \inst\ C$ means that atom $a$ is an \emph{instance} of concept $C$.
   This relation is used in the type system, in which $\inst$ assigns a type to every atom in the dataset.

   Relations serve to organize and store data, to allow a developer to represent facts.
   In this paper, variables $r$, $s$, and $d$ represent relations.
   All relations are taken from an infinite set $\Rels$.
   $\Rels$ is disjoint from $\Concepts$ and $\Atoms$.
   Every relation $r$ has a name, a source concept, and a target concept.
   We write $r=\declare{n}{A}{B}$ to denote that relation $r$ has name $n$, source concept $A$, and target concept $B$.

   Triples serve to represent data.
   A triple is an element of $\Triple{\Atoms}{\Rels}{\Atoms}$.

   A dataset $\dataset$ is a tuple $\pair{\triples}{\inst}$ that satisfies:
\begin{eqnarray}
   \forall\triple{a}{\declare{n}{A}{B}}{b}\in\triples&:&a\ \inst\ A\ \wedge\ b\ \inst\ B
   \label{eqn:wellTypedEdge}
\end{eqnarray}
   For example, $\triple{\text{\atom{Peter}}}{\declare{\id{phone}}{\tt Person}{\tt TelephoneNumber}}{\text{\atom{074238991}}}$ is a triple.
   Equation~\ref{eqn:wellTypedEdge} says that \atom{Peter} is an instance of {\tt Person} and \atom{074238991} is an instance of {\tt TelephoneNumber}.
   In practice, users can say that Peter has telephone number 074238991.
   So, the ``thing'' that \atom{Peter} refers to (which is Peter) has \atom{074238991} as a telephone number.
   This ``meaning from practice'' has no consequences in the formal world.
   We leave it entirely up to a user to attach a practical meaning to a triple.

   In case we need to be explicit about the dataset,
   we will write $\triples_{\dataset}$ and $\inst_{\dataset}$ to refer to $\triples$ and $\inst$ respectively.
   Every dataset is an element of an infinite set called $\Dataset$.
   To save writing in the sequel, we may write $a\ r\ b$ to denote that $\triple{a}{r}{b}\in\triples$.

   A relation $r$ can serve as a container of pairs,
   as defined by the function $\id{pop}_r:\Dataset\rightarrow\powerset{\Pair{\Atoms}{\Atoms}}$.
   It defines a set of pairs, which we call the population of $r$:
\begin{equation}
   \pop{r}{\dataset}\ =\ \{ \pair{a}{b}\mid\ \triple{a}{r}{b}\in\triples_{\dataset}\}
\label{eqn:pop}
\end{equation}
   Equation~\ref{eqn:wellTypedEdge} implies that for every dataset $\dataset$:
\[\pair{a}{b}\in\pop{\declare{n}{A}{B}}{\dataset}\ \Rightarrow\ a\ \inst\ A\ \wedge\ b\ \inst\ B\]
   For a developer, this means that the type of an atom depends only on the relation in which it resides; not on the actual population of the database.
   This allows for static typing, which has well established advantages for the software engineering process~\cite{HanenbergKRTS14,Petersen2014}.

\subsection{Schemas}
\label{sct:Schemas}
   To design an information system, a developer defines a schema in which she defines concepts, relations, and rules.
   Together, the concepts, relations, and rules capture the semantics of an information system~\cite{Spivak2012}.
   Each rule in a schema serves to constrain the dataset, to ensure its semantic integrity.
   Every rule is an element of an infinite set called $\Rules$,
   which is disjoint from $\Atoms$, $\Concepts$, $\Rels$, and $\Dataset$.
   In this paper, variables $u$ and $v$ represent rules.
   For every rule $u$ in the schema, we assume there is a predicate $\sat{u}{\Dataset}$
   that says whether dataset $\dataset$ satisfies rule $u$.
%    For every rule $u$ in the schema, we assume there is a function $\id{viol}_u : \Dataset\rightarrow\powerset{\Pair{\Atoms}{\Atoms}}$
%    that represents the set of violations%
% \footnote{The definition of ``violation'' is not needed in this paper. We only use the absence of violations to show satisfaction of a rule.}
%    of rule $u$ in any dataset.
%    We also assume a function $\id{sign} : \Rules\rightarrow\Pair{\Concepts}{\Concepts}$
%    that represents the signature of a rule, i.e. the type of atoms in violations:
% \begin{eqnarray}
%    \sign{u}=\pair{A}{B}\ \wedge\ \pair{a}{b}\in\viol{u}{\dataset}&\Rightarrow&a\ \inst\ A\wedge b\ \inst\ B
%    \label{eqn:wellTypedEdged violations}
% \end{eqnarray}
%    To determine whether rule $u$ is satisfied in dataset $\dataset$,
%    we define $\sat{u}{\Dataset}$:
% \begin{eqnarray}
%    \sat{u}{\dataset}&\Leftrightarrow&\viol{u}{\dataset}=\emptyset
%    \label{eqn:sat}
% \end{eqnarray}
%    Violations are used to produce meaningful error messages and to trigger changes in the dataset to restore satisfaction of that rule.

   Rules serve as invariants,
   i.e.\ constraints that the data in the database must satisfy at all times.
   At times in which the dataset does not satisfy a rule, we say that the rule is {\em broken}.
   In those cases, something or someone must take action to fix the situation and restore invariance of that rule.

   A schema $\schema$ is a triple $\triple{\concepts}{\rels}{\rules}$,
   in which $\concepts$ is a finite set of concepts,
   $\rels$ is a finite set of relations,
   and $\rules$ is a finite set of rules.
   We write $\schema_{\infsys}$ to refer to the schema of information system $\infsys$.
   If needed, we write $\concepts_{\schema}$, $\rels_{\schema}$, and $\rules_{\schema}$ to refer to $\concepts$, $\rels$, and $\rules$ respectively.
%  Every schema is an element of an infinite set called $\Schema$.

   In order to do static type checking,
   all concepts and relations must be ``known'' in the schema:
\begin{eqnarray}
   \declare{n}{A}{B}\in\rels&\Rightarrow&A\in\concepts\ \wedge\ B\in\concepts
   \label{eqn:relationsIntroduceConcepts}\\
   \triple{a}{\declare{n}{A}{B}}{b}\in\triples&\Rightarrow&\declare{n}{A}{B}\in\rels
   \label{eqn:define R}
\end{eqnarray}

   We assume a special kind of rule that is used for specialization.
   The rule $A\isa B$ (pronounce: $A$ is a $B$) states that any instance of $A$ is an instance of $B$ as well.
\begin{equation}
   \label{eqn:specialization}
   \sat{A\isa B}{\dataset}\ \Leftrightarrow\ \forall a: a\ \inst\ A\rightarrow a\ \inst\ B
\end{equation}
   We call this {\em specialization}, but it is also known as {\em generalization} or {\em subtyping}.
   Specialization is needed to allow statements such as: ``An employee is a person'' or ``A human is a mammal''.
   The specialization rules in a schema form a partial order $\isa$,
   meaning that $\isa$ is reflexive, transitive, and antisymmetric.
   Specialization rules are special for two reasons:
   Firstly, the system keeps $A\isa B$ satisfied automatically by changing $\inst$ whenever needed.
   This means that a system that contains rule $A\isa B$ can never be brought into a state in which $\id{sat}_{A\isa B}$ is False.
   This allows for certain run-time optimizations, which are beyond the scope of this paper.
   A second property that sets specialization rules apart, is that they play a role in the Ampersand compiler.
   For example, code that determines equality between atoms only works for atoms that are an instance of the same concept.
   To ensure that this does not result in problems at run-time, users are required to write all specialization rules explicitly.

   As a result, if an atom is an instance of concept $A$ and $A\isa B$,
   this atom has all properties that atoms of type $B$ have:
\begin{equation}
   \begin{array}[b]{ll}
      &a\ \inst\ A\wedge b\ \inst\ B\wedge A\isa B\\
      \wedge&a=b\\
      \wedge&b\ r\ x
   \end{array}
      \ \Rightarrow\ a\ r\ x
\end{equation}

\subsection{Information Systems}
\label{sct:Information Systems}
   This section defines the notion of information system.
   As before, we will suffix the elements of this definition if the situation requires it.
\begin{definition}[information system]
\label{def:information system}
\item An information system $\infsys$ is a tuple $\la\dataset,\schema,\roles,\maintain\ra$, in which
\begin{itemize}
   \item dataset $\dataset=\pair{\triples}{\inst}$ is defined as in section~\ref{sct:Datasets};
   \item schema $\schema=\triple{\concepts}{\rels}{\rules}$ is defined as in section~\ref{sct:Schemas};
   \item $\roles$ is a set of roles;
   \item $\maintain : \roles\times\rules$ is the maintainance relation between roles and rules.
\end{itemize}
\end{definition}
   A \define{role} is a name that identifies a group of users.
   It serves as a placeholder for a person or a machine (i.e. an actor) that works with the dataset (i.e. create, read, update, or delete triples).
   The purpose of a role is to mention an individual user (human) or an automated actor (bot) without knowing who that user is.
   In the sequel, when we talk about a role we actually mean an actor who fulfills that role in the system.

   The system is {\em at rest} when every rule in the schema is satisfied:
\begin{equation}
   \forall u\in\rules:\ \sat{u}{\dataset}\label{eqn:satisfaction}
\end{equation}
   However, the system continually registers events that insert or delete triples in its dataset,
   which may cause rules to be broken.
   This requires a reaction to restore invariance.
   Example: if a rule says that every request must be acknowledged, the receipt of a request (which is an event) will violate this rule.
   Sending a message that acknowledges this receipt (another event) will cause the rule to be satisfied again.
   During the time between the receipt of the request and sending the acknowledgment, the rule is (temporarily) not satisfied.

   In this paper we assume two ways of enforcing satisfaction of rules: automatic or manual.
   Manual enforcement is specified by the developer by assigning a rule $u$ to a role $o$.
   Any actor with role $o$, whether person or machine,
   can restore invariance of rule $u$ by inserting or deleting the right triples.
   For this reason, we assume that every rule is assigned to at least one role by means of the relation $\maintain$.
\begin{equation}
   \forall u\in\rules\ \exists o\in\roles:\ o\ \maintain\ u\label{eqn:maintain}
\end{equation}
   $o\ \maintain\ u$ means that role $o$ must keep rule $u$ satisfied, i.e.\ to keep $\sat{u}{\dataset}$ true.
   As long as rule $u$ is broken, the system should notify actors in the role(s) that are administered in $\maintain$ to do something about it.

   Automatic enforcement is specified by the developer with a special syntax,
   the {\em enforce rule}.
   An enforce rule specifies not only the rule, but also the way to restore invariance.
   We assume that the system features an engine that restores invariance of all enforce rules without unneccessary delay.
   Whenever the dataset violates an enforce rule,
   this engine will make other changes to the dataset to satisfy that rule.
   Hence, the rule is not satisfied during a small, finite amount of time.

   The system contains a dedicated role for the enforcement engine,
   so every rule has at least one role in the relation $\maintain$, means that all roles together are keeping all rules satisfied.

\subsection{Example}
\label{sct:Example existing IS}
   Having defined an information system in mathematical terms, let us discuss a small example.
   For this purpose we use the language Ampersand
   because it makes the examples more appealing to read.
   Let us first define a dataset of just a handful of triples and three relations.
\begin{verbatim}
RELATION takes[Student*Course] =
[ ("Peter", "Management")
; ("Susan", "Business IT")
; ("John", "Business IT")
]
\end{verbatim}
   This declaration introduces a relation with the name \verb#takes#,
   source concept \verb#Student#, and
   target concept \verb#Course#.
   The informal meaning of this relation is that it states which students are taking which courses.

   The example system also has a second relation that states which modules are part of which course.
   It is constrained by \verb-[UNI]- to be univalent.
   This means that every \verb-Module- is part of at most one \verb-Course-.
\begin{verbatim}
RELATION isPartOf[Module*Course] [UNI] =
[ ("Finance", "Management")
; ("Business Rules", "Business IT")
; ("Business Analytics", "Business IT")
; ("IT-Governance", "Management")
]
\end{verbatim}
   The third and last relation states which students are enrolled for which module.
   We leave it empty for now.
\begin{verbatim}
RELATION isEnrolledFor[Student*Module]
\end{verbatim}

   To complete the semantics,
   we also define a rule, {\tt EnrollRule}, that states that a student can enroll for any module that is part of a course that student takes.
   In Ampersand, which is a syntactically sugared form of relation algebra~\cite{JoostenRAMiCS2017},
   we give each rule a name and declare a role to maintain its invariance:
\begin{verbatim}
RULE EnrollRule: isEnrolledFor |- takes;isPartOf~
ROLE Administrator MAINTAINS EnrollRule
\end{verbatim}
   The semantics of this rule defines $\sat{\tt EnrollRule}{\dataset}$ as:
\begin{equation}
   \begin{array}{l}
   \forall \pair{s}{m}\in\pop{\tt isEnrolledFor}{\dataset}\ \exists c\in\text{\tt Course}:\\
s\ \text{\tt isEnrolledFor}\ m\ \rightarrow\ s\ \text{\tt takes}\ c\ \wedge\ m\ \text{\tt isPartOf}\ c
\end{array}
\label{eqn:example isEnrolledFor}
\end{equation}
   Rule {\tt EnrollRule} is satisfied because relation $\declare{\tt isEnrolledFor}{\tt Student}{\tt Module}$ is empty.

   Now let us check the requirements to verify that this example defines an information system.
   The Ampersand compiler generates a dataset $\dataset$, which contains a set of triples and a relation $\inst$.
   It defines the set of triples $\triples$ as:
   \[\begin{array}[t]{l}
         \triple{\text{\tt "Peter"}}{\declare{\text{\tt takes}}{\text{\tt Student}}{\text{\tt Course}}}{\text{\tt "Management"}}\\
         \triple{\text{\tt "Susan"}}{\declare{\text{\tt takes}}{\text{\tt Student}}{\text{\tt Course}}}{\text{\tt "Business IT"}}\\
         \triple{\text{\tt "John"}}{\declare{\text{\tt takes}}{\text{\tt Student}}{\text{\tt Course}}}{\text{\tt "Business IT"}}\\
         \triple{\text{\tt "Finance"}}{\declare{\text{\tt isPartOf}}{\text{\tt Module}}{\text{\tt Course}}}{\text{\tt "Management"}}\\
         \triple{\text{\tt "Business Rules"}}{\declare{\text{\tt isPartOf}}{\text{\tt Module}}{\text{\tt Course}}}{\text{\tt "Business IT"}}\\
         \triple{\text{\tt "Business Analytics"}}{\declare{\text{\tt isPartOf}}{\text{\tt Module}}{\text{\tt Course}}}{\text{\tt "Business IT"}}\\
         \triple{\text{\tt "IT-Governance"}}{\declare{\text{\tt isPartOf}}{\text{\tt Module}}{\text{\tt Course}}}{\text{\tt "Management"}}
   \end{array}\]
   The relation $\id{inst}$ contains the pairs:
   \[\begin{array}{l}
      \pair{\tt "Finance"}{\tt Module}\\
      \pair{\tt "Business Rules"}{\tt Module}\\
      \pair{\tt "Business Analytics"}{\tt Module}\\
      \pair{\tt "IT-Governance"}{\tt Module}\\
      \pair{\tt "Management"}{\tt Course}\\
      \pair{\tt "Business IT"}{\tt Course}\\
      \pair{\tt "Peter"}{\tt Student}\\
      \pair{\tt "Susan"}{\tt Student}\\
      \pair{\tt "John"}{\tt Student}
   \end{array}\]
   The tuple $\pair{\triples}{\inst}$ satisfies requirement~\ref{eqn:wellTypedEdge} so this is a dataset $\dataset$ as introduced in section~\ref{sct:Datasets}.

   The Ampersand compiler generates a schema $\schema$, which contains concepts, relations, and rules.
   It defines the set of concepts to satisfy requirement~\ref{eqn:relationsIntroduceConcepts}:
   \[\concepts=\{ {\tt Module}, {\tt Course}, {\tt Student}\}\]
   It defines the set of relations to satisfy requirement~\ref{eqn:define R}:
   \[\begin{array}{rcl}
      \rels&=&\{\begin{array}[t]{l}
                  \declare{\tt takes}{\tt Student}{\tt Course},\\
                  \declare{\tt isPartOf}{\tt Module}{\tt Course},\\
                  \declare{\tt isEnrolledFor}{\tt Student}{\tt Module}\ \}
                \end{array}
     \end{array}
   \]
   And, it defines the set of rules $\rules$ to contain just one rule: \verb-EnrollRule-.
   Requirement~\ref{eqn:specialization} is satisfied because this example contains no specialization.
   So, the schema $\schema=\triple{\concepts}{\rels}{\rules}$ satisfies the requirements from section~\ref{sct:Schemas}.

   Now let us check the definition of information system.
   Ampersand generates a set of roles $\roles=\{{\tt Administrator}\}$ and
   a relation $\maintain$, which contains one pair only: $\pair{{\tt Administrator}}{{\tt EnrollRule}}$.
   Requirement~\ref{eqn:satisfaction} is satisfied because the only rule, {\tt EnrollRule} is satisfied.
   This completes the requirements for $\infsys=\la\dataset,\schema,\roles,\maintain\ra$ from definition~\ref{def:information system}.
   So we can conclude that $\infsys$ is an  information system.

\subsection{Data migration}
   Sections~\ref{sct:Information Systems} and~\ref{sct:Example existing IS} both show a complete information system.
   This section studies the transition from an existing system to a desired one, involving a data migration.

   To migrate system $\infsys$ to $\infsys'$, we take the following steps:
\begin{enumerate}
   \item Let the existing system $\infsys$ have state $\dataset$ and schema $\schema$.
         A developer defines a desired system $\infsys'$, which contains a dataset of its own, $\dataset'$, a schema $\schema'$, and some functionality\footnote{For now, we ignore the functionality.} as illustrated in figure~\ref{fig:post-migration}.
         After the migration, the system will contain the triples from $\dataset$, to preserve the existing data.
         The developer may want to add some new data into the system,
         for instance to initialize new features on the moment of transition (MoT).
         She specifies such data in $\dataset'$.
         If $\schema$ equals $\schema'$, we call the migration trivial because a migration is not necessary.
   \item The generator generates a migration script $\migrsys$ based on $\schema$ and $\infsys'$
         to migrate the data and define the semantics during the migration phase as far as a generator can reasonably predict.
         The schema $\id{migrationrules}$ represents the enforce rules that migrate the existing data to the desired dataset.
   \item A migration engineer changes $\id{migrationrules}$ to accommodate the non-standard requirements of the migration,
         yielding migration script $\migrsys'$.
         This schema also contains data structures and rules that are needed for the duration of the migration phase.
         The migration engineer does not alter the existing system $\infsys$ nor the specification of the desired system $\infsys'$ that the developer has produced.
         As a result, the migration engineer can always restore the existing system because it is part of the migration system.
   \item The production environment switches instantly from $\infsys$ to $\migrsys'$,
         while the data of the existing system remains intact.
         This can be realized by configuring the ingress system to open the route to the desired system,
         while keeping the existing system accessible.
         This event marks the MoT.
         From this point onward, both the existing functionality and the desired functionality are available to users as illustrated in figure~\ref{fig:migration phase}.
   \item To ensure that all data is migrated before the the migration engineer makes the migration irreversible,
         we require that the rules of the the migration system $\rules_{\migrsys'}$ are at rest before the migration engineer takes down the existing system.
         Users (i.e.\ the business) must do the work to satisfy the manually enforced rules.
         Ampersand guarantees that enforce rules will eventually be satisfied.
   \item To give the business an opportunity to evaluate the change,
         we can require a go/no-go decision before the the migration engineer makes the migration irreversible.
   \item In case of a go-decision, the migration engineer can disable the undo-facility as soon as the migration system is at rest.
         In case of a no-go, the migration engineer restores the existing system.
         This event marks the moment of no return (MoN).
   \item The system retains the data, the schema, and the functionality that is covered by $\schema'$, dropping everything else.
         This establishes the post-migration situation (figure~\ref{fig:post-migration}).
\end{enumerate}

   Let us define the instruments needed to describe the derivation of $\schema_{\migrsys}$.
\begin{definition}[disjoint union of datasets]
\begin{eqnarray}
   \rlap{$\pair{\triples}{\inst}\sqcup\pair{\triples'}{\inst'}$}\notag\\
   &=&\pair{\triples\uplus\triples'}{\inst\uplus^2\inst'}\notag\\
      X\uplus Y&=&\{(x,0)\mid\ x\in X\}\ \cup\ \{(y,1)\mid\ y\in Y\}\\
      X\uplus^2 Y&=&\begin{array}[t]{@{}l}\{((x_1,0),(x_2,0))\mid\ (x_1,x_2)\in X\}\ \cup\\ \{((y_1,1),(y_2,1))\mid\ (y_1,y_2)\in Y\}\end{array}\\
\end{eqnarray}
\end{definition}

\begin{lemma}
   If $\dataset$ and $\dataset'$ are datasets, then so is $\dataset\sqcup\dataset'$.
\end{lemma}
   This lemma is being proven in Isabelle/HOL~\cite{Isabelle}. The proof is published \href{location.domain}{here}

\begin{definition}[disjoint union of schemas]
\begin{eqnarray}
   \triple{\concepts}{\rels}{\rules}\sqcup\triple{\concepts'}{\rels'}{\rules'}&=&\triple{\concepts\uplus\concepts}{\rels\uplus\rels'}{\rules\uplus\rules'}
\end{eqnarray}
\end{definition}
% \begin{definition}[disjoint union of datasets]
% \begin{eqnarray}
%    \omit\rlap{$\la\atoms,\concepts,\inst,\isa,\rels,\dataset\ra\sqcup\la\atoms',\concepts',\inst',\isa',\rels',\dataset'\ra$}\notag\\
%    &=&\la\atoms\uplus\atoms',\ \concepts\uplus\concepts',\ \inst\uplus^2\inst',\ \isa\uplus^2\isa',\ \rels\uplus^3\rels',\ \dataset\uplus^5\dataset'\ra\notag\\
%       X\uplus Y&=&\{(x,0)\mid\ x\in X\}\ \cup\ \{(y,1)\mid\ y\in Y\}\\
%       X\uplus^2 Y&=&\begin{array}[t]{@{}l}\{((x_1,0),(x_2,0))\mid\ (x_1,x_2)\in X\}\ \cup\\ \{((y_1,1),(y_2,1))\mid\ (y_1,y_2)\in Y\}\end{array}\\
%       X\uplus^3 Y&=&\begin{array}[t]{@{}l}\{\declare{(n,0)}{(A,0)}{(B,0)}\mid\ \declare{n}{A}{B}\in X\}\ \cup\\ \{\declare{(n,1)}{(A,1)}{(B,1)}\mid\ \declare{n}{A}{B}\in Y\}\end{array}\\
%       X\uplus^5 Y&=&\begin{array}[t]{@{}l}\{\triple{(a,0)}{\declare{(n,0)}{(A,0)}{(B,0)}}{(b,0)}\mid\ \triple{a}{\declare{n}{A}{B}}{b}\in X\}\ \cup\\ \{\triple{(a,1)}{\declare{(n,1)}{(A,1)}{(B,1)}}{(b,1)}\mid\ \triple{a}{\declare{n}{A}{B}}{b}\in Y\}\end{array}
% \end{eqnarray}
% \end{definition}
\begin{lemma}
   If $\schema$ and $\schema'$ are schemas, then so is $\schema\sqcup\schema'$.
\end{lemma}
The proof of this lemma is published \href{location.domain}{here}

To implement the disjoint union, we need to relabel 
\begin{definition}[relabel concepts]
   \[\subst{C}{D}{A}\ =\ \text{\bf if}\ C=A\ \text{\bf then}\ D\ \text{\bf else}\ A\]
\end{definition}
\begin{definition}[relabel concepts in triples]
   \[\subst{C}{D}{\triple{a}{r}{b}} = \triple{a}{\subst{C}{D}{r}}{b}\]
\end{definition}
\begin{definition}[relabel concepts in relations]
   \[\subst{C}{D}{(\declare{n}{A}{B})} = \declare{n}{\subst{C}{D}{A}}{\subst{C}{D}{B}}\]
\end{definition}
\begin{definition}[relabel concepts in $\inst$]
   \[a\ \subst{C}{D}{\inst}\ \subst{C}{D}{A}\ \Leftrightarrow\ a\ \inst\ A\]
\end{definition}
\begin{definition}[relabel concepts in datasets]
   \[\begin{array}{rcl}
      \rlap{$\subst{C}{D}{\pair{\triples}{\inst}}$}\\
      &=&\pair{\{\subst{C}{D}{t}\mid t\in\triples\}}{\subst{C}{D}{\inst}}
   \end{array}\]
\end{definition}
\begin{definition}[relabel concepts in rules]
   \[\sat{u}{\dataset}\ \Leftrightarrow\ \sat{\subst{C}{D}{u}}{\subst{C}{D}{\dataset}}\]
\end{definition}
\begin{definition}[relabel concepts in schemas]
   \begin{eqnarray}
      \rlap{$\subst{C}{D}{\triple{\concepts}{\rels}{\rules}}$}\notag\\
      &=&\begin{array}[t]{l@{}l}
         \la&\{\subst{C}{D}{c}\mid c\in\concepts\}\\
         ,&\{\subst{C}{D}{r}\mid r\in\rels\}\\
         ,&\{\subst{C}{D}{u}\mid u\in\rules\}\ \ra\notag
         \end{array}
   \end{eqnarray}
\end{definition}
   
   The relabeling of concepts is not trivial because of specialization.
   To illustrate this point, suppose $D\isa A$.
   Then the substitution $[C\rightarrow D]$ means that every atom $a$ that used to be of concept $C$ in the existing system, is now not only a $D$ but also an $A$.
   For example by relabeling the concept {\tt Hotel} to {\tt Motel}, in a situation where ${\tt Motel}\isa{\tt Parking}$,
   all atoms that represent hotels might suddenly be expected to have parking lots.
   So, whereas such examples may fly in a technical sense, the developer must use renaming with care to preserve the intended semantics.
\subsection{Example}
   Let us proceed to specify a desired system, $\infsys'$, to illustrate a (toy) migration.
   We will use the example in section~\ref{sct:Example existing IS} as the existing system
   and call that $\infsys$ in this section.
   Its population reflects the state of the system just before the migration.

   Information system $\infsys'$, the desired system, is specified by:
\begin{verbatim}
   RELATION takes[Student*Course]
   RELATION isPartOf[Module*Course] [UNI]=
      [ ("IT-Governance", "Business IT") ]
   RELATION isEnrolledFor [Student*Module] =
      [ ("Susan", "Business Analytics")
      ; ("Susan", "IT-Governance")
      ; ("Susan", "Business Rules")
      ]
   RULE EnrollRule: isEnrolledFor |- takes;isPartOf~
   ROLE Administrator MAINTAINS EnrollRule
   
   RELATION course[ExamReg*Course] [UNI] =
      [ ("ER1", "Management")
      ; ("ER2", "Business IT")
      ; ("ER3", "Business IT")
      ]
   RELATION student[ExamReg*Student] [UNI] =
      [ ("ER1", "Peter")
      ; ("ER2", "Susan")
      ]
   RULE ExamRule1: student~;course |- takes
   RULE ExamRule2: student~;course;isPartOf~ |- isEnrolledFor
\end{verbatim}
   This specification shows that $\infsys'$ gets to keep all three relations and the rule from $\infsys$.
   Some new population, two new relations, and two new rules have been added to $\infsys'$.
   $\infsys'$ contains exam registrations (concept \verb-ExamReg-), which is new.
   In an exam registration, a student registers for the examination of a course.
   $\infsys'$ contains two extra rules: \verb-ExamRule1- and \verb-ExamRule2-.
   The first one says that an exam registration requires that a student actually takes the course.
   In logic, the constraint $p_{\tt ExamRule1}$ is written as:
\[\begin{array}{l}\forall s\in\text{\tt Student}, e\in\text{\tt ExamReg}, c\in\text{\tt Course}:\\
   e\ \text{\tt student}\ s\ \wedge\ \ e\ \text{\tt course}\ c\ \rightarrow\ s\ \text{\tt takes}\ c\\
\end{array}\]
   Another requirement, \verb-ExamRule2-, is that the student is enrolled for every module that is part of the course.
   Its constraint, $p_{\tt ExamRule2}$, is written in logic as:
\[\begin{array}{l}\forall s\in\text{\tt Student}, e\in\text{\tt ExamReg}, m\in\text{\tt Module}, c\in\text{\tt Course}:\\
   e\ \text{\tt student}\ s\ \wedge\ \ e\ \text{\tt course}\ c\ \wedge\ m\ \text{\tt isPartOf}\ c\ \rightarrow\ s\ \text{\tt isEnrolledFor}\ m
\end{array}\]
   % Ampersand also derives the following violation sets:
% \[\begin{array}{l}
   % \viol{\text{\tt ExamRule1}}{\dataset}\\
   % \hspace{1cm}=\{\pair{s}{c}\mid\exists e:\ e\ \text{\tt student}\ s\ \wedge\ \ e\ \text{\tt course}\ c\ \wedge\neg(s\ \text{\tt takes}\ c)\}\\
   % \viol{\text{\tt ExamRule2}}{\dataset}\\
   % \hspace{1cm}=\{\pair{s}{m}\mid\exists e,c:\ e\ \text{\tt student}\ s\ \wedge\ \ e\ \text{\tt course}\ c\ \wedge\ m\ \text{\tt isPartOf}\ c\ \wedge\neg(s\ \text{\tt isEnrolledFor}\ m)\}
% \end{array}\]

   So let us now turn to the migration.
   The intention of any migration is to preserve as much of the data from $\infsys$ as possible into $\infsys'$,
   while adding the new triples to $\infsys'$.
   So the first thing to do is to take the disjoint union of both systems.
   This yields:
\begin{verbatim}
   RELATION takes[Student*Course] =
      [ ("Peter", "Management")
      ; ("Susan", "Business IT")
      ; ("John", "Business IT")
      ]
   
   RELATION isPartOf[Module*Course] [UNI]=
      [ ("Finance", "Management")
      ; ("Business Rules", "Business IT")
      ; ("Business Analytics", "Business IT")
      ; ("IT-Governance", "Management")
      ; ("IT-Governance", "Business IT")
      ]
   
   RELATION isEnrolledFor [Student*Module] =
      [ ("Susan", "Business Analytics")
      ; ("Susan", "IT-Governance")
      ; ("Susan", "Business Rules")
      ]
   
   RULE EnrollRule: isEnrolledFor |- takes;isPartOf~
   ROLE Administrator MAINTAINS EnrollRule
   
   RELATION course[ExamReg*Course] [UNI] =
      [ ("ER1", "Management")
      ; ("ER2", "Business IT")
      ; ("ER3", "Business IT")
      ]
   RELATION student[ExamReg*Student] [UNI] =
      [ ("ER1", "Peter")
      ; ("ER2", "Susan")
      ]
   RULE ExamRule1: student~;course |- takes
   RULE ExamRule2: student~;course;isPartOf~ |- isEnrolledFor
\end{verbatim}
   If we check this script, we get the following results:
\begin{verbatim}
   There are 2 violations of RULE "ExamRule2":
      ("Peter", "IT-Governance")
      ("Peter", "Finance")
   ==============================
   There is one violation of RULE "UNI isPartOf[Module*Course]":
      ("IT-Governance", "IT-Governance")
\end{verbatim}
   Any data migration must anticipate any population in the existing system.
   The only thing we know for sure about the existing population is that it satisfies the rules in $\infsys$.

\subsubsection{Strategies for restoring invariance}

We allow ExamRule2 to be broken during the transition to the desired system, and expect people with the Administrator role to deal with them.

However, the violations that arise when taking the (non-disjoint) union of the two scripts include violations of rules that the system should maintain.
This means that if we cannot generate software based on the disjoint union as it is.
To resolve this, the simplest solution is to change the roles assigned to the rules that are violated in the disjoint union:
\begin{verbatim}
   RELATION isPartOf[Module*Course]
   RULE isPartOfUNI: isPartOf;isPartOf~ |- I
   ROLE migrationHelper MAINTAINS isPartOfUNI
\end{verbatim}

Naturally, this solution requires effort from people with the role migrationHelper.
This in itself is not an issue: problems need to be solved one way or another.
However, a problem that may occur is that regular users of the system, who are used to the existing system, might be adding data to the system that causes violations faster than they can be solved.
To mitigate this, we propose two solutions.

One solution is to state that the only violations that may occur in the relation \verb=isPartOfUNI= are those that were present when the migration started:
\begin{verbatim}
   RELATION isPartOf[Module*Course]
   RULE isPartOfUNI: isPartOf;isPartOf~ |- I
   ROLE migrationHelper MAINTAINS isPartOfUNI
   
   RELATION isPartOfUNI_violations[Module*Module]
     = [("IT-Governance", "IT-Governance")]
   RULE isPartOfUNI_progress: -(isPartOf;isPartOf~) /\ I
         |- isPartOfUNI_violations
   SYSTEM MAINTAINS isPartOfUNI_progress
   
   ENFORCE isPartOfUNI_violations :< -(isPartOf;isPartOf~) /\ I  
\end{verbatim}

In this first solution, the first rule is the same as with the simple solution.
The second rule prevents any new violation from occurring.
Existing violations are recorded in the \verb=isPartOfUNI_violations= relation, and the rule \verb=isPartOfUNI_progress= prevents the set of violations to \verb=isPartOfUNI= from growing beyond this.
The \verb=ENFORCE= statement at the end of this code snippet states that the relation recording existing violations should be shrunk whenever violations are solved.
This way, we ensure that violations cannot re-occur.
As a whole, this means that \verb=migrationHelper= has a finite task.

As a second solution, we observe that the data in the designed system does not have any violations.
Consequentially, the cause of the violations comes from the triples in the existing system.
Rather than taking the union of all triples, we can take the disjoint union instead;
\begin{verbatim}
   RELATION isPartOf_old[Module*Course] [UNI]=
      [ ("Finance", "Management")
      ; ("Business Rules", "Business IT")
      ; ("Business Analytics", "Business IT")
      ; ("IT-Governance", "Management")
      ]
   RELATION isPartOf[Module*Course] [UNI]=
      [ ("IT-Governance", "Business IT") ]
   RELATION isPartOf_ignored[Module*Course]
   
   RULE isPartOf_isUnion:
         isPartOf_old |- isPartOf \/ isPartOf_ignored
   ROLE migrationHelper MAINTAINS isPartOf_isUnion
   
   ENFORCE isPartOf_ignored :> isPartOf
\end{verbatim}

\begin{verbatim}
   RELATION isPartOf_old[Module*Course] [UNI]=
      [ ("Finance", "Management")
      ; ("Business Rules", "Business IT")
      ; ("Business Analytics", "Business IT")
      ; ("IT-Governance", "Management")
      ]
   RELATION isPartOf[Module*Course] [UNI]=
      [ ("IT-Governance", "Business IT") ]
   RELATION isPartOf_ignored[Module*Course]

   ENFORCE isPartOf :> isPartOf_old - isPartOf_ignored
   ENFORCE isPartOf_ignored :> isPartOf
\end{verbatim}
In this second solution, we immediately move to a system in which the \verb=UNI= rule is maintained by the system.
The work for the \verb=migrationHelper= is again finite: the tuples in \verb=isPartOf_old= need to be copied over or explicitly ignored.
Once a tuple is in \verb=isPartOf=, the tuple is copied over to \verb=isPartOf_ignored= by the \verb=ENFORCE= statement, such that regular users can delete it without adding work to anyone with the \verb=migrationHelper= role.
Of course, since this solution starts with fewer pairs in the \verb=isPartOf= relation, other rules could be violated as a result of choosing this solution.
Indeed, ExamRule2 initially has more violations in this scenario.

\begin{definition}[migration dataset]
   \begin{eqnarray}
      &&\begin{array}[t]{@{}l}
         \{ {\tt ENFORCE\ }\declare{(nm',1)}{(A',1)}{(B',1)}\\\quad{\tt\ :=\ }\ident{(A',1)};\declare{(nm,0)}{(A,0)}{(B,0)};\ident{(B',1)}\\
            \mid\ \begin{array}[t]{@{}l}
               \declare{nm}{A}{B}\in\rels\wedge\declare{nm'}{A'}{B'}\in\rels'\wedge\id{nm}=\id{nm'}\ \wedge\\
               \{\pair{A}{A'},\pair{B}{B'}\}\subseteq\isa\cup\isa'\cup\flip{\isa}\cup\flip{\isa'}\}
               \end{array}
           \end{array}\\
           &\cup&\begin{array}[t]{@{}l}
            \{ {\tt CLASSIFY\ }(C',1){\tt\ IS\ }(C,0)\mid\ C\in\concepts,C'\in\concepts',C=C'\}\\
           \end{array}
   \end{eqnarray}
\end{definition}

The ${\tt CLASSIFY\ }(C',1){\tt\ IS\ }(C,0)$ statement adds two pairs to the $\isa$ relation:
$\pair{(C',1)}{(C,0)}$ and $\pair{(C,0)}{(C',1)}$.
Note that if $\dataset$ is a dataset, and $\dataset'$ is that dataset with pairs added%
\footnote{TODO: het woord `add' is informeel, dit kan beter} to the $\isa$ relation, then the result is again a dataset.
The ${\tt ENFORCE\ }r{\tt\ :=\ }e$ statements add triples $(x,r,y)$ for all $(x,y)\in e$.
The expressions $e$ in these statements are such that $x\ (\inst \compose \kleenestar{\flip{\isa}})\ (A',1)$ and $y \ (\inst \compose \kleenestar{\flip{\isa}})\ (B',1)$, thus preserving the property that the result is a dataset.

\begin{definition}[]
   \begin{eqnarray}
      \infsys\sqcup\infsys'&=&\la\roles',\rules',\maintain',\dataset\sqcup\dataset'\ra
   \end{eqnarray}
\end{definition}

\subsection{Changes}
% The purpose of this section is to explain why a dataset is structured the way it is.
   In the migration of a dataset we deal with changes to the elements that are not data:
   $\inst$, $\concepts$, and $\rels$.
   Such changes have further reaching consequences, however.
   Changes to $\inst$, $\concepts$, $\rels$ change the data structure in a dataset.
   We define a \define{migration of a dataset} $\la\atoms,\concepts,\inst,\rels,\dataset\ra$ as a change in which one or more of $\inst$, $\concepts$, or $\rels$ change.
   To satisfy the definition of datasets, $\dataset$ must change too,
   but the migration should try to preserve the maximal amount of data.

\section{Validation}
   To validate the theory, we have to prove that:
\begin{itemize}
   % -> het migratie-systeem is typefout-vrij (opmerking: als we de type-checker buiten beschouwing willen laten, kunnen we dit niet beschrijven)
   \item the migration system $\migrsys$ is free of type errors.
   % -> het migratie-systeem is vrij van overtredingen op regels die het migratie-systeem moet bewaken
   \item the migration system $\migrsys$ is an information system (definition~\ref{def:information system}),
         which implies that its population yields no violations.
   % -> het migratie-systeem bevat alle oude data, ihb nog steeds na het toepassen van de enforce regels
   \item $\migrsys$ contains the existing data, insofar its enforce rules have not deleted it.
   % -> er is een pad naar ingebruikname van het nieuwe systeem (vanaf de initiÃ«le toestand van het migratie-systeem)
   \item there is a path to releasing the desired system, starting with the initial state of the $\migrsys$.
   % -> corollary: op het moment van ingebruikname van het nieuwe systeem, is het migratie-systeem vrij van overtredingen op regels die het nieuwe systeem moet bewaken
   \item As a result, the desired system $\infsys'$ is void of violations at the start of its production life.
   % -> optioneel: na een begrensd aantal 'voortgangs-stappen' kan het nieuwe systeem in gebruik genomen worden
   \item Optionally, the desired system can be used in production after a limited number of steps.
   % -> optioneel: er bestaat een migratie-systeem dat de oude functionaliteit behoudt (mogelijk uitbreidt) totdat het nieuwe systeem in gebruik genomen is?
   \item Optionally, there is a migration systen that preserves the functionality of the existing system until the desired system has been put in production.
\end{itemize}
% Bewijsverplichtingen:

\section{Conclusions}
\begin{itemize}
   \item the part of the data that does not change can be migrated automatically;
   \item the part of the migration that can be automated is usually not sufficient;
         it takes additional human creativity to complete the migration specification;
\end{itemize}
\section{Bibliography}
\bibliographystyle{elsarticle-harv}
\bibliography{doc}


\end{document}
